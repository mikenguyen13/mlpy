{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Score Model\r\n",
    "Credit Scoring\n",
    "\n",
    " * Tool for classifying customers to reduce current and expected credit risk.\n",
    " * Defined as the process of modeling creditworthiness (Hand and Jacka, 1998).\n",
    " * Involves transforming relevant data into numerical measures for guiding credit decisions (Anderson, 2007).\n",
    "\n",
    "\r\n",
    "* A credit scoring model estimates the probability of default, indicating the likelihood of a credit event like bankruptcy or failure to pay.\r\n",
    "* The output of such a model is typically a credit score; a higher score indicates a lower risk of default.\r\n",
    "* Credit factors vary by loan type: for credit card loans, factors might include payment history and credit utilization, while for mortgages they could include down payment and job history.\r\n",
    "* The accuracy of these models is crucial for maximizing financial institutions' risk-adjusted returns.\r\n",
    "* Economic fluctuations like recessions or expansions necessitate that models be adaptable and quickly adjustable by risk managers and credit analysts.\r\n",
    "\r\n",
    "## Common Techniques in Credit Scoring Model Development and Validation\r\n",
    "* **Logistic regression and linear regression**\r\n",
    "* **Machine learning and predictive analytics**\r\n",
    "* **Gini Coefficients**\r\n",
    "* **Binning algorithms** (e.g., monotone, equal frequency, and equal width)\r\n",
    "* **Cumulative Accuracy Profile (CAP)**\r\n",
    "* **Receiver operating characteristic (ROC)**\r\n",
    "* **Kolmogorov-Smirnov (K-S) statistic**\r\n",
    "\r\n",
    "## Credit Score Model Types\r\n",
    "\r\n",
    "1. **Traditional Statistical Models**\r\n",
    "   * **Logistic Regression**: Still widely used for its simplicity and interpretability.\r\n",
    "   * **Decision Trees**: Used for their ability to handle non-linear relationships and interactions between variables.\r\n",
    "\r\n",
    "2. **Machine Learning Models**\r\n",
    "   * **Random Forests**: An ensemble method that uses multiple decision trees to improve predictive accuracy and control over-fitting.\r\n",
    "   * **Gradient Boosting Machines (GBM)**: Such as XGBoost and LightGBM, which build models in a stage-wise fashion and are highly effective for classification tasks like credit scoring.\r\n",
    "   * **Support Vector Machines (SVM)**: Effective in high-dimensional spaces and used for both regr\n",
    "   * **Reinforcement Learning**\n",
    "   * **Graph-based model**ession and classification.\r\n",
    "\r\n",
    "3. **Deep Learning Models**\r\n",
    "   * **Neural Networks**: Including feedforward neural networks and more complex architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). These models can capture complex patterns in large datasets.\r\n",
    "   * **Autoencoders**: Used for anomaly detection and feature extraction in credit scoring.\r\n",
    "\r\n",
    "4. **Hybrid Models**\r\n",
    "   * **Ensemble Methods**: Combining multiple models (e.g., blending logistic regression with gradient boosting) to improve robus\n",
    "       * Homogenous ensemble classifiers: (1) independent base models (e.g., bagging algo), (2) dependent base models (e.g., boosting algo).\n",
    "       * Heterogenous ensemble classifiers: different classification algos (e.g., logistic, random forests, etc.). If you prune some base models beforehand, it's called selective ensembles (either static or dynamic). tness and predictive performance.\r\n",
    "   * **Stacking**: A technique where predictions from multiple models are used as inputs to a higher-level model.\r\n",
    "\r\n",
    "5. **Alternative Data and Big Data Techniques**\r\n",
    "   * **Use of Alternative Data**: Incorporating non-traditional data sources such as social media activity, utility payments, and other digital footprints to enhance credit scoring models.\r\n",
    "   * **Big Data Analytics**: Leveraging large and diverse datasets to improve model accuracy and insights.\r\n",
    "\r\n",
    "6. **Explainable AI (XAI) Models**\r\n",
    "   * **SHAP (SHapley Additive exPlanations)**: Used to interpret complex models by assigning importance values to each feature.\r\n",
    "   * **LIME (Local Interpretable Model-agnostic Explanations)**: Provides explanations for individual predictions made by black-box models.\r\n",
    "\r\n",
    "7. **Regulatory and Ethical Considerations**\r\n",
    "   * **Fairness and Bias Mitigation**: Incorporating techniques to ensure models are fair and do not discriminate against protected groups.\r\n",
    "   * **Transparency**: Ensuring that models can be explained and understood by stakeholders, including regulatory bodies.\r\n",
    "\r\n",
    "## Examples of Implementations\r\n",
    "\r\n",
    "* **FICO Score 10**: Incorporates trended data to provide a more comprehensive view of an individual’s credit behavior over time.\r\n",
    "* **VantageScore 4.0**: Uses machine learning techniques and includes data on credit usage patterns, payment history, and total debt.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Augmenting Hybrid Credit Score Models with Alternative Datasets\r\n",
    "\r\n",
    "**Steps to Approach this:**\r\n",
    "\r\n",
    "1. **Data Integration**\r\n",
    "   * **Identify Relevant Features**: Determine which aspects of social media activity are relevant to credit scoring (e.g., frequency of posts, sentiment analysis, network size, engagement metrics).\r\n",
    "   * **Combine Datasets**: Merge traditional credit data with social media data for individuals who have a social media presence. Ensure that data from different sources are aligned properly.\r\n",
    "\r\n",
    "2. **Handling Missing Data**\r\n",
    "   * **Indicator Variables**: Create binary indicator variables to mark the presence or absence of social media data for each individual.\r\n",
    "   * **Separate Models**: Train separate models for individuals with and without social media data. Combine the predictions using a meta-model.\r\n",
    "   * **Imputation**: Use imputation techniques to handle missing social media data, though this should be done cautiously to avoid introducing bias.\r\n",
    "\r\n",
    "3. **Feature Engineering**\r\n",
    "   * **Extract Features**: Use natural language processing (NLP) and other techniques to extract features from social media text (e.g., sentiment scores, topic modeling).\r\n",
    "   * **Engagement Metrics**: Include metrics like the number of friends/followers, frequency of posts, and interaction rates.\r\n",
    "\r\n",
    "4. **Modeling Approach**\r\n",
    "   * **Hybrid Model Structure**: Use a hybrid model structure where social media features are added as additional inputs for the machine learning model. This could be an ensemble model where different data sources contribute to the final prediction.\r\n",
    "   * **Stacking and Blending**: Employ stacking or blending techniques where base models (one using traditional data and one using augmented data) are combined by a meta-learner.\r\n",
    "\r\n",
    "5. **Training and Validation**\r\n",
    "   * **Separate Training**: Train the model on individuals with complete traditional and social media data. Validate on a subset to ensure robustness.\r\n",
    "   * **Cross-validation**: Use cross-validation to test the performance of the model and prevent overfitting.\r\n",
    "   * **Fairness Checks**: Ensure that the inclusion of social media data does not introduce bias or unfair discrimination.\r\n",
    "\r\n",
    "6. **Model Interpretation and Explainability**\r\n",
    "   * **Explainable AI Tools**: Use tools like SHAP or LIME to interpret the impact of social media features on the model’s predictions.\r\n",
    "   * **Transparency**: Maintain transparency about how social media data is used and ensure compliance with privacy regulations.\r\n",
    "\r\n",
    "7. **Ethical and Privacy Considerations**\r\n",
    "   * **Consent and Privacy**: Ensure that individuals consent to the use of their social media data and that privacy regulations (e.g., GDPR) are strictly followed.\r\n",
    "   * **Ethical Use**: Be transparent about the use of social media data and ensure it is used ethically, without leading to discriminatory practices.\r\n",
    "\r\n",
    "## Example Workflow\r\n",
    "\r\n",
    "1. **Data Collection**:\r\n",
    "   * Collect traditional credit data (e.g., credit history, loan repayment).\r\n",
    "   * Collect social media data (e.g., public posts, engagement metrics) for consenting individuals.\r\n",
    "\r\n",
    "2. **Feature Engineering**:\r\n",
    "   * Extract features from both datasets.\r\n",
    "   * Create indicator variables for the presence of social media data.\r\n",
    "\r\n",
    "3. **Model Development**:\r\n",
    "   * Develop base models for traditional data and augmented data separately.\r\n",
    "   * Combine these models using an ensemble or stacking approach.\r\n",
    "\r\n",
    "4. **Model Training**:\r\n",
    "   * Train the hybrid model using a combined dataset.\r\n",
    "   * Validate using cross-validation techniques.\r\n",
    "\r\n",
    "5. **Model Interpretation**:\r\n",
    "   * Use SHAP/LIME to understand the contribution of social media features.\r\n",
    "\r\n",
    "6. **Implementation**:\r\n",
    "   * Deploy the model ensuring it meets regulatory and ethical standards.\r\n",
    "s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to {cite:t}`west2000neural`:\r\n",
    "\r\n",
    "* **Neural Network Models:**\r\n",
    "  - **Multilayer Perceptron**\r\n",
    "  - **Mixture-of-Experts** (recommended)\r\n",
    "  - **Radial Basis Function** (recommended)\r\n",
    "  - **Learning Vector Quantization**\r\n",
    "  - **Fuzzy Adaptive Resonance**\r\n",
    "\r\n",
    "* **Traditional Methods:**\r\n",
    "  - **Linear Discriminant Analysis**\r\n",
    "  - **Logistic Regression** (best among traditional methods)\r\n",
    "  - **k-Nearest Neighbor**\r\n",
    "  - **Kernel Density Estimation**\r\n",
    "  - **Decision Trees**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{cite:t}`chuang2009constructing` introduces a two-stage Reassigning Credit Scoring Model (RCSM) to improve accuracy and reduce Type I errors.\n",
    "1. The first stage involves constructing an ANN-based model to classify credit applicants as either accepted (good) or rejected (bad).\n",
    "2. The second stage reduces Type I errors by reassigning mistakenly rejected good applicants to a \"conditionally accepted\" category using a CBR-based classification technique.\n",
    "* The RCSM was tested on a credit card dataset from the UCI repository and demonstrated greater accuracy compared to four other commonly used approaches.\n",
    "    *  Linear Discriminant Analysis\n",
    "    *  Logistic Regression\n",
    "    *  CART (Classification and regression tree)\n",
    "    *  MARS (Multivariate adaptive regression spline)\n",
    "    *  ANNs (Artificial neural networks)\n",
    "    *  CBR (Case-based reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit Score card model** is easier to interpret and deploy {cite:t}`yap2011using`. \n",
    "\n",
    "{cite:t}`hlongwane2024enhancing` implement\n",
    " * XGBoost ( {cite}`gunnarsson2021deep` prefers)\n",
    " * LightGBM\n",
    " * CatBoost\n",
    " * Model-X knockoffs\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\r\n",
    "\r\n",
    "- **Information gain, gain ratio, and chi-square**: Trivedi et al. (2020) {cite}`trivedi2020study`\r\n",
    "- **Neighbourhood rough set (NRS)**: Tripathi and Aggarwal (2018) {cite}`tripathi2018hybrid`\r\n",
    "- **Other methods**: Nalic et al. (2020) {cite}`nalic2020new`\r\n",
    "\r\n",
    "## Credit Score for Business\r\n",
    "\r\n",
    "- **SMEs**: Roy et al. (2023) {cite}`roy2023credit`\r\n",
    "\r\n",
    "## Alternative Data Sources\r\n",
    "\r\n",
    "- **Email usage and psychometric variables**: Djeundje et al. (2021) {cite}`djeundje2021enhancing`, Arraiz et al. (2017) {cite}`arraiz2017psychometrics`\r\n",
    "- **Social Media data**: Wei et al. (2016) {cite}`wei2016credit`, Ge et al. (2017) {cite}`ge2017predicting`, de Souza et al. (2019) {cite}`de2019does`\r\n",
    "- **Telecommunication**: Oskarsdottir et al. (2019) {cite}`oskarsdottir2019value`, Ots and Li (2020) {cite}`ots2020mobile`, de Montjoye et al. (2011) {cite}`de2011towards`, Pedro et al. (2015) {cite}`pedro2015mobiscore`, Agarwal et al. (2018) {cite}`agarwal2018predicting`\r\n",
    "\r\n",
    "## Related Issues\r\n",
    "\r\n",
    "- **Credit card fraud detection**: Zhang et al. (2021) {cite}`zhang2021hoba`\r\n",
    "ng2021hoba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to {cite}`abdou2011credit`, common models include:\n",
    " * MOE: Mixture of Experts\n",
    " * RBF: Radial Basis Function\n",
    " * MLP: Multilayer Perceptron\n",
    " * LVQ: Learning Vector Quantization\n",
    " * FAR: Fuzzy Adaptive Resonance (all of which are neural network models)\n",
    " * Multilayer Feed-Forward Neural Network\n",
    " * Weight-of-Evidence Model\n",
    " * Genetic Programming\n",
    " * Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Criteria**                         | **Calculation**                                                                                                  | **Description**                                                                                                     | **Pros**                                                                                           | **Cons**                                                                                           | **Applications**                                  | **Global/Local**                    |\n",
    "|--------------------------------------|------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|--------------------------------------------------|-------------------------------------|\n",
    "| **Confusion Matrix**                 | NA                                                                                                               | Summarizes prediction results, showing the number of true positives, false positives, true negatives, and false negatives. | Simple to understand and interpret; useful for binary classification.                               | Doesn't provide information on the cost of misclassification; limited to binary classification.     | Binary classification problems.                  | Local                                |\n",
    "| **MSE/RMSE**                         | $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ <br> $\\text{RMSE} = \\sqrt{\\text{MSE}}$              | Measures the average squared difference between actual and predicted values. RMSE is the square root of MSE, giving error in the same unit as the output. | Provides a clear indication of prediction accuracy; RMSE is in the same units as the output.         | Sensitive to outliers; may not reflect actual performance for non-normally distributed errors.       | Regression problems.                             | Global                               |\n",
    "| **MAE**                              | $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\|y_i - \\hat{y}_i\\|$                                                     | Measures the average absolute difference between actual and predicted values, less sensitive to outliers than MSE.  | Robust to outliers; easier interpretation compared to MSE.                                          | Doesn't penalize larger errors more than smaller ones; less sensitive to large deviations.          | Regression problems.                             | Global                               |\n",
    "| **Mean Error**                       | $\\text{Mean Error} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$                                               | Measures the average difference between actual and predicted values, giving a simple error estimate.                 | Simple to compute and interpret; good for initial assessment.                                       | Can be misleading if not combined with other metrics; does not penalize larger errors.              | Initial error assessment.                        | Global                               |\n",
    "| **R² / Adj R²**                      | $R² = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$ <br> $\\text{Adj R²} = 1 - \\frac{(1-R²)(n-1)}{n-p-1}$ | Measures the proportion of variance in the dependent variable explained by the model. Adjusted R² adjusts for the number of predictors. | Gives insight into model's explanatory power; Adjusted R² corrects for overfitting.                  | May not indicate model's performance on different datasets; Adjusted R² can be complex to interpret. | Evaluating explanatory power of regression models. | Global                               |\n",
    "| **Sensitivity/Specificity (ROC Curve)** | $\\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$ <br> $\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$ | Analyzes the true positive rate against the false positive rate, used to evaluate the diagnostic ability of a binary classifier. | Useful for evaluating the trade-off between sensitivity and specificity; provides a graphical representation. | Doesn't consider the costs of false positives and false negatives; ROC analysis can be complex.      | Evaluating diagnostic tests, medical testing.    | Global                               |\n",
    "| **Discrimination (C-statistic/AUC)** | $AUC = \\int_{0}^{1} \\text{ROC}(t) \\, dt$                                                                         | Measures the model's ability to discriminate between different classes, AUC is the area under the ROC curve.         | Provides a single metric for model's discriminative power; widely used and understood.               | May not provide detailed error analysis; AUC interpretation can be context-dependent.                | Binary classification, credit scoring.           | Global                               |                           |\n",
    "| **Accuracy (ACC)**                   | $\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total}}$                                                   | Measures the overall accuracy of the model by comparing the number of correct predictions to the total predictions.  | Simple to interpret; provides a clear indication of overall model accuracy.                          | Does not provide insights into the types of errors made; may be misleading in imbalanced datasets.   | Classification problems.                         | Global                               |\n",
    "| **Estimated Misclassification Cost** | $\\text{Cost} = \\sum_{i=1}^{n} \\text{Cost}(y_i, \\hat{y}_i)$                                                       | Estimates the cost associated with incorrect classifications, taking into account different types of errors.         | Provides a cost-based evaluation of model performance, useful in applications where error costs differ. | Requires estimation of costs, which can be subjective and may vary by context.                       | Applications where misclassification costs vary, such as fraud detection. | Local                                |\n",
    "| **Percentage Correctly Classified (PCC)** | $\\text{PCC} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total}} \\times 100$                                             | Measures the percentage of correct predictions relative to the total predictions.                                     | Simple and easy to interpret; provides a basic measure of model performance.                          | Doesn't account for the balance between classes; may be misleading in imbalanced datasets.          | General classification problems.                 | Global                               |\n",
    "| **Partial Gini Index (PG)**          | $\\text{PG} = \\frac{\\sum_{i=1}^{n} (y_i \\text{Gini})}{\\sum_{i=1}^{n} y_i}$                                         | Measures the performance of the model relative to a partial area under the ROC curve.                                | Focuses on the performance in a specific part of the ROC curve; useful for targeted performance analysis. | Requires setting a specific threshold for evaluation; can be complex to calculate and interpret.    | Targeted model performance analysis.             | Local                                |\n",
    "| **H-measure**                        | NA                                                                                                               | A metric designed to overcome some limitations of AUC by incorporating misclassification costs and prevalence.       | Incorporates misclassification costs; robust to variations in prevalence.                             | Requires estimation of costs and prevalence; more complex than traditional metrics.                 | Binary classification, situations with varied misclassification costs. | Global                               |\n",
    "| **Brier Score (BS)**                 | $\\text{BS} = \\frac{1}{n} \\sum_{i=1}^{n} (p_i - o_i)^2$                                                           | Measures the mean squared difference between predicted probabilities and actual outcomes.                             | Provides a probabilistic assessment of model performance; accounts for prediction confidence.          | Sensitive to the calibration of predicted probabilities; can be less intuitive than other metrics.   | Evaluating probabilistic forecasts.              | Global                               |\n",
    "| **Kolmogorov–Smirnov Statistic (KS)**| $\\text{KS} = \\max_{t} \\|F_1(t) - F_2(t)\\|$                                                              | Measures the maximum difference between the cumulative distribution functions of the observed and predicted distributions. | Provides insight into the separation of the distributions; useful for comparing model performance.     | Can be sensitive to sample size; may require large sample sizes for reliable estimates.             | Model performance comparison, goodness-of-fit tests. | Global                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{cite}`lessmann2015benchmarking`\n",
    "\n",
    "**Individual Classification**:\n",
    "\n",
    "Bayesian network\n",
    "CART (Classification and Regression Trees)\n",
    "Extreme Learning Machine\n",
    "Kernelized ELM (Extreme Learning Machine)\n",
    "K-Nearest Neighbor\n",
    "J4.8 (an implementation of C4.5 in WEKA)\n",
    "LDA (Linear Discriminant Analysis)\n",
    "Linear Support Vector Machine\n",
    "Logistic Regression\n",
    "Multilayer Perceptron Artificial Neural Network\n",
    "Naive Bayes\n",
    "Quadratic Discriminant Analysis\n",
    "Radial Basis Function Network\n",
    "SVM with Radial Basis Kernel Function\n",
    "Voted Perceptron\n",
    "\n",
    "\n",
    "**Homogeneous Ensembles**:\n",
    "\n",
    "Alternating Decision Tree\n",
    "Bagged Decision Trees\n",
    "Bagged MLP (Multilayer Perceptron)\n",
    "Boosted Decision Trees\n",
    "Logistic Model Tree\n",
    "Random Forest\n",
    "Rotation Forest\n",
    "Stochastic Gradient Boosting\n",
    "\n",
    "**Heterogeneous Ensembles**:\n",
    "\n",
    "Simple Average Ensemble\n",
    "Weighted Average Ensemble\n",
    "Stacking\n",
    "Complementary Measure\n",
    "Ensemble Pruning via Reinforcement Learning\n",
    "GASEN (Genetic Algorithm-based Selective Ensemble)\n",
    "Hill-Climbing Ensemble Selection\n",
    "HCES (Hill-Climbing Ensemble Selection) with Bootstrap Sampling\n",
    "Matching Pursuit Optimization Ensembles\n",
    "Top-T Ensemble\n",
    "Clustering Using Compound Error\n",
    "K-Means Clustering\n",
    "Kappa Pruning\n",
    "Margin Distance Minimization\n",
    "Uncertainty Weighted Accuracy\n",
    "Probabilistic Model for Classifier Competence\n",
    "K-Nearest Oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to Handle Imbalanced Data\r\n",
    "\r\n",
    "### 1. Data-Level Approaches\r\n",
    "\r\n",
    "#### a. Oversampling Techniques\r\n",
    "\r\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique)**: Generates synthetic examples of the minority class by interpolating between existing samples.\r\n",
    "- **ADASYN (Adaptive Synthetic Sampling)**: Similar to SMOTE but focuses more on generating synthetic samples for harder-to-classify instances.\r\n",
    "- **Random Oversampling**: Replicates minority class examples until the dataset is balanced.\r\n",
    "\r\n",
    "#### b. Undersampling Techniques\r\n",
    "\r\n",
    "- **Random Undersampling**: Randomly removes instances from the majority class to balance the dataset.\r\n",
    "- **Cluster-Based Undersampling**: Clusters the majority class and removes samples based on cluster proximity.\r\n",
    "- **NearMiss**: Selects majority samples closest to the minority samples or farthest from other majority samples.\r\n",
    "\r\n",
    "#### c. Hybrid Techniques\r\n",
    "\r\n",
    "- **SMOTE-Tomek Links**: Combines SMOTE with Tomek links to remove overlapping samples from the majority class.\r\n",
    "- **SMOTE-ENN (Edited Nearest Neighbors)**: Uses SMOTE for oversampling and ENN for cleaning the dataset by removing misclassified instances.\r\n",
    "\r\n",
    "### 2. Algorithm-Level Approaches\r\n",
    "\r\n",
    "#### a. Cost-Sensitive Learning\r\n",
    "\r\n",
    "- **Cost-Sensitive Classifiers**: Adjusts the learning process to minimize the cost of misclassifications, such as higher penalties for minority class errors.\r\n",
    "- **Weighted Loss Functions**: Assigns different weights to classes in the loss function to emphasize the minority class.\r\n",
    "\r\n",
    "#### b. Ensemble Methods\r\n",
    "\r\n",
    "- **Balanced Random Forest**: Modifies the random forest algorithm to balance each bootstrap sample.\r\n",
    "- **EasyEnsemble**: Combines multiple weak learners trained on different balanced subsets of the majority class.\r\n",
    "- **RUSBoost (Random UnderSampling with Boosting)**: Integrates undersampling with boosting techniques.\r\n",
    "\r\n",
    "#### c. Anomaly Detection\r\n",
    "\r\n",
    "- **One-Class SVM**: Treats the minority class as the target and identifies it against the majority background.\r\n",
    "- **Isolation Forest**: Detects outliers, assuming the minority class can be seen as anomalies.\r\n",
    "\r\n",
    "### 3. Deep Learning Approaches\r\n",
    "\r\n",
    "#### a. Data Augmentation\r\n",
    "\r\n",
    "- **GANs (Generative Adversarial Networks)**: Generate realistic minority class samples to augment the dataset.\r\n",
    "- **Autoencoders**: Learn latent features of the minority class and use them to generate new samples.\r\n",
    "\r\n",
    "#### b. Transfer Learning\r\n",
    "\r\n",
    "- **Feature Transfer**: Uses features learned from a balanced or related task to improve minority class recognition.\r\n",
    "- **Fine-Tuning**: Fine-tunes pre-trained models on the imbalanced dataset to leverage general features.\r\n",
    "\r\n",
    "#### c. Specialized Architectures\r\n",
    "\r\n",
    "- **Focal Loss**: Modifies the cross-entropy loss to focus more on hard-to-classify examples, often used in object detection tasks.\r\n",
    "- **Class-Balanced Loss**: Scales the loss by the inverse of the class frequency to balance the influence of each class.\r\n",
    "s frequency to balance the influence of each class.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "Cross-validation is a statistical method used to estimate the skill of machine learning models. It is primarily used in applied machine learning to estimate the predictive power of a model on new data. Cross-validation involves partitioning a dataset into a training set and a test set, training the model on the training set, and evaluating it on the test set. This process is repeated multiple times with different splits to reduce variability and obtain a more accurate measure of model performance.\n",
    "\n",
    "### 1. Nested Cross-Validation\n",
    "\n",
    "Nested cross-validation is used for model selection and hyperparameter tuning while avoiding overfitting.\n",
    "\n",
    "- **Inner Loop**: Performs cross-validation for hyperparameter tuning.\n",
    "- **Outer Loop**: Evaluates the model performance using the optimal hyperparameters found in the inner loop.\n",
    "- Provides an unbiased estimate of the model’s performance.\n",
    "\n",
    "### 2. Monte Carlo Cross-Validation (Repeated Random Subsampling Validation)\n",
    "\n",
    "- Randomly splits the dataset into training and test sets multiple times (more than two).\n",
    "- Averages the performance metrics across different splits.\n",
    "- Offers a better approximation of model performance by considering varied data splits.\n",
    "\n",
    "### 3. Stratified K-Fold Cross-Validation\n",
    "\n",
    "- Ensures each fold has a representative proportion of each class.\n",
    "- Important for imbalanced datasets.\n",
    "- Reduces biased performance estimates due to uneven class distribution.\n",
    "\n",
    "### 4. Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "- Uses each sample once as a test set, with the remaining samples as the training set.\n",
    "- Provides a nearly unbiased performance estimate but is computationally intensive.\n",
    "- Suitable for small datasets.\n",
    "\n",
    "### 5. Group K-Fold Cross-Validation\n",
    "\n",
    "- Ensures that samples from the same group (e.g., same subject or time period) are not split across different folds.\n",
    "- Useful for datasets where samples are not independent and identically distributed (i.i.d.).\n",
    "\n",
    "### 6. Time Series Cross-Validation (Rolling Forecasting Origin)\n",
    "\n",
    "- Designed for time series data where the order of observations is crucial.\n",
    "- Uses a rolling window approach to train on past data and test on future data.\n",
    "- Preserves temporal order, making it suitable for time-dependent datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To partition data for model comparison in the credit scoring model:\n",
    "\n",
    "1. **Stratified K-Fold Cross-Validation**: \n",
    "  - **Purpose**: To create training and test sets for all models.\n",
    "  - **Process**: Partition the data into K folds, ensuring each fold has a representative proportion of each class.\n",
    "\n",
    "2. **Nested Cross-Validation**:\n",
    "  - **Applied to Each Training Set**: Use the training sets obtained from stratified K-fold.\n",
    "  - **Inner Loop**: \n",
    "    - **Purpose**: Train the model.\n",
    "    - **Process**: Further split the training set into inner training and validation sets to train the model.\n",
    "  - **Outer Loop**:\n",
    "    - **Purpose**: Hyperparameter tuning.\n",
    "    - **Process**: Use the performance on the inner validation sets to tune hyperparameters and evaluate model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have different objective functions based on various metrics, we need to repeat the process for each optimization metric.\r\n",
    "Moreover, it is important to assess the correspondence of classifier performance across these metrics. Specifically, we can use the agreement of classifier rankings across accuracy indicators by applying Kendall’s rank correlation coefficient. This helps us determine whether the metrics have high agreement and provide consistent recommendations (the best case), or if they disagree. If there is disagreement, we can decide which metric to focus on, whether it be a local or global assessment.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profitability Calculation\n",
    "\n",
    "The goal of this calculation is to estimate the profitability of a credit scoring model (or scorecard) by analyzing the costs associated with classification errors—specifically, false positives and false negatives. This involves determining how often good credit risks are wrongly classified as bad (False Positive Rate, FPR) and bad credit risks are wrongly classified as good (False Negative Rate, FNR), and then weighting these errors by their respective costs.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **False Positive Rate (FPR)**: The fraction of good credit risks that are incorrectly classified as bad. \n",
    "\n",
    "2. **False Negative Rate (FNR)**: The fraction of bad credit risks that are incorrectly classified as good.\n",
    "\n",
    "3. **Misclassification Costs**:\n",
    "   - $C(+ | -)$: The opportunity cost of denying credit to a good risk. This is the cost incurred when a good applicant is mistakenly rejected.\n",
    "   - $C(- | +)$: The cost of granting credit to a bad risk. This includes financial losses, often quantified as the net present value of exposure at default (EAD) times the loss given default (LGD).\n",
    "\n",
    "### Calculation\n",
    "\n",
    "The misclassification cost of a scorecard, \\( C(s) \\), is calculated using the formula:\n",
    "\n",
    "$$\n",
    "C(s) = C(+|−) \\cdot \\text{FPR} + C(−|+) \\cdot \\text{FNR}\n",
    "$$\n",
    "\n",
    "Here's a step-by-step breakdown:\n",
    "\n",
    "1. **Determine the Costs**: \n",
    "   - $C(+ | -)$ represents the cost of wrongly denying credit to a good risk.\n",
    "   - $C(- | +)$ represents the cost of wrongly granting credit to a bad risk.\n",
    "\n",
    "2. **Calculate the FPR and FNR**:\n",
    "   - FPR is the proportion of good applicants that are incorrectly classified as bad.\n",
    "   - FNR is the proportion of bad applicants that are incorrectly classified as good.\n",
    "\n",
    "3. **Combine the Costs and Rates**:\n",
    "   - Multiply the cost of each type of error by its rate to get the weighted costs.\n",
    "\n",
    "4. **Sum the Weighted Costs**:\n",
    "   - Add the weighted FPR and FNR to get the total misclassification cost for the scorecard.\n",
    "\n",
    "### Cost Ratios and Scenarios\n",
    "\n",
    "To cover different scenarios, the calculation considers various ratios of $C(+ | -)$ to $C(- | +)$, assuming that it is generally more costly to grant credit to a bad risk than to reject a good application. For example, the ratios range from 1:2 to 1:50. By fixing $C(+ | -)$ at 1 and varying $C(- | +)$, the analysis can explore how different misclassification costs impact the profitability estimation.\n",
    "\n",
    "### Normalization and Comparison\n",
    "\n",
    "1. **Compute Misclassification Costs**: For each cost setting and credit scoring dataset, the misclassification costs $C(s)$ are calculated using the formula above.\n",
    "\n",
    "2. **Estimate Expected Error Costs**: These costs are averaged over different datasets to get an overall estimate.\n",
    "\n",
    "3. **Normalize Costs**: The costs are normalized to represent percentage improvements compared to a baseline model, such as a logistic regression (LR) model.\n",
    "\n",
    "### Example for Clarification\n",
    "\n",
    "**Assume**:\n",
    "- $C(+ | -) = 1$ (Opportunity cost for rejecting a good applicant).\n",
    "- $C(- | +) = 10$ (Cost for approving a bad applicant).\n",
    "- FPR = 0.05 (5% of good applicants are wrongly rejected).\n",
    "- FNR = 0.10 (10% of bad applicants are wrongly approved).\n",
    "\n",
    "**Calculation**:\n",
    "$$\n",
    "C(s) = 1 \\cdot 0.05 + 10 \\cdot 0.10 = 0.05 + 1 = 1.05\n",
    "$$\n",
    "\n",
    "This result suggests that the total misclassification cost for this scorecard, given the specified costs and error rates, is 1.05.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "- **Linear Models:**\n",
    "  - Linear Discriminant Analysis (LDA)\n",
    "  - Logistic Regression (LR)\n",
    "  - Naïve Bayes (NB)\n",
    "- **Instance-Based Learning:**\n",
    "  - k-Nearest Neighbor (k-NN)\n",
    "- **Decision Trees:**\n",
    "  - Decision Trees (DTs)\n",
    "  - Random Forests (RFs)\n",
    "- **Support Vector Machines (SVMs)**\n",
    "- **Neural Networks:**\n",
    "  - Artificial Neural Networks (ANNs)\n",
    "  - Convolutional Neural Networks (CNNs)\n",
    "  - Deep Multi-Layer Perceptron (DMLP)\n",
    "  - Restricted Boltzmann Machines (RBMs)\n",
    "  - Deep Belief Networks (DBNs)\n",
    "- **Ensemble Methods:**\n",
    "  - Boosting\n",
    "  - Extreme Gradient Boost (XGBoost)\n",
    "  - Bagging\n",
    "\n",
    "## Feature Selection Methods\n",
    "\n",
    "### 1. Filter Methods\n",
    "\n",
    "- **F-score:**\n",
    "  - Measures how well a feature discriminates between two sets of data.\n",
    "  - **Formula:** Compares average values of a feature across the whole dataset, positive instances, and negative instances.\n",
    "\n",
    "- **Rough Set Theory:**\n",
    "  - Defines important features based on the indiscernibility relation.\n",
    "  - Uses subsets of features to find a reduced set of important features.\n",
    "\n",
    "### 2. Wrapper Methods\n",
    "\n",
    "- **Stepwise Selection:**\n",
    "  - **Forward Selection:** Adds features one by one based on significance.\n",
    "  - **Backward Elimination:** Starts with all features and removes insignificant ones.\n",
    "  - **Stepwise Feature Selection:** Combines forward and backward methods.\n",
    "\n",
    "- **Genetic Algorithm:**\n",
    "  - Evolves a population of solutions using selection, crossover, and mutation.\n",
    "  - Uses a fitness score to measure model performance (e.g., classification accuracy).\n",
    "  - Balances exploration (searching new regions) and exploitation (using known information) to find the best feature subsets.\n",
    "\n",
    "### 3. Embedded Methods\n",
    "\n",
    "- **LASSO (Least Absolute Shrinkage and Selection Operator):**\n",
    "  - Uses L1-penalized regression to select features.\n",
    "  - **Objective:** Minimize prediction error with a penalty for the number of features.\n",
    "  - Simplifies the model by reducing coefficients of less important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
