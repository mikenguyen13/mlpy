{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36762e06-b33a-46f0-b916-c2c268f2a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers scikit-learn pandas\n",
    "#!pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cdc9e5-2851-4a2e-a9bf-f7757dc9b255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miken\\anaconda3\\envs\\mlpy\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\miken\\anaconda3\\envs\\mlpy\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching website is at index 0 with a similarity score of 0.8684\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Your industry description\n",
    "industry_text = \"Nganh nghe buon ban du lieu\"\n",
    "\n",
    "# Texts from different websites\n",
    "website_texts = [\n",
    "    \"Chung toi buon ban du lieu va cong nghe\",\n",
    "    \"Ben cung cap cong nghe\",\n",
    "    \"Hien tai chung toi dang phat trien he thong\",\n",
    "    # Add more website texts\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "industry_embedding = model.encode(industry_text)\n",
    "website_embeddings = model.encode(website_texts)\n",
    "\n",
    "# Compute similarities\n",
    "similarities = cosine_similarity([industry_embedding], website_embeddings)[0]\n",
    "\n",
    "# Find the best match\n",
    "best_match_index = np.argmax(similarities)\n",
    "best_match_score = similarities[best_match_index]\n",
    "\n",
    "print(f\"Best matching website is at index {best_match_index} with a similarity score of {best_match_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cdc858-6b9a-4678-ae98-8b6aa2426877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miken\\anaconda3\\envs\\mlpy\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Multilingual SBERT model\n",
    "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ca21e0-75a9-427c-aba3-29eb6ea2771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7585b26-c79f-40fa-8d3a-de1a6643934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize Vietnamese text\n",
    "    return ' '.join(word_tokenize(text))\n",
    "\n",
    "# Your industry description\n",
    "industry_text = \"Nganh nghe buon ban du lieu\"\n",
    "\n",
    "# Texts from different websites\n",
    "website_texts = [\n",
    "    \"Chung toi buon ban du lieu va cong nghe\",\n",
    "    \"Ben cung cap cong nghe\",\n",
    "    \"Hien tai chung toi dang phat trien he thong\",\n",
    "    # Thêm nhiều nội dung trang web nếu cần\n",
    "]\n",
    "\n",
    "# Apply preprocessing\n",
    "industry_text = preprocess_text(industry_text)\n",
    "website_texts = [preprocess_text(text) for text in website_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b465fd1-409f-4239-978c-117ee9011cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "industry_embedding_sbert = sbert_model.encode(industry_text)\n",
    "website_embeddings_sbert = sbert_model.encode(website_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323887e4-72a3-4e73-8135-79f16a628665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarities\n",
    "similarities_sbert = cosine_similarity([industry_embedding_sbert], website_embeddings_sbert)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "550e373c-1218-4e03-926e-aee4f030390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miken\\anaconda3\\envs\\mlpy\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LaBSE (Language-agnostic BERT Sentence Embedding)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize LaBSE model\n",
    "labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "# Generate embeddings\n",
    "industry_embedding_labse = labse_model.encode(industry_text)\n",
    "website_embeddings_labse = labse_model.encode(website_texts)\n",
    "# Compute cosine similarities\n",
    "similarities_labse = cosine_similarity([industry_embedding_labse], website_embeddings_labse)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8dc496-1b6b-4fae-b153-4eff763d921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn pandas underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc4a12a-3d79-4889-8c6f-be6af5373e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with Cosine Similarity (Customized for Vietnamese)\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize Vietnamese text\n",
    "    return ' '.join(word_tokenize(text))\n",
    "\n",
    "# Apply preprocessing\n",
    "industry_text = preprocess_text(industry_text)\n",
    "website_texts = [preprocess_text(text) for text in website_texts]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine all texts for fitting the vectorizer\n",
    "all_texts = [industry_text] + website_texts\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # You can customize stop words for Vietnamese\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Industry vector is the first vector\n",
    "industry_vector_tfidf = tfidf_matrix[0]\n",
    "\n",
    "# Website vectors\n",
    "website_vectors_tfidf = tfidf_matrix[1:]\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities_tfidf = cosine_similarity(industry_vector_tfidf, website_vectors_tfidf)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d48b4ee-1fef-46fe-9285-fb9cf24e67b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\miken\\anaconda3\\envs\\mlpy\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow tensorflow-hub sentence-transformers scikit-learn pandas underthesea\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f642c0c2-cb20-46fd-a843-9fd213f019b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch scikit-learn pandas underthesea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d44265ea-e1b4-4c79-b230-56a017c3718a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miken\\anaconda3\\envs\\mlpy\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# PhoBERT (Vietnamese-Specific BERT Model)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize PhoBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize Vietnamese text\n",
    "    return ' '.join(word_tokenize(text))\n",
    "\n",
    "# Apply preprocessing\n",
    "industry_text = preprocess_text(industry_text)\n",
    "website_texts = [preprocess_text(text) for text in website_texts]\n",
    "\n",
    "def get_phobert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = phobert_model(**inputs)\n",
    "        # Use the [CLS] token representation\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "# Generate embeddings\n",
    "industry_embedding_phobert = get_phobert_embedding(industry_text)\n",
    "website_embeddings_phobert = [get_phobert_embedding(text) for text in website_texts]\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities_phobert = cosine_similarity([industry_embedding_phobert], website_embeddings_phobert)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb3ffb4-4654-4b1a-bddc-b57c9d567d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Website  SBERT Similarity  LaBSE Similarity  TF-IDF Similarity  \\\n",
      "0  Website 1          0.786298          0.671671           0.611115   \n",
      "1  Website 2          0.353927          0.558076           0.102860   \n",
      "2  Website 3          0.344390          0.398221           0.000000   \n",
      "\n",
      "   PhoBERT Similarity  \n",
      "0            0.837453  \n",
      "1            0.485002  \n",
      "2            0.798745  \n"
     ]
    }
   ],
   "source": [
    "# Comparing the Methods for Vietnamese\n",
    "                     \n",
    "# Create a DataFrame to store similarity scores\n",
    "df_results = pd.DataFrame({\n",
    "    'Website': [f\"Website {i+1}\" for i in range(len(website_texts))],\n",
    "    'SBERT Similarity': similarities_sbert,\n",
    "    'LaBSE Similarity': similarities_labse,\n",
    "    'TF-IDF Similarity': similarities_tfidf,\n",
    "    'PhoBERT Similarity': similarities_phobert\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da7b265-15b4-4e47-97fd-2b8dac837999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Matches per Method:\n",
      "SBERT: Website 1 with similarity 0.7863\n",
      "LaBSE: Website 1 with similarity 0.6717\n",
      "TF-IDF: Website 1 with similarity 0.6111\n",
      "PhoBERT: Website 1 with similarity 0.8375\n",
      "\n",
      "Consensus on Best Match:\n",
      "Website 1    4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find the best match for each method\n",
    "best_sbert = df_results.loc[df_results['SBERT Similarity'].idxmax()]\n",
    "best_labse = df_results.loc[df_results['LaBSE Similarity'].idxmax()]\n",
    "best_tfidf = df_results.loc[df_results['TF-IDF Similarity'].idxmax()]\n",
    "best_phobert = df_results.loc[df_results['PhoBERT Similarity'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Matches per Method:\")\n",
    "print(f\"SBERT: {best_sbert['Website']} with similarity {best_sbert['SBERT Similarity']:.4f}\")\n",
    "print(f\"LaBSE: {best_labse['Website']} with similarity {best_labse['LaBSE Similarity']:.4f}\")\n",
    "print(f\"TF-IDF: {best_tfidf['Website']} with similarity {best_tfidf['TF-IDF Similarity']:.4f}\")\n",
    "print(f\"PhoBERT: {best_phobert['Website']} with similarity {best_phobert['PhoBERT Similarity']:.4f}\")\n",
    "\n",
    "# Find the best match index for each method\n",
    "best_matches = {\n",
    "    'SBERT': df_results['SBERT Similarity'].idxmax(),\n",
    "    'LaBSE': df_results['LaBSE Similarity'].idxmax(),\n",
    "    'TF-IDF': df_results['TF-IDF Similarity'].idxmax(),\n",
    "    'PhoBERT': df_results['PhoBERT Similarity'].idxmax()\n",
    "}\n",
    "\n",
    "# Map indices to website names\n",
    "best_matches = {method: df_results.loc[idx, 'Website'] for method, idx in best_matches.items()}\n",
    "\n",
    "# Count occurrences\n",
    "consensus = pd.Series(list(best_matches.values())).value_counts()\n",
    "\n",
    "print(\"\\nConsensus on Best Match:\")\n",
    "print(consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ade868-35de-44f8-b10d-3e13ede1afff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
