{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff32bbf7-ae32-4527-95af-0189f5f117bd",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "\n",
    "Matrix factorization is one of the bedrocks of modern data analysis, appearing in fields as diverse as recommendation systems, computer vision, and natural language processing. Its beauty lies in its simplicity: we take a complex matrix (e.g., a user-item interaction matrix in a recommendation system) and decompose it into smaller, interpretable matrices that reveal hidden patterns in the data.\n",
    "\n",
    "The goal of this chapter is to explore **what matrix factorization is**, **why it works**, and **how it is applied** in a range of settings. We will also discuss **advanced variations** of the basic technique and **state-of-the-art methods** in matrix factorization.\n",
    "\n",
    "## What Is Matrix Factorization?\n",
    "\n",
    "Matrix factorization (MF) is the process of decomposing a large matrix into the product of two or more smaller matrices. Formally, for a matrix $ M $ of size $ m \\times n $, we seek to express it as:\n",
    "\n",
    "$$\n",
    "M \\approx U \\times V^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ U $ is an $ m \\times k $ matrix (e.g., representing latent user factors in a recommendation system).\n",
    "- $ V $ is an $ n \\times k $ matrix (e.g., representing latent item factors).\n",
    "- $ k $ is the number of **latent features**—hidden dimensions that describe the interactions between the rows and columns of $ M $.\n",
    "\n",
    "The goal is to find the matrices $ U $ and $ V $ that minimize the error in this approximation. The problem can be written as an optimization objective, such as minimizing the sum of squared differences between the observed entries in $ M $ and the predicted entries from $ U \\times V^T $:\n",
    "\n",
    "$$\n",
    "\\min_{U, V} \\sum_{(i,j) \\in \\text{Observed}} \\left( M_{ij} - U_i V_j^T \\right)^2 + \\lambda (\\| U \\|^2 + \\| V \\|^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ (i,j) \\in \\text{Observed} $ means we only sum over the observed entries in $ M $.\n",
    "- $ \\lambda $ is the regularization parameter, preventing overfitting by discouraging extreme values in $ U $ and $ V $.\n",
    "\n",
    "This formulation leads us to the **Alternating Least Squares (ALS)** and **Stochastic Gradient Descent (SGD)** algorithms, which we’ll explore in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfd3b8-9bd6-44ec-8cdc-7fcca416ff1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Matrix Factorization Works\n",
    "\n",
    "To understand why matrix factorization is effective, let’s look at a recommendation system. Imagine we have a matrix of user-item interactions where the rows represent users and the columns represent items (e.g., movies, products), and the entries of the matrix represent ratings or purchases. However, most entries are missing—most users haven’t rated most movies or bought most products.\n",
    "\n",
    "This matrix is sparse, but **matrix factorization** helps uncover **latent patterns** in the data. These latent patterns could represent users' preferences (e.g., a user’s inclination towards action or romance movies) and items’ characteristics (e.g., a movie’s genre or popularity).\n",
    "\n",
    "By factorizing this matrix, we can:\n",
    "1. **Fill in the missing entries**: By predicting the unknown interactions (ratings, purchases), we can recommend new items to users.\n",
    "2. **Identify hidden relationships**: The learned latent factors offer interpretable insights into the underlying structure of the data. For example, in movie recommendation, the factors might represent genre preferences or the intensity of emotional content in films."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d921f-2461-40cc-8a65-ff9e2d8b3091",
   "metadata": {},
   "source": [
    "## Understanding the Differences Between Matrix Factorization and Matrix Decomposition\n",
    "\n",
    "Matrix factorization and matrix decomposition are fundamental concepts in linear algebra, machine learning, and AI, and although they share similarities, they serve different purposes in practice. While both techniques involve breaking down a matrix into simpler components, the nuances between these methods are important depending on their application. \n",
    "\n",
    "Below is a detailed comparison highlighting the specific differences between matrix factorization and matrix decomposition:\n",
    "\n",
    "| **Feature**               | **Matrix Factorization**                                        | **Matrix Decomposition**                                    |\n",
    "|---------------------------|---------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Conceptual Overview**    | Factorizes a matrix into a product of smaller matrices, often low-rank approximations. Typically involves optimization algorithms to find the best factorization based on data (e.g., user-item matrices in recommendation systems). | Decomposes a matrix into specific component matrices that represent properties like orthogonality, diagonal form, or triangular form. These are exact transformations revealing intrinsic matrix properties. |\n",
    "| **Primary Goal**           | Approximates the original matrix for data-driven tasks, often for prediction or recommendation purposes. | Breaks a matrix into components to reveal theoretical properties or to simplify complex operations (e.g., solving linear systems). |\n",
    "| **Common Use Cases**       | - Recommender systems (collaborative filtering) <br> - Dimensionality reduction in large datasets <br> - Topic modeling (e.g., NMF for text data) <br> - Feature extraction in latent factor models. | - Solving systems of linear equations <br> - Principal component analysis (PCA) <br> - Eigenvalue problems <br> - Signal processing (SVD) <br> - Numerical analysis for computational efficiency. |\n",
    "| **Key Techniques**         | - **Non-negative Matrix Factorization (NMF)**: Factorizes a matrix into two non-negative matrices. <br> - **Latent Factor Models**: Used in collaborative filtering to find latent user/item features. <br> - **Alternating Least Squares (ALS)**: Optimizes matrix factorization for large datasets. | - **Singular Value Decomposition (SVD)**: Decomposes a matrix into three matrices (U, Σ, V). <br> - **LU Decomposition**: Splits a matrix into lower and upper triangular matrices. <br> - **QR Decomposition**: Factorizes a matrix into an orthogonal and an upper triangular matrix. <br> - **Eigenvalue Decomposition**: Decomposes a matrix into eigenvectors and eigenvalues. |\n",
    "| **Output**                 | Produces an **approximation** of the original matrix, typically with lower rank, enabling data compression or predictions (e.g., approximating missing ratings in a user-item matrix). | Produces **exact** decompositions, often used to expose structural properties like eigenvalues, orthogonality, or diagonal form. Useful in solving linear algebra problems. |\n",
    "| **Mathematical Nature**    | Relies on **optimization algorithms** (e.g., stochastic gradient descent, alternating least squares) to approximate the factorization. Often geared toward approximating large datasets for practical applications. | Involves **exact algebraic transformations**, focusing on mathematical properties of the matrix. The decomposition results are exact and deterministic. |\n",
    "| **Common Applications**    | - Recommender systems <br> - Feature extraction <br> - Collaborative filtering <br> - Dimensionality reduction for large-scale data | - Principal component analysis (PCA) <br> - Linear regression (QR decomposition for solving least squares) <br> - Eigenvalue problems in physics and engineering <br> - Signal and image processing (e.g., SVD for image compression) |\n",
    "| **Optimization or Exact**  | Typically involves **optimization**, meaning factorization is an approximation (especially in practical applications like AI). <br> For example, collaborative filtering often aims for low-rank factorization based on user and item interactions. | Decomposition is **exact** (e.g., SVD, LU), and the breakdown is deterministic, reflecting precise mathematical properties. |\n",
    "| **Example Algorithms**     | - **Matrix Factorization in Recommender Systems**: Factorizes user-item matrices to predict ratings or interactions. <br> - **Non-negative Matrix Factorization (NMF)**: Used in text mining, where matrices are decomposed into non-negative factors, often for topic discovery. <br> - **Latent Factor Models**: Used for uncovering hidden patterns in data. | - **Singular Value Decomposition (SVD)**: Decomposes a matrix into three matrices to reveal its rank and underlying structure. <br> - **Eigenvalue Decomposition**: Used to understand the spectral properties of a matrix (e.g., finding the directions of maximum variance). <br> - **LU Decomposition**: Decomposes a matrix for solving systems of equations by breaking it into triangular components. |\n",
    "| **Real-world Example**     | A recommender system factors a user-item rating matrix into a user-feature matrix and an item-feature matrix to predict missing ratings (e.g., Netflix movie recommendations). | Eigenvalue decomposition is used to determine the stability of dynamic systems, or SVD is used in PCA for dimensionality reduction and signal compression. |\n",
    "| **Focus on Dimensionality**| Typically focused on reducing dimensionality by approximating the original matrix with smaller matrices. Low-rank matrix approximations allow for feature compression in large datasets. | While matrix decomposition can be used for dimensionality reduction (e.g., PCA using SVD), its primary focus is not on approximation but on deriving exact mathematical properties. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec7d4d-4016-41d9-8663-af18c277092f",
   "metadata": {},
   "source": [
    "## Understanding the Math Behind Matrix Factorization and Matrix Decomposition\n",
    "\n",
    "Matrix factorization and matrix decomposition share some mathematical foundations but differ in their goals and the specific mathematical techniques used. Both involve breaking down a matrix into simpler components, but the way they approach this process varies significantly.\n",
    "\n",
    "Matrix factorization often involves **approximation** and uses **optimization techniques** to minimize reconstruction error. On the other hand, matrix decomposition is generally an **exact transformation** where the matrix is broken down into specific components that represent properties like orthogonality, diagonal structure, or eigenvalues.\n",
    "\n",
    "### 1. General Mathematical Framework\n",
    "Both methods aim to represent a matrix $ A $ in terms of simpler matrices:\n",
    "\n",
    "$$\n",
    "A = B \\times C\n",
    "$$\n",
    "\n",
    "However, matrix factorization focuses on **approximation**, while matrix decomposition aims for **exact** algebraic transformations.\n",
    "\n",
    "### 2. Matrix Factorization: Optimization and Approximation\n",
    "The math behind matrix factorization involves **optimization** to approximate the original matrix. Typically, the objective is to minimize the difference between the original matrix $ A $ and the product of matrices $ B $ and $ C $:\n",
    "\n",
    "$$\n",
    "A \\approx B \\times C\n",
    "$$\n",
    "\n",
    "The goal is to solve:\n",
    "\n",
    "$$\n",
    "\\min_{B, C} \\| A - B \\times C \\|^2\n",
    "$$\n",
    "\n",
    "This involves iterative algorithms like **stochastic gradient descent (SGD)** or **alternating least squares (ALS)**. Factorization methods often include additional constraints, such as non-negativity in **Non-negative Matrix Factorization (NMF)**.\n",
    "\n",
    "#### Example: Non-negative Matrix Factorization (NMF)\n",
    "In NMF, the goal is to factorize a matrix into non-negative matrices by minimizing reconstruction error:\n",
    "\n",
    "$$\n",
    "\\min_{B, C \\geq 0} \\| A - B \\times C \\|^2\n",
    "$$\n",
    "\n",
    "This involves solving a constrained optimization problem.\n",
    "\n",
    "### 3. Matrix Decomposition: Exact Algebraic Methods\n",
    "Matrix decomposition uses **exact algebraic transformations** without the need for optimization. The idea is to decompose a matrix into components that reveal its structural properties. A key example is **Singular Value Decomposition (SVD)**, which decomposes a matrix $ A $ as:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $ U $ and $ V $ are orthogonal matrices, and $ \\Sigma $ is a diagonal matrix containing singular values. Decomposition methods like **Eigenvalue Decomposition** also involve solving characteristic equations.\n",
    "\n",
    "#### Example: Eigenvalue Decomposition\n",
    "Eigenvalue decomposition breaks down a square matrix $ A $ into:\n",
    "\n",
    "$$\n",
    "A = V \\Lambda V^{-1}\n",
    "$$\n",
    "\n",
    "where $ \\Lambda $ is a diagonal matrix of eigenvalues, and $ V $ is the matrix of eigenvectors. This decomposition is **exact** and based on solving the equation:\n",
    "\n",
    "$$\n",
    "A v = \\lambda v\n",
    "$$\n",
    "\n",
    "### 4. Key Mathematical Differences\n",
    "\n",
    "| **Aspect**                  | **Matrix Factorization**                                | **Matrix Decomposition**                                |\n",
    "|-----------------------------|---------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Mathematical Goal**        | Approximate a matrix as a product of smaller matrices.  | Exactly decompose a matrix into specific components.     |\n",
    "| **Process**                  | Iterative optimization (minimizing a loss function).    | Algebraic transformations (solving characteristic equations, performing matrix operations). |\n",
    "| **Typical Equation**         | $ A \\approx B \\times C $ (approximation)             | $ A = U \\Sigma V^T $, $ A = V \\Lambda V^{-1} $ (exact) |\n",
    "| **Solving Method**           | Involves optimization techniques like gradient descent, alternating least squares, or other iterative methods. | Solved via exact algebraic methods like solving for eigenvalues, or applying orthogonal transformations. |\n",
    "| **Error Handling**           | Focus on minimizing the reconstruction error $ \\| A - B \\times C \\|^2 $. The output matrices are not exact but aim to minimize the difference. | No approximation error. The decomposed matrices are exact representations of the original matrix. |\n",
    "| **Example Techniques**       | Stochastic Gradient Descent (SGD), Alternating Least Squares (ALS), Non-negative Matrix Factorization (NMF). | Singular Value Decomposition (SVD), Eigenvalue Decomposition, QR Decomposition, LU Decomposition. |\n",
    "\n",
    "### 5. Similarities\n",
    "- Both methods involve breaking down a matrix into simpler components.\n",
    "- Both can reduce dimensionality, but matrix factorization typically does so through approximation, while matrix decomposition exposes exact structural properties.\n",
    "- Some decomposition methods, like **SVD**, are also used in matrix factorization to approximate low-rank matrices.\n",
    "\n",
    "### Conclusion\n",
    "- **Matrix Factorization** focuses on approximation using optimization algorithms (e.g., minimizing reconstruction error) to achieve a product of simpler matrices.\n",
    "- **Matrix Decomposition** focuses on exact transformations that reveal the intrinsic properties of a matrix (e.g., eigenvalues, singular values, or triangular forms).\n",
    "\n",
    "While both methods are related, the underlying mathematical approaches differ, with factorization relying on optimization and decomposition relying on exact algebraic transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732e4a3-4958-4352-a30c-b0d30d6e71a0",
   "metadata": {},
   "source": [
    "## Why Can SVD Be Used in Both Matrix Decomposition and Matrix Factorization?\n",
    "\n",
    "The **Singular Value Decomposition (SVD)** method can be used in both **matrix decomposition** and **matrix factorization** because it provides a precise mathematical breakdown of a matrix that is both useful for **exact transformations** (decomposition) and for **approximation tasks** (factorization). However, the way SVD is applied in each context differs in purpose and emphasis.\n",
    "\n",
    "### 1. SVD as Matrix Decomposition\n",
    "In matrix decomposition, SVD is used as an **exact algebraic tool** to decompose any matrix $ A $ (rectangular or square) into three component matrices:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ U $ is an orthogonal matrix containing the **left singular vectors**.\n",
    "- $ \\Sigma $ is a diagonal matrix containing the **singular values** of $ A $, which represent the \"strength\" of each dimension.\n",
    "- $ V^T $ is the transpose of an orthogonal matrix containing the **right singular vectors**.\n",
    "\n",
    "This decomposition is exact and exposes several key properties of the matrix:\n",
    "- **Rank**: The number of non-zero singular values.\n",
    "- **Energy**: The importance of each singular value, which helps identify how much of the original matrix is captured by each dimension.\n",
    "- **Orthogonality**: The vectors in $ U $ and $ V $ form orthonormal bases, simplifying calculations like projections and transformations.\n",
    "\n",
    "SVD, in this case, is used to understand the **structural properties** of the matrix, without approximations or optimization.\n",
    "\n",
    "### 2. SVD as Matrix Factorization\n",
    "In matrix factorization, SVD is used to **approximate** a matrix, often with a focus on **low-rank approximations**. This is particularly useful in applications like **dimensionality reduction** and **recommender systems**, where the goal is to approximate a large matrix with fewer dimensions.\n",
    "\n",
    "Here’s how SVD works in matrix factorization:\n",
    "- Instead of keeping all the singular values from $ \\Sigma $, you can approximate the original matrix $ A $ by **truncating** the singular value matrix $ \\Sigma $ and reducing the rank.\n",
    "- You select the top $ k $ largest singular values, reducing the matrix dimensions:\n",
    "\n",
    "$$\n",
    "A \\approx U_k \\Sigma_k V_k^T\n",
    "$$\n",
    "\n",
    "Where $ U_k $, $ \\Sigma_k $, and $ V_k^T $ retain only the first $ k $ singular values and corresponding vectors. The matrix $ A $ is now approximated by matrices of much smaller dimensions (rank $ k $), significantly reducing storage and computation requirements, while still retaining most of the important information from the original matrix.\n",
    "\n",
    "#### Example: SVD for Recommender Systems\n",
    "In recommender systems, the user-item interaction matrix (which may have missing values) can be factorized using SVD. The truncated version of SVD helps discover **latent factors** (e.g., preferences or patterns) that approximate user-item interactions.\n",
    "\n",
    "- **Low-rank Approximation**: Instead of using all the singular values, you use only the most significant ones to represent user preferences and item features.\n",
    "- **Prediction**: With the factorized matrices, you can predict missing entries in the matrix, such as a user's potential rating for a movie.\n",
    "\n",
    "### Key Difference: Exact vs. Approximation\n",
    "\n",
    "| **Aspect**               | **SVD in Matrix Decomposition**                               | **SVD in Matrix Factorization**                              |\n",
    "|--------------------------|---------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Purpose**               | To exactly decompose a matrix into its singular vectors and singular values, revealing its full structure. | To approximate a matrix by reducing its rank, keeping only the most significant singular values, for tasks like dimensionality reduction or collaborative filtering. |\n",
    "| **Output**                | $ A = U \\Sigma V^T $, with no loss of information.           | $ A \\approx U_k \\Sigma_k V_k^T $, with some loss of information, but focusing on the most important dimensions. |\n",
    "| **Application**           | Used in exact computations like PCA, signal processing, and solving systems of linear equations. | Used in data-driven applications like recommender systems, where the goal is to approximate a large matrix with fewer dimensions (low-rank factorization). |\n",
    "| **Dimensionality**        | Retains all dimensions (no truncation).                       | Truncates dimensions, keeping only the top $ k $ singular values. |\n",
    "| **Use of Singular Values**| All singular values are kept, revealing the matrix's full rank and structure. | Only the top singular values are kept, simplifying the matrix while preserving the most important features. |\n",
    "\n",
    "### Why Can SVD Be Used in Both?\n",
    "- **Theoretical Flexibility**: SVD is fundamentally a decomposition method, but it can also serve as an efficient tool for factorization. By truncating the singular value matrix, you switch from an exact decomposition to an approximate factorization.\n",
    "- **Low-rank Property**: In many practical applications (like machine learning), matrices can be well-approximated by low-rank versions. SVD provides an optimal way to reduce the rank while preserving as much of the important structure of the original matrix as possible.\n",
    "- **Applications in Data Science**: In data science, exactness is often less important than finding patterns, reducing dimensionality, or making predictions. SVD, as a factorization tool, can approximate a matrix and uncover latent structures (e.g., user preferences in recommender systems) without needing the full exact decomposition.\n",
    "\n",
    "### Conclusion\n",
    "SVD is versatile because it serves both purposes:\n",
    "- In **matrix decomposition**, it provides an exact breakdown of the matrix into orthogonal components.\n",
    "- In **matrix factorization**, it allows for low-rank approximations by truncating the singular value matrix, which is key in data-driven applications like recommender systems and dimensionality reduction.\n",
    "\n",
    "Thus, the mathematical properties of SVD make it powerful for both exact matrix analysis and approximate data-driven tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228fceb-467c-4447-ae42-17ae2ac836fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Algorithms for Matrix Factorization\n",
    "\n",
    "### Alternating Least Squares (ALS)\n",
    "\n",
    "**Alternating Least Squares (ALS)** is one of the most popular algorithms for matrix factorization, especially in large-scale recommendation systems. It alternates between optimizing for the user matrix $ U $ and the item matrix $ V $, holding one constant while solving for the other.\n",
    "\n",
    "#### ALS Algorithm:\n",
    "1. Initialize matrices $ U $ and $ V $ randomly.\n",
    "2. **Fix $ V $** and solve for $ U $ by minimizing the objective function:\n",
    "   $$\n",
    "   U = \\arg \\min_U \\sum_{i,j} \\left( M_{ij} - U_i V_j^T \\right)^2 + \\lambda \\| U \\|^2\n",
    "   $$\n",
    "   This is a standard least-squares problem.\n",
    "3. **Fix $ U $** and solve for $ V $ by minimizing the objective function:\n",
    "   $$\n",
    "   V = \\arg \\min_V \\sum_{i,j} \\left( M_{ij} - U_i V_j^T \\right)^2 + \\lambda \\| V \\|^2\n",
    "   $$\n",
    "4. Repeat steps 2 and 3 until convergence.\n",
    "\n",
    "#### Why ALS Works:\n",
    "- ALS leverages the fact that, when one of the matrices (either $ U $ or $ V $) is fixed, the optimization problem becomes a **linear least-squares problem**, which has an efficient solution.\n",
    "- Alternating between optimizing $ U $ and $ V $ guarantees that the objective function will decrease at every step, though it may only converge to a **local minimum**.\n",
    "\n",
    "ALS is particularly efficient when implemented in distributed computing environments (e.g., Spark’s ALS implementation), allowing it to scale to massive datasets with billions of entries.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "While ALS is effective for large-scale matrix factorization, **Stochastic Gradient Descent (SGD)** offers a more flexible and often faster approach for optimizing the factorized matrices.\n",
    "\n",
    "#### SGD Algorithm:\n",
    "1. Initialize matrices $ U $ and $ V $ randomly.\n",
    "2. For each observed entry $ M_{ij} $, compute the error:\n",
    "   $$\n",
    "   e_{ij} = M_{ij} - U_i V_j^T\n",
    "   $$\n",
    "3. Update the latent factors for user $ i $ and item $ j $ based on the error:\n",
    "   $$\n",
    "   U_i \\leftarrow U_i + \\eta (e_{ij} V_j - \\lambda U_i)\n",
    "   $$\n",
    "   $$\n",
    "   V_j \\leftarrow V_j + \\eta (e_{ij} U_i - \\lambda V_j)\n",
    "   $$\n",
    "   Where $ \\eta $ is the learning rate, and $ \\lambda $ is the regularization parameter.\n",
    "4. Repeat steps 2-3 for a fixed number of iterations or until convergence.\n",
    "\n",
    "#### Why SGD Works:\n",
    "- **Stochastic Gradient Descent** updates parameters incrementally based on each data point, which makes it well-suited for very large and sparse datasets.\n",
    "- It can converge faster than ALS when the data is sparse, but it requires careful tuning of hyperparameters like the learning rate and regularization.\n",
    "\n",
    "\n",
    "### Weighted Alternating Least Squares (WALS)\n",
    "\n",
    "In standard ALS, all observed entries are treated equally. However, in many cases, some observations are more reliable than others (e.g., user ratings may vary in confidence). **WALS** introduces **weights** that give different importance to different observations.\n",
    "\n",
    "#### WALS Objective:\n",
    "$$\n",
    "\\min_{U, V} \\sum_{(i,j)} w_{ij} \\left( M_{ij} - U_i V_j^T \\right)^2 + \\lambda (\\| U \\|^2 + \\| V \\|^2)\n",
    "$$\n",
    "Where $ w_{ij} $ is the weight for entry $ M_{ij} $, representing its importance.\n",
    "\n",
    "This method is particularly useful in recommendation systems with **implicit feedback**, such as clicks, purchases, or views, where the presence of an interaction implies user interest, but the absence does not necessarily imply disinterest.\n",
    "\n",
    "### Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "In some cases, it doesn’t make sense for the latent factors to have negative values. For example, if we’re factorizing a matrix of item sales or user clicks, it’s unrealistic for the factors to be negative. **Non-Negative Matrix Factorization (NMF)** solves this by constraining the factors to be non-negative.\n",
    "\n",
    "#### NMF Objective:\n",
    "$$\n",
    "\\min_{U, V \\geq 0} \\sum_{(i,j)} \\left( M_{ij} - U_i V_j^T \\right)^2\n",
    "$$\n",
    "\n",
    "This constraint can make the latent factors more interpretable, as they represent additive combinations of underlying features.\n",
    "\n",
    "### Probabilistic Matrix Factorization (PMF)\n",
    "\n",
    "Standard matrix factorization techniques like ALS and SGD treat the matrix factorization problem as a **deterministic** optimization task. However, in some cases, it’s beneficial to model the uncertainty in the data. **Probabilistic Matrix Factorization (PMF)** introduces a probabilistic framework, assuming that the observed matrix $ M $ is generated from a probabilistic process.\n",
    "\n",
    "#### PMF Model:\n",
    "Assume that each entry $ M_{ij} $ is generated from a Gaussian distribution with mean $ U_i V_j^T $ and variance $ \\sigma^2 $:\n",
    "$$\n",
    "P(M_{ij} | U_i, V_j) = \\mathcal{N}(M_{ij} | U_i V_j^T, \\sigma^2)\n",
    "$$\n",
    "\n",
    "The goal is to maximize the likelihood of the observed data:\n",
    "$$\n",
    "\\max_{U, V} P(M | U, V) = \\prod_{(i,j)} P(M_{ij} | U_i, V_j)\n",
    "$$\n",
    "\n",
    "PMF allows for modeling **uncertainty** in predictions and is more flexible in handling noisy or uncertain data.\n",
    "\n",
    "### Factorization Machines (FM)\n",
    "\n",
    "**Factorization Machines (FMs)** generalize matrix factorization to model interactions between all pairs of variables in the dataset, not just between users and items. This is useful when we have **contextual information** (e.g., the time of day or user demographics) that could influence the interactions.\n",
    "\n",
    "The FM model is given by:\n",
    "$$\n",
    "\\hat{y} = w_0 + \\sum_i w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x_i $ are the input features (e.g., user and item IDs).\n",
    "- $ v_i $ are the latent factors for feature $ i $.\n",
    "- $ \\langle v_i, v_j \\rangle $ is the dot product between latent factors.\n",
    "\n",
    "Factorization machines can model **complex interactions** between variables and are widely used in recommendation systems, click-through rate prediction, and advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10d900-be80-4682-bde3-81d60d0fe720",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison of Matrix Factorization Methods\n",
    "\n",
    "Here's a comparison table of the different matrix factorization techniques, along with their strengths and weaknesses:\n",
    "\n",
    "| **Method**                     | **Description**                                                                  | **Pros**                                                                 | **Cons**                                           |\n",
    "|---------------------------------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------|\n",
    "| **ALS (Alternating Least Squares)** | Alternates between optimizing user and item matrices.                            | Efficient for large datasets; easy to parallelize.                      | May converge to local minima.                      |\n",
    "| **SGD (Stochastic Gradient Descent)** | Optimizes matrix factorization using gradient descent on each observation.       | Flexible and can handle very large datasets.                            | Requires careful tuning of learning rate.          |\n",
    "| **WALS (Weighted ALS)**         | Extends ALS by assigning weights to observations based on their importance.       | Effective for handling implicit feedback.                               | More complex to implement and tune.                |\n",
    "| **NMF (Non-Negative MF)**       | Adds a non-negativity constraint to factorized matrices.                          | Produces interpretable results in certain contexts (e.g., counts).       | More computationally intensive.                    |\n",
    "| **PMF (Probabilistic MF)**      | Introduces a probabilistic framework for matrix factorization.                    | Captures uncertainty in predictions; more flexible.                     | Computationally more expensive; harder to implement.|\n",
    "| **FMs (Factorization Machines)**| Generalizes MF to model interactions between all pairs of variables in the data.  | Can handle contextual features and complex interactions.                 | More complex to train; harder to interpret.        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda0dea-8094-4aeb-90a6-0fdbc245782d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Applications of Matrix Factorization\n",
    "\n",
    "### Recommendation Systems\n",
    "\n",
    "Matrix factorization is the backbone of modern **recommendation systems**, where it helps predict user preferences for items (e.g., movies, products) they haven’t yet interacted with. Companies like Netflix, Amazon, and Spotify rely on matrix factorization to power their personalized recommendation engines.\n",
    "\n",
    "#### Case Study: Netflix Prize\n",
    "In the **Netflix Prize** competition, the goal was to predict how users would rate movies they hadn’t yet seen, based on a sparse matrix of user-movie ratings. Matrix factorization, specifically ALS, was one of the leading techniques used by participants to win the competition.\n",
    "\n",
    "### Natural Language Processing (NLP)\n",
    "\n",
    "Matrix factorization is also used in **NLP**, particularly in **latent semantic analysis (LSA)**, which is a technique for discovering relationships between words and documents. LSA uses **Singular Value Decomposition (SVD)** to factorize the term-document matrix, revealing clusters of words and documents that are related in meaning.\n",
    "\n",
    "### Computer Vision\n",
    "\n",
    "In **computer vision**, matrix factorization is used for **image compression** and **noise reduction**. By factorizing the pixel matrix of an image into low-rank components, we can compress the image while retaining its essential features, or remove noise by filtering out the smaller singular values in the factorized matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be347e-79dc-49c3-b402-e712501d2b8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## State-of-the-Art in Matrix Factorization\n",
    "\n",
    "As of 2024, **Alternating Least Squares (ALS)** remains one of the most widely used matrix factorization techniques in industry, due to its scalability and ease of parallelization. However, **Probabilistic Matrix Factorization (PMF)** and **Factorization Machines (FMs)** are increasingly popular in more complex settings where uncertainty and contextual features are important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac4cbf-b29e-4db8-8977-e9a48c05078b",
   "metadata": {},
   "source": [
    "## Other Issues\n",
    "\n",
    "### Temporal Dynamics in Matrix Factorization\n",
    "\n",
    "Matrix factorization models can become biased because they don’t account for changes in user preferences or item popularity over time. By incorporating **temporal dynamics**, we can capture these shifts, allowing the model to evolve its understanding of users and items as new interactions occur.\n",
    "\n",
    "\n",
    "Traditional matrix factorization assumes that latent factors remain static over time. However, in real-world scenarios, users’ preferences change. For instance, a user who used to prefer action movies may start watching more documentaries, or an item that was once unpopular might gain traction. Ignoring these shifts leads to inaccurate predictions.\n",
    "\n",
    "\n",
    "One effective method for capturing temporal dynamics is the **TimeSVD++** model. It extends the basic matrix factorization model by introducing time-aware biases that evolve over time. The model predicts a rating $ \\hat{r}_{ui}(t) $ as:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ui}(t) = \\mu + b_u(t) + b_i(t) + p_u^T q_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu $ is the global bias (mean rating across all users and items).\n",
    "- $ b_u(t) $ and $ b_i(t) $ are the time-dependent biases for user $ u $ and item $ i $ at time $ t $.\n",
    "- $ p_u $ and $ q_i $ are the latent factor vectors for user $ u $ and item $ i $, capturing their underlying characteristics.\n",
    "\n",
    "\n",
    "In **TimeSVD++**, both user and item biases can be made time-dependent by introducing time-based functions, such as linear functions or time bins. For example, the user bias $ b_u(t) $ can be expressed as:\n",
    "\n",
    "$$\n",
    "b_u(t) = b_u + \\alpha_u \\cdot (t - t_0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ b_u $ is the base bias for user $ u $.\n",
    "- $ \\alpha_u $ is a user-specific parameter capturing how much the user’s preferences drift over time.\n",
    "- $ t_0 $ is a reference time (e.g., the first interaction).\n",
    "\n",
    "By allowing these parameters to change over time, the model can adjust to evolving user preferences and item popularity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b0da6-f8ac-442b-90d0-fbb0db5677ff",
   "metadata": {},
   "source": [
    "### Addressing Exposure Bias in Matrix Factorization\n",
    "\n",
    "Exposure bias arises in recommendation systems when certain items are shown more frequently to users, causing the model to overestimate their true relevance. This happens when popular items are overrepresented in the data, skewing the model’s predictions.\n",
    "\n",
    "When some items (e.g., top-selling products or trending movies) dominate the dataset, matrix factorization tends to overfit these frequently interacted items, causing them to appear in recommendations even if they’re not the user’s true preference.\n",
    "\n",
    "To counteract exposure bias, we can apply **propensity scoring** to adjust the importance of interactions. The **propensity score** $ P(u, i) $ represents the probability that user $ u $ would interact with item $ i $ based on its characteristics (e.g., popularity, exposure).\n",
    "\n",
    "The weighted objective function becomes:\n",
    "\n",
    "$$\n",
    "\\min_{U, V} \\sum_{(i,j)} w_{ij} \\cdot \\left( M_{ij} - U_i V_j^T \\right)^2 + \\lambda (\\| U \\|^2 + \\| V \\|^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ w_{ij} = \\frac{1}{P(u,i)} $ is the inverse propensity score, giving less weight to frequently exposed items and more weight to underrepresented ones.\n",
    "- $ \\lambda $ is the regularization term to prevent overfitting.\n",
    "\n",
    "This ensures that popular items don’t dominate the model’s learning process, leading to more balanced recommendations.\n",
    "\n",
    "\n",
    "In a movie recommendation system, propensity scoring helps ensure that less popular films are still considered during training. For example, a widely popular blockbuster will have a higher propensity score (since many users have seen it), so its interactions will be down-weighted relative to a niche indie film.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83fb119-7e8f-4e3c-b0b1-05fbee6c761e",
   "metadata": {},
   "source": [
    "### Active Learning and Exploration in Matrix Factorization\n",
    "\n",
    "A common issue in matrix factorization is **missing data**: users interact with only a small portion of the items, leaving much of the matrix unobserved. This incomplete data can bias the model.\n",
    "\n",
    "When only popular items are frequently interacted with, the model can overfit to this subset and fail to learn about less popular items, creating a feedback loop where niche items are never recommended.\n",
    "\n",
    "To explore the matrix more effectively, we can use **active learning**. The goal is to recommend items that haven’t been frequently interacted with, helping the model learn more about them. This can be framed as a **multi-armed bandit** problem, where the model must balance exploration (recommending less-known items) and exploitation (recommending known popular items).\n",
    "\n",
    "The exploration-exploitation trade-off is captured by the **Upper Confidence Bound (UCB)** strategy:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu + b_u + b_i + p_u^T q_i + \\beta \\cdot \\sqrt{\\frac{\\log N}{N_i}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\beta $ controls the level of exploration.\n",
    "- $ N $ is the total number of recommendations made so far.\n",
    "- $ N_i $ is the number of times item $ i $ has been recommended.\n",
    "\n",
    "This formula ensures that items with fewer interactions are explored more frequently, while items with more interactions are exploited based on the current knowledge.\n",
    "\n",
    "In a shopping platform, active learning might recommend products that are less frequently viewed by the user. Over time, this exploration helps the system learn about the user’s potential interest in niche or less popular items.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375be18-c051-471a-abe1-40250ba42caf",
   "metadata": {},
   "source": [
    "## Regularization with Side Information\n",
    "\n",
    "Incorporating side information, such as user demographics or item metadata, helps reduce bias caused by limited interaction data. Regularization with these additional features prevents overfitting and improves generalization.\n",
    "\n",
    "\n",
    "Incorporating **user attributes** (e.g., age, location) and **item metadata** (e.g., genre, price) helps regularize the matrix factorization model. We modify the prediction function to account for these additional features:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu + b_u + b_i + p_u^T q_i + f(a_u, m_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ a_u $ represents user attributes.\n",
    "- $ m_i $ represents item metadata.\n",
    "- $ f(a_u, m_i) $ is a function capturing the influence of these additional features on the predicted rating.\n",
    "\n",
    "The regularization term becomes:\n",
    "\n",
    "$$\n",
    "\\lambda \\left( \\| U \\|^2 + \\| V \\|^2 + \\| W_a \\|^2 + \\| W_m \\|^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_a $ and $ W_m $ are the weights for the user attributes and item metadata, respectively.\n",
    "\n",
    "This helps guide the model to avoid overfitting and make more robust predictions.\n",
    "\n",
    "\n",
    "In a book recommendation system, user demographics (e.g., age or reading level) and item metadata (e.g., genre or author) help refine the recommendations. For example, knowing a reader’s age might lead to better suggestions based on common preferences among that demographic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a0703-34f5-49f9-bbe7-6d2af47c90b9",
   "metadata": {},
   "source": [
    "## Retraining and Continuous Learning\n",
    "\n",
    "Matrix factorization models trained on static datasets become outdated as user preferences and item characteristics evolve. Retraining the model as new interactions occur allows it to remain up-to-date.\n",
    "\n",
    "\n",
    "By incorporating **continuous learning**, the matrix factorization model can adjust its latent factors in real-time as new interactions are collected. The objective function is updated as:\n",
    "\n",
    "$$\n",
    "\\min_{U, V} \\sum_{(i,j)} \\left( M_{ij}(t) - U_i(t) V_j(t)^T \\right)^2 + \\lambda (\\| U(t) \\|^2 + \\| V(t) \\|^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ M_{ij}(t) $ is the updated interaction matrix at time $ t $.\n",
    "- $ U(t) $ and $ V(t) $ are the latent factors at time $ t $, evolving with the new data.\n",
    "\n",
    "\n",
    "The **online learning** approach updates the model incrementally as new data arrives, rather than retraining the entire model from scratch. This is particularly useful in dynamic environments where user behavior changes frequently.\n",
    "\n",
    "\n",
    "In an e-commerce platform, continuously retraining the recommendation model ensures that new products and evolving user preferences are incorporated in real-time. For example, if a user’s purchase history shifts toward eco-friendly products, the model adjusts its predictions accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0f9ae-b1ea-4d78-807c-f45c72e57aa7",
   "metadata": {},
   "source": [
    "### Combining the Approaches\n",
    "\n",
    "By combining these approaches, we can create a highly adaptive, debiased recommendation system. Each of the techniques we've discussed—temporal dynamics, propensity scoring, active learning, side information regularization, and continuous learning—addresses different forms of bias and ensures that the recommendation model remains accurate and fair over time.\n",
    "\n",
    "Here’s how we can integrate these techniques into a unified matrix factorization framework.\n",
    "\n",
    "The prediction for user $ u $ interacting with item $ i $ at time $ t $ is:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ui}(t) = \\mu + b_u(t) + b_i(t) + p_u^T q_i + f(a_u, m_i) + \\beta \\cdot \\sqrt{\\frac{\\log N}{N_i}} \\cdot w_{ij}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu $ is the global bias representing the overall average interaction.\n",
    "- $ b_u(t) $ and $ b_i(t) $ are time-dependent biases for user $ u $ and item $ i $, respectively, that evolve over time, capturing temporal shifts in user preferences and item popularity.\n",
    "- $ p_u^T q_i $ represents the interaction between user and item latent factors, capturing the hidden features that influence user-item interactions.\n",
    "- $ f(a_u, m_i) $ integrates additional user attributes $ a_u $ (e.g., demographics) and item metadata $ m_i $ (e.g., category, genre) to regularize the model.\n",
    "- $ \\beta \\cdot \\sqrt{\\frac{\\log N}{N_i}} $ introduces an exploration term inspired by **multi-armed bandit** strategies, encouraging the model to explore less-known or underrepresented items, while $ \\beta $ controls the exploration-exploitation trade-off.\n",
    "- $ w_{ij} = \\frac{1}{P(u,i)} $ is the inverse propensity score that adjusts for **exposure bias**, giving more weight to interactions that are underrepresented (less exposed) and down-weighting interactions from overexposed items.\n",
    "\n",
    "\n",
    "Let’s break down why this formulation works:\n",
    "\n",
    "1. **Temporal Dynamics**: By introducing time-dependent biases $ b_u(t) $ and $ b_i(t) $, we ensure that the model captures how user preferences and item popularity evolve. This allows the recommendation system to stay current with shifting trends and user behavior.\n",
    "\n",
    "2. **Propensity Scoring**: The inverse propensity score $ w_{ij} = \\frac{1}{P(u,i)} $ mitigates exposure bias. Popular items that are frequently exposed (and thus have a high $ P(u,i) $) are down-weighted, preventing the model from overfitting to them. Conversely, underrepresented items receive a higher weight, ensuring that the model does not ignore them.\n",
    "\n",
    "3. **Active Learning**: The exploration term $ \\beta \\cdot \\sqrt{\\frac{\\log N}{N_i}} $ ensures that the model balances exploration (trying less-known items) with exploitation (recommending items it is confident about). This is essential for filling in the gaps in sparse matrices and learning more about users’ true preferences. The inverse propensity score is applied here to ensure that exploration focuses on items that are not already overexposed.\n",
    "\n",
    "4. **Side Information**: The function $ f(a_u, m_i) $ incorporates side information, such as user demographics and item metadata, to guide the model. By using this additional context, the model becomes more robust and can generalize better, especially when interaction data is sparse.\n",
    "\n",
    "5. **Continuous Learning**: Retraining the model over time ensures that it continuously updates its latent factors and biases as new interactions are collected. This prevents the model from becoming outdated and ensures that it remains responsive to evolving user behavior and item characteristics.\n",
    "\n",
    "\n",
    "To put this combined model into practice, follow these steps:\n",
    "\n",
    "1. **Initialize the model**: Start with random latent factors for users $ U $ and items $ V $, as well as initial values for the biases and parameters associated with time, propensity, and exploration.\n",
    "\n",
    "2. **Incorporate time-dependent biases**: Introduce time-based functions for both users and items, such as $ b_u(t) = b_u + \\alpha_u \\cdot (t - t_0) $, to account for temporal shifts.\n",
    "\n",
    "3. **Apply propensity scores**: Estimate the propensity score $ P(u,i) $ for each user-item pair. Use models like logistic regression or a click-through model to estimate these scores based on item popularity or user demographics.\n",
    "\n",
    "4. **Use active learning**: Implement multi-armed bandit strategies like the **Upper Confidence Bound (UCB)** to explore underrepresented regions of the matrix. Compute the exploration term $ \\sqrt{\\frac{\\log N}{N_i}} $, and apply inverse propensity scoring to weight exploration efforts appropriately.\n",
    "\n",
    "5. **Add side information**: Incorporate user attributes $ a_u $ and item metadata $ m_i $ into the prediction function. This helps the model learn from additional features and reduces overfitting to noisy or sparse interaction data.\n",
    "\n",
    "6. **Continuously retrain the model**: As new interactions are collected, update the latent factors and biases regularly. Use online matrix factorization methods to update the model incrementally, ensuring that it remains current with real-time data.\n",
    "\n",
    "Imagine a streaming platform like Netflix that aims to provide personalized movie recommendations. By combining these techniques:\n",
    "\n",
    "- **Temporal Dynamics**: The model captures how users' preferences evolve, such as a shift from comedy to drama.\n",
    "- **Propensity Scoring**: Popular blockbusters that everyone watches don’t dominate recommendations, allowing niche films to be explored.\n",
    "- **Active Learning**: The system occasionally recommends less popular, underrepresented movies to learn more about users’ potential interests.\n",
    "- **Side Information**: The model uses user demographics (e.g., age, region) and movie metadata (e.g., genre, release year) to fine-tune its predictions.\n",
    "- **Continuous Learning**: As users continue to watch new content, the model updates its understanding of their preferences in real-time.\n",
    "\n",
    "This combined approach leads to a balanced, adaptive recommendation system that avoids bias, improves predictions, and remains responsive to new data.\n",
    "\n",
    "\n",
    "By integrating these techniques, the matrix factorization model becomes dynamic, fair, and adaptive, capturing both short-term and long-term shifts in user preferences while mitigating various forms of bias. The model evolves alongside the data, ensuring that it provides timely, accurate, and diverse recommendations.\n",
    "\n",
    "This combined framework addresses key issues in traditional matrix factorization and helps build a recommendation system that is both robust and flexible. Each technique contributes to reducing bias, improving predictions, and ensuring that the model remains responsive to new data and evolving preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e70c4-e10a-4f09-bee8-ced553931629",
   "metadata": {},
   "source": [
    "## Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e771aab0-3c97-42ff-bb56-776f9bc9546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Environment and Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For matrix factorization\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Setting a random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2ce6d-dcd0-4c75-832e-717c1be542b5",
   "metadata": {},
   "source": [
    "Prepare the Synthetic Dataset\n",
    "We will generate a synthetic user-item interaction dataset that includes:\n",
    " * Temporal Dynamics: Simulated time-based interactions.\n",
    " * Exposure Bias: Some items will be more popular and shown more frequently.\n",
    " * User and Item Attributes: Features for users and items for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e50bfe5-fc4c-484a-9043-06a9952effdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users, items, and interactions over time\n",
    "num_users = 100\n",
    "num_items = 50\n",
    "time_steps = 10\n",
    "\n",
    "# Randomly generate user latent factors\n",
    "user_factors = np.random.normal(0, 1, (num_users, 10))\n",
    "\n",
    "# Randomly generate item latent factors with some items having a higher propensity (popularity)\n",
    "item_factors = np.random.normal(0, 1, (num_items, 10))\n",
    "item_popularity = np.random.uniform(0.5, 2, num_items)  # Simulating exposure bias\n",
    "\n",
    "# Time-dependent preferences (users may change preferences over time)\n",
    "time_bias = np.random.normal(0, 0.1, (time_steps, num_users))\n",
    "\n",
    "# Creating the interaction matrix over time with bias towards popular items\n",
    "def generate_interaction_matrix(user_factors, item_factors, time_bias, item_popularity, noise_level=0.1):\n",
    "    interaction_matrix = np.zeros((num_users, num_items, time_steps))\n",
    "    for t in range(time_steps):\n",
    "        for u in range(num_users):\n",
    "            for i in range(num_items):\n",
    "                # Simulated interaction with temporal dynamics and exposure bias\n",
    "                interaction_matrix[u, i, t] = (user_factors[u] @ item_factors[i].T) + time_bias[t, u] \\\n",
    "                                              + np.log(item_popularity[i]) + np.random.normal(0, noise_level)\n",
    "    return interaction_matrix\n",
    "\n",
    "# Generate the synthetic interaction matrix\n",
    "interaction_matrix = generate_interaction_matrix(user_factors, item_factors, time_bias, item_popularity)\n",
    "\n",
    "# Flatten the matrix for processing\n",
    "interaction_df = pd.DataFrame({\n",
    "    'user': np.repeat(np.arange(num_users), num_items * time_steps),\n",
    "    'item': np.tile(np.repeat(np.arange(num_items), time_steps), num_users),\n",
    "    'time': np.tile(np.arange(time_steps), num_users * num_items),\n",
    "    'rating': interaction_matrix.flatten()\n",
    "})\n",
    "\n",
    "# Add some user attributes (e.g., age, location) and item attributes (e.g., genre)\n",
    "user_attributes = pd.DataFrame({\n",
    "    'user': np.arange(num_users),\n",
    "    'age': np.random.randint(18, 65, num_users),\n",
    "    'location': np.random.choice(['urban', 'suburban', 'rural'], num_users)\n",
    "})\n",
    "\n",
    "item_attributes = pd.DataFrame({\n",
    "    'item': np.arange(num_items),\n",
    "    'genre': np.random.choice(['action', 'comedy', 'drama'], num_items),\n",
    "    'release_year': np.random.randint(1980, 2020, num_items)\n",
    "})\n",
    "\n",
    "# Merge side information with interactions\n",
    "data = interaction_df.merge(user_attributes, on='user').merge(item_attributes, on='item')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0593b96-4074-4b1e-9069-0b3c6297f220",
   "metadata": {},
   "source": [
    "Implementing the Combined Matrix Factorization Model\n",
    "\n",
    "We will now implement the combined matrix factorization model using a modified version of SVD (Singular Value Decomposition), where we:\n",
    " * Incorporate temporal dynamics using time-based biases.\n",
    " * Apply propensity scoring to adjust for exposure bias.\n",
    " * Use active learning (Upper Confidence Bound, UCB) for exploration.\n",
    " * Integrate side information for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b065320-5d18-42be-8245-1645ca3c329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMatrixFactorization:\n",
    "    def __init__(self, num_factors, num_users, num_items, learning_rate=0.01, regularization=0.02):\n",
    "        self.num_factors = num_factors\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "\n",
    "        # Initialize latent factors\n",
    "        self.user_factors = np.random.normal(0, 1, (num_users, num_factors))\n",
    "        self.item_factors = np.random.normal(0, 1, (num_items, num_factors))\n",
    "\n",
    "        # Time-dependent biases\n",
    "        self.user_bias = np.zeros((num_users, time_steps))\n",
    "        self.item_bias = np.zeros((num_items, time_steps))\n",
    "\n",
    "        # Propensity scores for exposure bias\n",
    "        self.propensity_scores = np.random.uniform(0.5, 2, num_items)\n",
    "\n",
    "        # UCB exploration term\n",
    "        self.exploration_counts = np.zeros(num_items)\n",
    "    \n",
    "    def train(self, data, num_epochs=10):\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for index, row in data.iterrows():\n",
    "                user = int(row['user'])\n",
    "                item = int(row['item'])\n",
    "                time = int(row['time'])\n",
    "                true_rating = row['rating']\n",
    "\n",
    "                # Compute prediction\n",
    "                pred_rating = (self.user_factors[user] @ self.item_factors[item].T +\n",
    "                               self.user_bias[user, time] +\n",
    "                               self.item_bias[item, time] +\n",
    "                               np.log(self.propensity_scores[item]))\n",
    "\n",
    "                # Calculate UCB exploration term\n",
    "                ucb_exploration = np.sqrt(np.log(np.sum(self.exploration_counts) + 1) /\n",
    "                                          (self.exploration_counts[item] + 1))\n",
    "                \n",
    "                # Add exploration term to the prediction\n",
    "                pred_rating += ucb_exploration\n",
    "\n",
    "                # Calculate the error\n",
    "                error = true_rating - pred_rating\n",
    "\n",
    "                # Update latent factors with stochastic gradient descent (SGD)\n",
    "                self.user_factors[user] += self.learning_rate * (error * self.item_factors[item] -\n",
    "                                                                 self.regularization * self.user_factors[user])\n",
    "                self.item_factors[item] += self.learning_rate * (error * self.user_factors[user] -\n",
    "                                                                 self.regularization * self.item_factors[item])\n",
    "\n",
    "                # Update biases\n",
    "                self.user_bias[user, time] += self.learning_rate * (error - self.regularization * self.user_bias[user, time])\n",
    "                self.item_bias[item, time] += self.learning_rate * (error - self.regularization * self.item_bias[item, time])\n",
    "\n",
    "                # Update exploration counts\n",
    "                self.exploration_counts[item] += 1\n",
    "\n",
    "                # Track the total loss\n",
    "                total_loss += error ** 2\n",
    "\n",
    "            # Print progress\n",
    "            print(f'Epoch {epoch + 1}, Loss: {total_loss / len(data)}')\n",
    "\n",
    "    def predict(self, user, item, time):\n",
    "        pred_rating = (self.user_factors[user] @ self.item_factors[item].T +\n",
    "                       self.user_bias[user, time] +\n",
    "                       self.item_bias[item, time] +\n",
    "                       np.log(self.propensity_scores[item]))\n",
    "        # UCB exploration term\n",
    "        ucb_exploration = np.sqrt(np.log(np.sum(self.exploration_counts) + 1) /\n",
    "                                  (self.exploration_counts[item] + 1))\n",
    "        return pred_rating + ucb_exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aab1bab-839b-42a0-8cb3-fbe9dba355de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.371380709048709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.8316048082039055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.2989214002872452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.039496720079336856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.01870775835190221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.01667715566750287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.01595962168286774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.01564299834355057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.015487378312402723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.015404138081753616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for combined matrix factorization: 0.12627746252350194\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the combined matrix factorization model\n",
    "num_factors = 10\n",
    "cmf_model = CombinedMatrixFactorization(num_factors, num_users, num_items)\n",
    "cmf_model.train(train_data, num_epochs=10)\n",
    "\n",
    "# Predict ratings on test set and calculate RMSE\n",
    "preds = []\n",
    "true_ratings = []\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    user = int(row['user'])\n",
    "    item = int(row['item'])\n",
    "    time = int(row['time'])\n",
    "    true_ratings.append(row['rating'])\n",
    "    preds.append(cmf_model.predict(user, item, time))\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(true_ratings, preds))\n",
    "print(f'RMSE for combined matrix factorization: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad18d5b1-585d-4fef-9b1c-098b1d995817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for standard SVD (baseline): 0.3672481827139079\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Prepare for SVD\n",
    "# Aggregate ratings to handle duplicate entries\n",
    "train_data_aggregated = train_data.groupby(['user', 'item']).agg({'rating': 'mean'}).reset_index()\n",
    "\n",
    "# Pivot the aggregated DataFrame\n",
    "train_pivot = train_data_aggregated.pivot(index='user', columns='item', values='rating').fillna(0)\n",
    "\n",
    "# Convert the pivoted DataFrame to a sparse matrix\n",
    "train_sparse = csr_matrix(train_pivot.values)\n",
    "\n",
    "# Perform SVD on the sparse matrix\n",
    "u, sigma, vt = svds(train_sparse, k=num_factors)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "baseline_preds = np.dot(np.dot(u, sigma), vt)\n",
    "\n",
    "# Extract predictions for the test set users and items\n",
    "test_data_aggregated = test_data.groupby(['user', 'item']).agg({'rating': 'mean'}).reset_index()\n",
    "\n",
    "# Get the corresponding predictions for the test set users and items\n",
    "test_baseline_preds = [\n",
    "    baseline_preds[user, item] for user, item in zip(test_data_aggregated['user'], test_data_aggregated['item'])\n",
    "]\n",
    "\n",
    "# Calculate RMSE for the baseline SVD\n",
    "baseline_rmse = np.sqrt(mean_squared_error(test_data_aggregated['rating'], test_baseline_preds))\n",
    "print(f'RMSE for standard SVD (baseline): {baseline_rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c230b1bd-4c2c-4b33-8514-29bac32fcd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for standard SVD (using last period): 0.3830291923679267\n"
     ]
    }
   ],
   "source": [
    "# Aggregate by taking the last period's interaction for each user-item pair\n",
    "train_data_last_period = train_data.groupby(['user', 'item']).agg({'time': 'max'}).reset_index()\n",
    "train_data_last_period = train_data_last_period.merge(train_data, on=['user', 'item', 'time'])\n",
    "\n",
    "# Pivot the table to create the user-item matrix for the last period\n",
    "train_pivot_last = train_data_last_period.pivot(index='user', columns='item', values='rating').fillna(0)\n",
    "\n",
    "# Convert to sparse matrix and perform SVD\n",
    "train_sparse_last = csr_matrix(train_pivot_last.values)\n",
    "u, sigma, vt = svds(train_sparse_last, k=num_factors)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Predict ratings for the test set based on the last period\n",
    "baseline_preds_last = np.dot(np.dot(u, sigma), vt)\n",
    "\n",
    "# Compare predictions for the test set using the last period's data\n",
    "test_data_last_period = test_data.groupby(['user', 'item']).agg({'time': 'max'}).reset_index()\n",
    "test_data_last_period = test_data_last_period.merge(test_data, on=['user', 'item', 'time'])\n",
    "\n",
    "# Get corresponding predictions\n",
    "test_baseline_preds_last = [\n",
    "    baseline_preds_last[user, item] for user, item in zip(test_data_last_period['user'], test_data_last_period['item'])\n",
    "]\n",
    "\n",
    "# Calculate RMSE for the baseline using last period data\n",
    "baseline_rmse_last = np.sqrt(mean_squared_error(test_data_last_period['rating'], test_baseline_preds_last))\n",
    "print(f'RMSE for standard SVD (using last period): {baseline_rmse_last}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542ab01d-af19-4bf2-9ef0-2d11f9271b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot RMSE comparison for combined model, SVD (mean aggregation), and SVD (last period)\n",
    "methods = ['Combined Model', 'Standard SVD (Mean Aggregation)', 'Standard SVD (Last Period)']\n",
    "rmse_values = [rmse, baseline_rmse, baseline_rmse_last]\n",
    "\n",
    "plt.barh(methods, rmse_values, color=['blue', 'orange', 'green'])\n",
    "plt.xlabel('RMSE')\n",
    "plt.title('Comparison of RMSE for Combined Model vs Standard SVD (Mean) vs Standard SVD (Last Period)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c889c06-afb0-4566-b6e0-c0e560569681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}