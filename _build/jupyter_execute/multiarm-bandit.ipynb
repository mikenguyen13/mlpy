{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff2b153-0483-4abd-9703-8c0bac0609e1",
   "metadata": {},
   "source": [
    "\n",
    "Multi-armed bandits is a rich, multi-disciplinary research area which receives attention from computer science, operations research, economics and statistics. It has been studied since (Thompson, 1933), with a big\n",
    "surge of activity in the past 15-20 years\n",
    "\n",
    "\n",
    "Other sources:\n",
    "    * https://tor-lattimore.com/downloads/book/book.pdf by lattimore2020bandit by \n",
    "    * Intro to Multi-Armed Bandits slivkins2019introduction\n",
    "@book{lattimore2020bandit,\n",
    "  title={Bandit algorithms},\n",
    "  author={Lattimore, Tor and Szepesv{\\'a}ri, Csaba},\n",
    "  year={2020},\n",
    "  publisher={Cambridge University Press}\n",
    "}\n",
    "          \n",
    "@article{slivkins2019introduction,\n",
    "  title={Introduction to multi-armed bandits},\n",
    "  author={Slivkins, Aleksandrs and others},\n",
    "  journal={Foundations and Trends{\\textregistered} in Machine Learning},\n",
    "  volume={12},\n",
    "  number={1-2},\n",
    "  pages={1--286},\n",
    "  year={2019},\n",
    "  publisher={Now Publishers, Inc.}\n",
    "}\n",
    "    \n",
    "@article{thompson1933likelihood,\n",
    "  title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},\n",
    "  author={Thompson, William R},\n",
    "  journal={Biometrika},\n",
    "  volume={25},\n",
    "  number={3-4},\n",
    "  pages={285--294},\n",
    "  year={1933},\n",
    "  publisher={Oxford University Press}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adf077-feec-4841-8215-75eecb5835ff",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Problem\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Multi-Armed Bandit (MAB) problem is a classical reinforcement learning problem that exemplifies the exploration-exploitation trade-off. The goal is to maximize the cumulative reward by choosing the best arm to pull at each time step.\n",
    "\n",
    "## Formal Definition\n",
    "\n",
    "A multi-armed bandit problem can be formally defined as follows:\n",
    "- **Arms**: A set of $K$ arms, where each arm $i$ provides a reward from an unknown probability distribution $P_i$.\n",
    "- **Time Steps**: The agent interacts with the bandit over a sequence of $T$ time steps.\n",
    "- **Rewards**: At each time step $t$, the agent selects an arm $a_t$ and receives a reward $r_t$ drawn from the distribution $P_{a_t}$.\n",
    "\n",
    "The objective is to maximize the expected cumulative reward over $T$ time steps.\n",
    "\n",
    "POssible applications; \n",
    "a crowdsourcing platform can improve the assignment of tasks, workers and prices\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Exploration vs. Exploitation\n",
    "\n",
    "- **Exploration**: Trying different arms to gather information about their reward distributions.\n",
    "- **Exploitation**: Selecting the arm with the highest known reward based on the information gathered so far.\n",
    "\n",
    "Balancing these two aspects is crucial for achieving optimal performance in the MAB problem.\n",
    "\n",
    "### Regret\n",
    "\n",
    "Regret measures the difference between the cumulative reward obtained by the optimal strategy and the reward obtained by the algorithm. It is defined as:\n",
    "\n",
    "$$\n",
    "R(T) = T \\mu^* - \\sum_{t=1}^T \\mu_{a_t}\n",
    "$$\n",
    "\n",
    "where $\\mu^*$ is the mean reward of the best arm, and $\\mu_{a_t}$ is the mean reward of the arm chosen at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02c785-4326-4533-82b4-b6b1c66f90f2",
   "metadata": {},
   "source": [
    "## Strategies for Multi-Armed Bandit Problems\n",
    "\n",
    "### 1. Epsilon-Greedy\n",
    "\n",
    "The epsilon-greedy strategy is one of the simplest approaches:\n",
    "- With probability $\\epsilon$, select a random arm (exploration).\n",
    "- With probability $1 - \\epsilon$, select the arm with the highest average reward observed so far (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df11d73b-6bec-4995-8191-1bdefa511832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated values: [0.14915631 0.52210932 0.48625763 0.99539302 0.760989   0.1701503\n",
      " 0.1915776  0.34115175 0.85734105 0.05478585]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, n_arms, epsilon):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "        else:\n",
    "            return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Example usage\n",
    "n_arms = 10\n",
    "epsilon = 0.1\n",
    "n_rounds = 1000\n",
    "\n",
    "bandit = EpsilonGreedy(n_arms, epsilon)\n",
    "rewards = np.random.rand(n_arms)  # Simulated rewards for each arm\n",
    "\n",
    "for _ in range(n_rounds):\n",
    "    chosen_arm = bandit.select_arm()\n",
    "    reward = rewards[chosen_arm]\n",
    "    bandit.update(chosen_arm, reward)\n",
    "\n",
    "print(\"Estimated values:\", bandit.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9754d7-0b3d-4962-8d39-dfa62fe1e0cd",
   "metadata": {},
   "source": [
    "### Uppder Confidence Bound (UCB)\n",
    "The UCB algorithm selects the arm that maximizes the upper confidence bound of the reward estimate. For each arm $i$, the upper confidence bound is calculated as:\n",
    "$$\n",
    "UCB_i (t) = \\hat{\\mu}_i + \\sqrt{\\frac{2 \\ln t}{n_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59b06bb-96f4-4daa-b411-0362cc691cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated values: [0.14915631 0.52210932 0.48625763 0.99539302 0.760989   0.1701503\n",
      " 0.1915776  0.34115175 0.85734105 0.05478585]\n"
     ]
    }
   ],
   "source": [
    "class UCB1:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        total_counts = np.sum(self.counts)\n",
    "        if 0 in self.counts:\n",
    "            return np.argmin(self.counts)\n",
    "        else:\n",
    "            ucb_values = self.values + np.sqrt((2 * np.log(total_counts)) / self.counts)\n",
    "            return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Example usage\n",
    "bandit = UCB1(n_arms)\n",
    "\n",
    "for _ in range(n_rounds):\n",
    "    chosen_arm = bandit.select_arm()\n",
    "    reward = rewards[chosen_arm]\n",
    "    bandit.update(chosen_arm, reward)\n",
    "\n",
    "print(\"Estimated values:\", bandit.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c59109-0b5c-4b4f-99f3-35f08a50ed65",
   "metadata": {},
   "source": [
    "### Thompson Sampling\n",
    "Thompson Sampling is a Bayesian approach to the MAB problem:\n",
    "\n",
    "- Maintain a probability distribution (prior) over the possible reward distributions of each arm.\n",
    "- At each time step, sample from these distributions (posterior) and select the arm with the highest sampled value.\n",
    "- Update the posterior distributions based on the observed rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df122ec-2d22-4158-972c-1d153205e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successes: [  1.   1.   0. 965.   0.   0.   0.   1.  13.   0.]\n",
      "Failures: [1. 2. 2. 4. 2. 2. 1. 1. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "class ThompsonSampling:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.successes = np.zeros(n_arms)\n",
    "        self.failures = np.zeros(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        samples = np.random.beta(self.successes + 1, self.failures + 1)\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        if reward == 1:\n",
    "            self.successes[chosen_arm] += 1\n",
    "        else:\n",
    "            self.failures[chosen_arm] += 1\n",
    "\n",
    "# Example usage\n",
    "bandit = ThompsonSampling(n_arms)\n",
    "\n",
    "for _ in range(n_rounds):\n",
    "    chosen_arm = bandit.select_arm()\n",
    "    reward = np.random.binomial(1, rewards[chosen_arm])  # Simulated binary reward\n",
    "    bandit.update(chosen_arm, reward)\n",
    "\n",
    "print(\"Successes:\", bandit.successes)\n",
    "print(\"Failures:\", bandit.failures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3887082-555f-4c11-b573-de245a3fc845",
   "metadata": {},
   "source": [
    "## Variants of the Multi-Armed Bandit Problem\n",
    "### Contextual Bandits\n",
    "In contextual bandits, the decision-making process is conditioned on the context $x_t$ available at each time step $t$. The objective is to learn a policy that maps contexts to arms, maximizing the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21fc7c8-d8a4-40cb-931d-d9545f6b95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta values: [[-4.18811054e-02 -5.76686108e-02 -9.55660940e-02 -6.14116128e-02\n",
      "  -2.09012132e-01]\n",
      " [-2.69611909e-01 -2.78705183e-01 -1.79543465e-01 -9.26437616e-02\n",
      "  -9.71837634e-02]\n",
      " [-3.33083324e-01 -2.52651385e-01 -3.19156163e-01 -6.83125569e-01\n",
      "  -3.35290587e-01]\n",
      " [-3.00467737e-01 -1.54410670e-01 -3.40915593e-01  1.07832829e-02\n",
      "  -1.07722033e-01]\n",
      " [-2.42237296e-01  1.37255424e-01 -8.63753225e-01 -8.76779119e-03\n",
      "  -2.70387902e-01]\n",
      " [-2.81536735e-01 -1.38880891e-01 -8.68862001e-02 -8.15497878e-02\n",
      "  -3.39912484e-01]\n",
      " [-2.11081451e-01 -1.25938685e-01 -6.50009732e-02 -7.97526458e-02\n",
      "  -1.22911190e-01]\n",
      " [-2.30866679e-01 -2.33171158e-01  6.74073763e-02 -2.08244699e-01\n",
      "  -1.70848785e-01]\n",
      " [-7.31335998e-01 -1.29179557e+00  5.37027553e-01  1.46193163e+00\n",
      "   1.20663803e+00]\n",
      " [ 1.82195012e-01 -6.63277737e-03 -1.35120108e-03 -1.15987907e-02\n",
      "   8.09158509e-02]]\n"
     ]
    }
   ],
   "source": [
    "class ContextualBandit:\n",
    "    def __init__(self, n_arms, n_features):\n",
    "        self.n_arms = n_arms\n",
    "        self.n_features = n_features\n",
    "        self.beta = np.zeros((n_arms, n_features))\n",
    "    \n",
    "    def select_arm(self, context):\n",
    "        means = context @ self.beta.T\n",
    "        return np.argmax(means)\n",
    "    \n",
    "    def update(self, chosen_arm, context, reward):\n",
    "        x = context\n",
    "        self.beta[chosen_arm] += (reward - x @ self.beta[chosen_arm]) * x\n",
    "\n",
    "# Example usage\n",
    "n_features = 5\n",
    "contextual_bandit = ContextualBandit(n_arms, n_features)\n",
    "contexts = np.random.rand(n_rounds, n_features)  # Simulated contexts\n",
    "\n",
    "for t in range(n_rounds):\n",
    "    context = contexts[t]\n",
    "    chosen_arm = contextual_bandit.select_arm(context)\n",
    "    reward = rewards[chosen_arm]  # Simulated reward based on context\n",
    "    contextual_bandit.update(chosen_arm, context, reward)\n",
    "\n",
    "print(\"Estimated beta values:\", contextual_bandit.beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f69f2-1911-4af9-ad9c-6a4432bdca5d",
   "metadata": {},
   "source": [
    "### Non-Stationary Bandits\n",
    "Non-stationary bandits consider scenarios where the reward distributions of the arms change over time. Algorithms for this variant need to adapt to these changes, often incorporating mechanisms for discounting past observations or detecting change points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8216492-5e5a-47b3-96aa-466df0643393",
   "metadata": {},
   "source": [
    "### Combinatorial Bandits\n",
    "Combinatorial bandits involve selecting subsets of arms rather than a single arm at each time step. This variant is relevant in scenarios where the decision-maker must allocate resources across multiple options simultaneously.\n",
    "\n",
    "## Applications\n",
    "The multi-armed bandit problem has numerous real-world applications, including:\n",
    "\n",
    "- Online Advertising: Selecting which ads to display to maximize click-through rates.\n",
    "- Clinical Trials: Allocating treatments to patients to identify the most effective treatment.\n",
    "- Recommendation Systems: Presenting items (e.g., movies, products) to users to maximize engagement.\n",
    "\n",
    "## Conclusion\n",
    "The Multi-Armed Bandit problem is a fundamental challenge in reinforcement learning and decision-making under uncertainty. By effectively balancing exploration and exploitation, various algorithms can achieve near-optimal performance in diverse settings. Understanding and applying these strategies can lead to significant improvements in fields ranging from online advertising to clinical trials and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38873e2-067c-42d9-a726-e77c1ee0b7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}