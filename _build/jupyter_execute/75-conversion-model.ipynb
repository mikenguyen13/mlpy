{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7782ba-de89-4a66-b424-076784409e58",
   "metadata": {},
   "source": [
    "# Conversion Model in Banks\n",
    "\n",
    "## Understanding the Multiple Filters\n",
    "\n",
    "You have three main layers that determine whether someone ends up as a “final sign-up”:\n",
    "\n",
    "### Texting Filter – Who gets contacted?\n",
    "\n",
    "1. Historically, you have a “propensity to answer” (or “likely to respond”) model. Anyone above a threshold gets texted; others do not.  \n",
    "2. However, you also maintain a random subset that bypasses your propensity threshold (they get texted at random).  \n",
    "\n",
    "This yields two subpopulations:  \n",
    "$$\n",
    "T = 1 \\quad (\\text{texted, often chosen by your old model + a random group})  \n",
    "$$\n",
    "$$\n",
    "T = 0 \\quad (\\text{not texted}).  \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Response Filter – Who actually responds?\n",
    "\n",
    "Even among those texted ($T=1$), not everyone answers or applies. We observe “responded” or “applied” as a further selection step.\n",
    "\n",
    "---\n",
    "\n",
    "### Bank Acceptance Filter – Who is accepted by the bank?\n",
    "\n",
    "For those who do apply, the bank runs its own credit model. Only those who pass the bank’s credit threshold get truly onboarded, i.e., show up as “final sign-ups.”\n",
    "\n",
    "You said you can turn off your credit filter, so we’ll ignore your side’s credit cutoff. But the bank’s filter is not under your control, so it’s effectively the last gate.\n",
    "\n",
    "Thus, you only observe “final sign-up” for those who:\n",
    "\n",
    "- Are texted ($T=1$)  \n",
    "- Decide to respond/apply  \n",
    "- Pass the bank’s filter  \n",
    "\n",
    "Everyone else is either unobserved (not texted) or observed with an outcome of “did not sign up,” but that “did not sign up” might be due to not responding or due to being rejected by the bank.\n",
    "\n",
    "---\n",
    "\n",
    "## A Conceptual Multi-Stage Modeling Approach\n",
    "\n",
    "One way to tackle this is to model each stage separately, then combine. In essence:\n",
    "\n",
    "1. **Stage A:** Model the probability of being texted ($T=1$)  \n",
    "   This corrects for the fact that your older model (plus the random subset) determined who was texted.\n",
    "\n",
    "2. **Stage B:** Among those texted, model the probability of responding / applying.\n",
    "\n",
    "3. **Stage C:** Among those who respond, model the probability of being accepted by the bank.\n",
    "\n",
    "Finally, you can combine these probabilities to get an overall estimate of $\\,P(\\text{final sign-up} \\mid X)$. Specifically:\n",
    "\n",
    "$$\n",
    "P(\\text{Texted} = 1 \\mid X) \\;(\\text{Stage A}) \\; \\times \\; P(\\text{Respond} = 1 \\mid \\text{Texted} = 1, X) \\;(\\text{Stage B}) \\; \\times \\; P(\\text{BankAccept} = 1 \\mid \\text{Respond} = 1, X) \\;(\\text{Stage C}).\n",
    "$$\n",
    "\n",
    "However, in many marketing contexts, you have control over **Stage A** going forward (you can choose to text or not). So your real question might be:\n",
    "\n",
    "> “If I do text this person, what is the probability they end up a final sign-up?”\n",
    "\n",
    "Then, the formula simplifies to:\n",
    "\n",
    "$$\n",
    "P(\\text{final sign-up} \\mid \\text{Texted} = 1, X) = P(\\text{Respond} = 1 \\mid X) \\times P(\\text{BankAccept} = 1 \\mid \\text{Respond} = 1, X).\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P(\\text{Respond} = 1 \\mid X)$ (Stage B) is the “likelihood they answer/apply if texted.”  \n",
    "- $P(\\text{BankAccept} = 1 \\mid \\text{Respond} = 1, X)$ (Stage C) is the “likelihood the bank accepts them if they do apply.”\n",
    "\n",
    "---\n",
    "\n",
    "### Accounting for the Random Subset and Non-Random Subset\n",
    "\n",
    "Because your historical texting was mostly guided by a propensity model, you only see responses for the subset that got texted. This is selection bias.\n",
    "\n",
    "Fortunately, you also have a random group that was texted purely at random, bypassing your old model. **This random group is crucial** to estimate the true relationship between $X$ and “likelihood of responding,” free from the old model’s bias.\n",
    "\n",
    "Similarly, for bank acceptance, you only observe acceptance decisions for those who responded. Another selection. But you do see accept/reject outcomes among that subset, so you can approximate the bank’s credit filter from that data.\n",
    "\n",
    "---\n",
    "\n",
    "## Causal / Double-Robust Strategies\n",
    "\n",
    "To correct for these multiple selection steps, you can use a combination of:\n",
    "\n",
    "- **Inverse Probability Weighting (IPW)**\n",
    "  - Estimate the probability of each filtering step. Weight or “rebalance” the data so that it reflects the entire population.\n",
    "\n",
    "- **Heckman 2-Stage or Double-Robust (DR) Learners**\n",
    "  - Popular in econometrics or libraries like `econml`.  \n",
    "  - DR Learners can handle partial observability (some people not texted, some texted but no response, etc.) by modeling both the outcome and the selection process.  \n",
    "\n",
    "**Key**: Your random subset helps you build or validate these models, because randomization ensures at least some fraction of every type of $X$ is texted, letting you estimate the true patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Explanation of the Multi-Layer Strategy\n",
    "\n",
    "Below is how you might implement a three-model approach:\n",
    "\n",
    "1. **Model the Bank’s Acceptance (Stage C)**  \n",
    "   - **Data**: Among those who responded, you observe who got accepted vs. rejected by the bank.  \n",
    "   - **Features**: (Credit-related info, demographics, maybe bureau data if you have it).  \n",
    "   - Train a classifier $\\hat{m}_{\\text{accept}}(X)$ that predicts acceptance $\\in \\{0,1\\}$. This approximates the bank’s “proprietary” threshold.\n",
    "\n",
    "2. **Model the Probability of Responding (Stage B)**  \n",
    "   - **Data**: Among those who were texted, label “1” if they responded, “0” otherwise.  \n",
    "   - Because your old texting strategy was not purely random, incorporate the randomly texted subset to correct bias. Specifically:  \n",
    "     - Either do a propensity-score weighting for “who got texted” (to mimic a random scenario),  \n",
    "     - Or incorporate an indicator for random vs. non-random into the model.  \n",
    "   - The result: $\\hat{m}_{\\text{respond}}(X) = P(\\text{Respond}=1 \\mid \\text{Texted}=1, X)$.\n",
    "\n",
    "3. **Combine for “If Texted” Probability**  \n",
    "   $$\n",
    "   P(\\text{final sign-up} \\mid \\text{Texted}=1, X) = \\hat{m}_{\\text{respond}}(X) \\times \\hat{m}_{\\text{accept}}(X).\n",
    "   $$\n",
    "\n",
    "   This is your key final metric: if I text this person, what’s the chance they end up fully signed up?\n",
    "\n",
    "4. **Rank or Threshold**  \n",
    "   Once you have an estimate of $P(\\text{final sign-up} \\mid \\text{Texted}=1, X)$ for each potential lead, you can sort them in descending order and choose how many to text (depending on your budget or capacity).\n",
    "\n",
    "---\n",
    "\n",
    "### Where Does Causal “Uplift” Fit In?\n",
    "\n",
    "If you also want the incremental effect—i.e., the difference in outcome if text vs. not text—then you need a method that models both scenarios:\n",
    "\n",
    "$$\n",
    "\\tau(X) = P(\\text{final}=1 \\mid T=1, X) \\;-\\; P(\\text{final}=1 \\mid T=0, X).\n",
    "$$\n",
    "\n",
    "For $P(\\text{final}=1 \\mid T=0, X)$, you might have a random “no text” group for comparison.\n",
    "\n",
    "If your random subset includes both “text” and “no text” for a portion of your population, you can do an uplift model or a DR Learner from `econml` to estimate $\\tau(X)$.\n",
    "\n",
    "However, in practice, many marketing teams focus on “What is the predicted final sign-up probability if texted?” to decide who to contact.\n",
    "\n",
    "---\n",
    "\n",
    "## Putting It All Together in Python\n",
    "\n",
    "Below is a more detailed code snippet illustrating a two-step approach (respond + accept) while also correcting for non-random texting. We’ll outline the logic rather than just a minimal code—this should help you see how each filter is modeled.\n",
    "\n",
    "**Important**: This is not a copy-paste final solution. It’s a template showing how you might structure a multi-stage approach with Python libraries like scikit-learn. You will adjust data columns, hyperparameters, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Data Assumptions\n",
    "\n",
    "`df` has columns:\n",
    "\n",
    "- `texted`: 1 if the individual was texted, 0 if not.\n",
    "- `responded`: 1 if the individual applied or responded, 0 if not (only makes sense if `texted=1`).\n",
    "- `accepted`: 1 if the bank accepted them, 0 if rejected (only makes sense if `responded=1`).\n",
    "- `features...`: your input features known before deciding to text.\n",
    "- A special column `random_group` (1 if in the random texting subset, 0 otherwise) may exist. This can help you correct for your old texting model’s bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5597b959-b95a-4da4-bb42-441c2b1d8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number texted: 3040\n",
      "Number responded: 1278\n",
      "Number accepted: 678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 individuals (texted, responded, accepted, predicted final probability):\n",
      "      texted  responded  accepted  pred_final_if_texted\n",
      "1475       1          1         1                0.9114\n",
      "1155       1          1         1                0.9000\n",
      "145        1          1         1                0.8827\n",
      "3187       1          1         1                0.8640\n",
      "4458       1          1         1                0.8633\n",
      "4828       1          1         1                0.8556\n",
      "4055       1          1         1                0.8556\n",
      "770        1          1         1                0.8554\n",
      "2390       1          1         1                0.8554\n",
      "4605       1          1         1                0.8550\n",
      "\n",
      "Top decile actual acceptance rate: 0.996\n",
      "Bottom decile actual acceptance rate: 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "###############################################################################\n",
    "# 1. SIMULATE SYNTHETIC DATA\n",
    "###############################################################################\n",
    "N = 5000  # number of individuals\n",
    "\n",
    "# Generate 5 numeric features (X1 through X5)\n",
    "X1 = np.random.normal(0, 1, N)\n",
    "X2 = np.random.normal(2, 1.5, N)\n",
    "X3 = np.random.normal(-1, 2, N)\n",
    "X4 = np.random.uniform(-2, 2, N)\n",
    "X5 = np.random.exponential(1, N)\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "df = pd.DataFrame({\n",
    "    'X1': X1,\n",
    "    'X2': X2,\n",
    "    'X3': X3,\n",
    "    'X4': X4,\n",
    "    'X5': X5\n",
    "})\n",
    "\n",
    "###############################################################################\n",
    "# 1.1 Create a \"random_group\" indicator (20% of individuals)\n",
    "###############################################################################\n",
    "df['random_group'] = (np.random.rand(N) < 0.20).astype(int)\n",
    "\n",
    "###############################################################################\n",
    "# 2. SIMULATE TEXTING (Stage A)\n",
    "# -----------------------------------------------------------------------------\n",
    "# For individuals NOT in the random group, an \"old model\" decides who to text.\n",
    "# For those in the random group, we text them with a fixed 50% probability.\n",
    "###############################################################################\n",
    "def old_model_prob(x1, x2):\n",
    "    # A simple logistic function based on X1 and X2\n",
    "    return 1 / (1 + np.exp(- (0.5 * x1 + 0.3 * x2)))\n",
    "\n",
    "# Compute the old-model probability for each individual\n",
    "old_probs = old_model_prob(df['X1'], df['X2'])\n",
    "\n",
    "# Decide who is texted:\n",
    "texted = []\n",
    "for i in range(N):\n",
    "    if df.loc[i, 'random_group'] == 1:\n",
    "        # For random group: 50% chance\n",
    "        t = (np.random.rand() < 0.50)\n",
    "    else:\n",
    "        # Otherwise, follow the old model's probability\n",
    "        t = (np.random.rand() < old_probs[i])\n",
    "    texted.append(int(t))\n",
    "\n",
    "df['texted'] = texted\n",
    "\n",
    "###############################################################################\n",
    "# 3. SIMULATE RESPONSE (Stage B)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Only those who are texted can respond.\n",
    "# We simulate the response probability using features X3 and X4.\n",
    "###############################################################################\n",
    "def prob_respond(x3, x4):\n",
    "    return 1 / (1 + np.exp(- (0.4 * x3 - 0.2 * x4)))\n",
    "\n",
    "# Initialize an array for responses\n",
    "responded = np.zeros(N, dtype=int)\n",
    "for i in range(N):\n",
    "    if df.loc[i, 'texted'] == 1:\n",
    "        p = prob_respond(df.loc[i, 'X3'], df.loc[i, 'X4'])\n",
    "        # Convert the boolean outcome to int (1 if responded, 0 otherwise)\n",
    "        responded[i] = int(np.random.rand() < p)\n",
    "    else:\n",
    "        responded[i] = 0  # Cannot respond if not texted\n",
    "\n",
    "df['responded'] = responded\n",
    "\n",
    "###############################################################################\n",
    "# 4. SIMULATE BANK ACCEPTANCE (Stage C)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Among those who responded, simulate bank acceptance based on X2 and X5.\n",
    "###############################################################################\n",
    "def prob_accept(x2, x5):\n",
    "    return 1 / (1 + np.exp(- (0.3 * x2 - 0.5 * x5)))\n",
    "\n",
    "accepted = np.zeros(N, dtype=int)\n",
    "for i in range(N):\n",
    "    if df.loc[i, 'responded'] == 1:\n",
    "        p = prob_accept(df.loc[i, 'X2'], df.loc[i, 'X5'])\n",
    "        accepted[i] = int(np.random.rand() < p)\n",
    "    else:\n",
    "        accepted[i] = 0  # Cannot be accepted if not responded\n",
    "\n",
    "df['accepted'] = accepted\n",
    "\n",
    "# Print overall counts for a quick check\n",
    "print(\"Number texted:\", df['texted'].sum())\n",
    "print(\"Number responded:\", df['responded'].sum())\n",
    "print(\"Number accepted:\", df['accepted'].sum())\n",
    "\n",
    "###############################################################################\n",
    "# 5. MULTI-STAGE MODELING TO CORRECT FOR SELECTION BIAS\n",
    "###############################################################################\n",
    "# Our goal: estimate P(final sign-up | texted, X) = P(respond | texted, X) * P(accept | responded, X)\n",
    "#\n",
    "# To correct for the fact that historical texting was non-random,\n",
    "# we first model the probability of being texted using all data,\n",
    "# then use IPW to reweight the response model.\n",
    "\n",
    "# 5.1 Define the feature set (include the random_group indicator)\n",
    "feature_cols = ['X1', 'X2', 'X3', 'X4', 'X5', 'random_group']\n",
    "X_all = df[feature_cols]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Optional: Model the \"texted\" decision to compute propensity scores.\n",
    "# This approximates P(texted=1 | X) based on historical data.\n",
    "# ---------------------------------------------------------------------------\n",
    "model_texted = LogisticRegression(max_iter=1000)\n",
    "model_texted.fit(X_all, df['texted'])\n",
    "df['propensity_texted'] = model_texted.predict_proba(X_all)[:, 1]\n",
    "\n",
    "# 5.2 Model \"responded\" among those texted, using IPW to correct for non-random texting.\n",
    "df_texted = df[df['texted'] == 1].copy()\n",
    "X_texted = df_texted[feature_cols]\n",
    "y_respond = df_texted['responded']\n",
    "\n",
    "# IPW weights: w = 1 / P(texted=1 | X)\n",
    "df_texted['weight_ipw'] = 1.0 / df_texted['propensity_texted']\n",
    "\n",
    "model_respond = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model_respond.fit(X_texted, y_respond, sample_weight=df_texted['weight_ipw'])\n",
    "\n",
    "def predict_respond_if_texted(X_new):\n",
    "    return model_respond.predict_proba(X_new)[:, 1]\n",
    "\n",
    "# 5.3 Model \"accepted\" among those who responded.\n",
    "df_responded = df_texted[df_texted['responded'] == 1].copy()\n",
    "X_responded = df_responded[feature_cols]\n",
    "y_accepted = df_responded['accepted']\n",
    "\n",
    "model_accept = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model_accept.fit(X_responded, y_accepted)\n",
    "\n",
    "def predict_bank_accept(X_new):\n",
    "    return model_accept.predict_proba(X_new)[:, 1]\n",
    "\n",
    "# 5.4 Combine the two probabilities:\n",
    "# Final sign-up probability if texted = P(respond | texted, X) * P(accept | responded, X)\n",
    "def predict_final_if_texted(X_new):\n",
    "    p_resp = predict_respond_if_texted(X_new)\n",
    "    p_acc = predict_bank_accept(X_new)\n",
    "    return p_resp * p_acc\n",
    "\n",
    "df['pred_final_if_texted'] = predict_final_if_texted(df[feature_cols])\n",
    "\n",
    "###############################################################################\n",
    "# 6. RANK INDIVIDUALS BY FINAL PREDICTION\n",
    "###############################################################################\n",
    "df_ranked = df.sort_values('pred_final_if_texted', ascending=False)\n",
    "\n",
    "# Display the top 10 individuals by predicted final sign-up probability\n",
    "print(\"\\nTop 10 individuals (texted, responded, accepted, predicted final probability):\")\n",
    "print(df_ranked[['texted','responded','accepted','pred_final_if_texted']].head(10))\n",
    "\n",
    "###############################################################################\n",
    "# 7. QUICK EVALUATION: Compare Acceptance Rates in Top vs. Bottom Deciles\n",
    "###############################################################################\n",
    "decile = int(0.1 * len(df_ranked))\n",
    "top_decile = df_ranked.head(decile)\n",
    "bottom_decile = df_ranked.tail(decile)\n",
    "\n",
    "actual_conv_top = top_decile['accepted'].mean()\n",
    "actual_conv_bottom = bottom_decile['accepted'].mean()\n",
    "\n",
    "print(\"\\nTop decile actual acceptance rate: {:.3f}\".format(actual_conv_top))\n",
    "print(\"Bottom decile actual acceptance rate: {:.3f}\".format(actual_conv_bottom))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd290927-1adc-463a-9722-58205e850cb8",
   "metadata": {},
   "source": [
    "### What This Code Does\n",
    "\n",
    "- **(Optional) Stage A**: Learns how your old model decided who to text. We then compute each individual’s “propensity to be texted” to do IPW (if needed).  \n",
    "- **Stage B**: Restricts to historically texted people, trains a response model (`model_respond`), and uses IPW to partially correct for the non-random texting.  \n",
    "- **Stage C**: Among responders, trains a bank acceptance model (`model_accept`), approximating the bank’s filter.\n",
    "\n",
    "**Combine**: For any new individual, we estimate\n",
    "\n",
    "$$\n",
    "P(\\text{final} \\mid \\text{texted}=1, X) = P(\\text{respond}=1 \\mid X) \\times P(\\text{accept}=1 \\mid X).\n",
    "$$\n",
    "\n",
    "We then sort or rank by that final probability to see who is truly most likely to end up signed up if they receive a text.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Corrects the Multiple Filters\n",
    "\n",
    "- We only observe “responded=1” for `texted=1` people, so **Stage B** is conditioned on “texted=1.” We use IPW to handle that texting was not fully random.  \n",
    "- We only observe “accepted=1” for `responded=1` people, so **Stage C** is conditioned on “responded=1.” We fit a bank acceptance model on that subset.  \n",
    "\n",
    "By chaining these models, we produce an estimate of the final sign-up probability for anyone under the scenario “If we text them.”\n",
    "\n",
    "This approach, in effect, deals with each selection layer. If you had further layers (e.g., your own credit filter, or further user actions), you could add more stages similarly.\n",
    "\n",
    "---\n",
    "\n",
    "### Important Nuances\n",
    "\n",
    "#### IPW Reliance\n",
    "In Stage B, we used $1 / \\text{propensity_to_be_texted}$ as sample weights to address who historically got texted. If your random subset is large enough, you can build a robust model for “texted=1” vs. “not texted=0” that generalizes to the entire population.  \n",
    "You can refine weighting strategies (e.g., trim or cap extreme weights).\n",
    "\n",
    "#### Bank Acceptance\n",
    "We assume the bank acceptance model can be approximated from the data of people who responded. You do not see the bank’s internal process, but you observe who got accepted vs. not. This is typically enough to train a decent classifier.  \n",
    "If the bank changes its rules, you must retrain periodically.\n",
    "\n",
    "#### Response vs. Application\n",
    "“Response” might be “clicked link,” “started application,” or something similar. The key is that only a subset of texted individuals respond. That’s your second filter.  \n",
    "Make sure your labeling is consistent with your actual funnel steps.\n",
    "\n",
    "#### Random Subset\n",
    "The random subset is crucial for building an accurate stage B model. It ensures that you observe a variety of $X$-profiles who were texted, even if your old model would have excluded them.  \n",
    "If your random subset is too small, you might face high variance, but it’s still better than having no randomization.\n",
    "\n",
    "#### (Optional) One-Stage vs. Multi-Stage\n",
    "Instead of building separate models for response & acceptance, you could do a single “final outcome” model—but you’d still have to correct for the fact you only see final outcomes for texted + responded + accepted.  \n",
    "Some data scientists prefer Heckman selection or Double-Robust meta-learners (like `DRLearner` from `econml`) that attempt to unify everything. But conceptually, you still need to handle the multi-filter problem.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion (Putting It Into Practice)\n",
    "\n",
    "By explicitly modeling each layer—especially the response stage and the bank acceptance stage—you correct the missing-data problem where you only observe final sign-ups for those who got texted and responded. The random subset is the anchor that lets you handle your older non-random texting rules. Then you can produce an accurate measure:\n",
    "\n",
    "> **“If I text this person, what’s the probability they end up fully signed up (passed the bank’s filter, etc.)?”**\n",
    "\n",
    "Finally, you can rank potential leads by that probability and target the top group—thereby maximizing your conversions and dealing with the multi-layer selection bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085a387-6f41-44de-850f-39e0b8dfee06",
   "metadata": {},
   "source": [
    "## Benefits of the Random Texted Group\n",
    "\n",
    "Beyond simply serving as a way to randomly determine who gets texted (i.e., a 50% chance for those in the random group), the **random group provides several key benefits**:\n",
    "\n",
    "### Unbiased Baseline for Causal Inference\n",
    "- The random group acts as a **gold standard** because it is **not subject to the selection biases** inherent in your older targeting model.  \n",
    "- This **unbiased sample** helps you accurately estimate the **true treatment effect (uplift)** by providing a direct comparison between texted and non-texted individuals **without the confounding influence of historical selection rules**.\n",
    "\n",
    "### Improved Propensity Score Estimation\n",
    "- When you build models to estimate the **propensity of being texted** or the **likelihood of responding**, having a randomized subgroup allows you to **calibrate these models more reliably**.  \n",
    "- The random group **captures the natural variability** in responses across all segments of your population, making your **propensity score estimates (and any IPW corrections) more robust**.\n",
    "\n",
    "### Validation and Model Evaluation\n",
    "- The **random group can be used as a validation set** to check whether your models (for response, acceptance, or the overall uplift) are performing as expected.  \n",
    "- If you compare outcomes in the random group against those **predicted by your models**, you get a clearer signal of how well your models generalize to the broader population.\n",
    "\n",
    "### Enhanced Uplift Estimation\n",
    "- In **uplift modeling** (estimating the incremental effect of texting), having a **truly random assignment** in part of your dataset helps you distinguish between:\n",
    "  - The **natural propensity to convert**  \n",
    "  - The **conversion that is actually caused by receiving a text**\n",
    "- This leads to **more precise targeting**, as you can better isolate the **incremental benefit of texting** for different customer segments.\n",
    "\n",
    "### Reduced Confounding\n",
    "- Because the random group is **free from the biases** introduced by the old propensity model, it helps **reduce confounding** when you combine the data to train your causal or double robust models.  \n",
    "- This makes your estimates of **treatment effect less susceptible to bias** from unobserved factors that might influence both the **decision to text** and the **outcome**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "The **random group is not just a mechanism** to decide who gets texted randomly—it also serves as an **essential tool** for:\n",
    "\n",
    "- **Establishing a causal baseline**  \n",
    "- **Improving the accuracy and robustness of your modeling efforts**  \n",
    "- **Validating your estimated treatment effects**  \n",
    "- **Reducing confounding bias in selection**  \n",
    "- **Ensuring that targeting is based on the **true incremental impact** of texting, rather than artifacts of previous selection rules**  \n",
    "\n",
    "These benefits are **critical** for making sure that when you **target customers**, you’re **truly focusing on those who will benefit** from receiving a text—thereby improving both **conversion rates** and the **efficiency of your marketing spend**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd45d1b-f312-4ac3-94b2-6a2d94081466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}