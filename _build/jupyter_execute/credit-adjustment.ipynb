{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0faad08-5245-4aaf-ab3f-80ca53f0bbb3",
   "metadata": {},
   "source": [
    "# Credit Adjustment\n",
    "\n",
    "## Problem Breakdown\n",
    "\n",
    " * **Inputs**: Customer covariates, past financial behavior, payment history (last 6 months, customizable), payment amount, utilization rate, default history, and other relevant features.\n",
    " * **Outputs**: New credit limits and interest rates over the prediction period (3 months or longer, as specified by the user).\n",
    " * **Constraints**:\n",
    "     * Interest rate range: 22%-60%.\n",
    "     * Credit limit range: 10,000 – 500,000.\n",
    "     * Customers must be notified of an interest rate change a month in advance.\n",
    "     * Rarely decrease interest rates, only for top-performing customers.\n",
    " * **Objective**: Maximize profit, which is a combination of interest received from customers and minimizing losses from defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39fce1-51f4-4214-896c-283bbfa4def5",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "### Predicting Interest Rates and Credit Limits\n",
    "To predict both interest rates and credit limits, we’ll need a model that can handle both time-series forecasting and optimization. Here’s a suitable approach:\n",
    "\n",
    " * **Reinforcement Learning (RL)**, specifically a **Deep Q-Learning** or **Proximal Policy Optimization (PPO)** method, which is suitable for scenarios involving continuous decision-making over time):\n",
    "     * **State**: The state includes all customer data such as covariates, payment history, utilization rates, and credit behavior over time.\n",
    "     * **Action**: Adjust the interest rate or credit limit. These actions must consider the constraints (e.g., notifying a month in advance for interest rate increases or decreases).\n",
    "     * **Reward**: The reward function is the profit earned by the bank, calculated as the interest received minus default costs. This encourages the model to maximize profit while managing the risk of defaults.\n",
    "     * **Policy**: The policy learns when and how much to adjust the interest rate or credit limit based on the customer’s financial behavior and covariates.\n",
    "\n",
    "### Temporal Aspects (Handling Time Dependency)\n",
    "Because credit updates and notifications occur monthly, the model must handle time dependencies:\n",
    "\n",
    " * Recurrent Neural Networks (RNNs) or LSTMs (Long Short-Term Memory) can be used to model the sequential nature of the data (e.g., monthly updates of financial behavior and credit decisions). LSTMs are particularly useful to capture long-term dependencies (e.g., the effect of payment behavior over several months).\n",
    " * Time-series forecasting models like ARIMA can be used for predicting financial variables such as utilization rates and repayment amounts.\n",
    "\n",
    "\n",
    "### Optimization Component\n",
    "The model also needs an optimization layer to find the ideal balance between profit and risk. This can be handled through:\n",
    "\n",
    " * **Profit Function**:\n",
    "     * The profit function is based on the following factors:\n",
    "         * Interest Income: `(credit limit * interest rate * utilization rate * repayment schedule)`\n",
    "         * Cost of Default: `(default rate * credit limit)`\n",
    "     * The goal is to maximize the interest income while minimizing the cost of defaulting customers.\n",
    "     * A penalty term can be added for high-risk customers to ensure that credit limits are not too high, leading to defaults.\n",
    " * Penalty for Interest Rate Increases:\n",
    "     * To factor in the cost of increasing interest rates (which could negatively affect customer retention), you can include a penalty term in the objective function when the interest rate is increased.\n",
    "     * Interest rate decreases should be applied only in rare cases (for top-performing customers). This can be modeled as a constrained optimization problem where interest rate decreases are limited based on a threshold of past performance (e.g., high repayment rates, low utilization rates).\n",
    "\n",
    "\n",
    "Profit Maximization Function\n",
    "Your profit function is based on:\n",
    "\n",
    " * **Interest Income**: Interest Income = Credit Limit x Interest rate x Utilization Rate x Repayment Schedule\n",
    " * **Cost of Default**: Default cost = Credit Limit x Default Risk\n",
    " * **Profit**: Itnerest Income - Default Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199d219-e8a9-49ac-b97a-12cb231b3a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffa4eef0-172e-4d86-9b8c-9e9bc274d567",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Key features should be engineered to capture a customer’s financial behavior and repayment risk. These might include:\n",
    "\n",
    " * **Utilization Rate**: Ratio of credit used to the total credit limit.\n",
    " * **Repayment Behavior**: Average repayment amount over the past 6 months, delinquency counts, etc.\n",
    " * **Default Risk**: A predicted probability of default based on past behavior, financial stability, and other risk factors.\n",
    " * **Income and Employment**: These may provide insights into the customer’s repayment capacity.\n",
    " * **Historical Interest Rates**: To assess how past interest rates have influenced repayment behavior and defaults.\n",
    " * **Macro-economic Factors**: Inflation, employment rates, and other factors that might affect repayment behavior and credit demand.\n",
    " * **Behavioral Variables**: These could include the frequency of transactions, types of transactions (e.g., large vs. small purchases), and social network data (as a proxy for financial stability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b801401-6c7e-4700-8adb-800bef1d838f",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Training Data\n",
    "You will need a dataset with the following characteristics:\n",
    " * Historical payment data (e.g., credit limits, repayment amounts, interest rates).\n",
    " * Customer-level information (e.g., income, employment status, default history).\n",
    " * Macro-economic variables (e.g., inflation, GDP growth).\n",
    "### Model Architecture\n",
    "\n",
    " * **Input Layer**: Customer covariates, past financial behavior, macroeconomic variables.\n",
    " * **Hidden Layers**:\n",
    "     * LSTMs for capturing sequential patterns in customer behavior.\n",
    "     * Dense layers to learn non-linear interactions between customer features.\n",
    " * **Output Layer**: Predict the interest rate and credit limit for each customer.\n",
    " * **Reinforcement Learning Layer**: Use RL to optimize the interest rate and credit limit updates based on the bank’s profit objectives.\n",
    "\n",
    "### Loss Function\n",
    "The loss function should include:\n",
    "\n",
    " * A profit-maximization component that encourages higher interest income and lower default rates.\n",
    " * A regularization term to penalize high-risk actions (e.g., giving too much credit to a risky customer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30448a-bc26-4fbb-9fcd-a782e9a79d0c",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "You can implement this model using a framework like TensorFlow or PyTorch for deep learning and reinforcement learning. For time-series forecasting, you can integrate models like Prophet or ARIMA.\n",
    " * RL libraries such as OpenAI's Gym or Stable-Baselines can help in building the reinforcement learning component.\n",
    " * The optimization function can be implemented using SciPy’s optimization package or any modern gradient-based optimizers (e.g., Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1474275e-62e2-4069-80f3-0d7cf24c5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\mlpy\\lib\\site-packages\\gymnasium\\spaces\\box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\mlpy\\lib\\site-packages\\gymnasium\\spaces\\box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 767  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | 1.78e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8044375e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -3.58e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.36e+11      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -1.97e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.66e+11      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | 1.78e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 452           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8626451e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.6e+11       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -2.46e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.55e+11      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3.6e+03       |\n",
      "|    ep_rew_mean          | 1.78e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 459           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 17            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1490725e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.44e+11      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -1.64e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.02e+11      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 225\u001b[0m\n\u001b[0;32m    222\u001b[0m     env \u001b[38;5;241m=\u001b[39m CreditEnv(train_fold)\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# Train the model on the current fold\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# After training on all folds, perform counterfactual analysis with the holdout data and predict for 6 months\u001b[39;00m\n\u001b[0;32m    228\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m counterfactual_analysis(model, env, holdout_data, future_months\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\mlpy\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\mlpy\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\mlpy\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    221\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    222\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 224\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\mlpy\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:474\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the environment for Reinforcement Learning\n",
    "class CreditEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for predicting credit limit and interest rate, based on customer data.\n",
    "    This environment is built to maximize profit for the bank.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, n_months=6):\n",
    "        super(CreditEnv, self).__init__()\n",
    "\n",
    "        # Customer data\n",
    "        self.data = data\n",
    "        self.n_months = n_months\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Observation space: customer features (normalized between 0 and 1)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.data.drop(columns=['customer_id']).shape[1],), dtype=np.float32)\n",
    "        \n",
    "        # Action space: credit limit and interest rate (continuous values)\n",
    "        self.action_space = spaces.Box(low=np.array([10000, 22]), high=np.array([500000, 60]), dtype=np.float32)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment at the beginning of each episode.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Return observation and an empty info dictionary\n",
    "        return self.data.drop(columns=['customer_id']).iloc[self.current_step].values, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment based on the action (credit limit and interest rate).\n",
    "        \"\"\"\n",
    "        # Extract customer data for current step\n",
    "        customer = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Unpack action (credit_limit, interest_rate)\n",
    "        credit_limit, interest_rate = action\n",
    "        \n",
    "        # --- Custom profit function ---\n",
    "        utilization_rate = customer['utilization_rate']\n",
    "        repayment_schedule = customer['repayment_schedule']  # Binary: 1 for payment, 0 for no payment\n",
    "        default_risk = customer['default_risk']\n",
    "        \n",
    "        # Interest income and default cost calculation\n",
    "        interest_income = credit_limit * interest_rate * utilization_rate * repayment_schedule\n",
    "        default_cost = credit_limit * default_risk\n",
    "        \n",
    "        # Calculate profit as the reward\n",
    "        profit = interest_income - default_cost\n",
    "        \n",
    "        # --- Reward is the calculated profit ---\n",
    "        reward = profit\n",
    "        \n",
    "        # Move to the next customer in the dataset\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data)\n",
    "        \n",
    "        # Get the next observation (next customer) or reset\n",
    "        if done:\n",
    "            obs, _ = self.reset()\n",
    "        else:\n",
    "            obs = self.data.drop(columns=['customer_id']).iloc[self.current_step].values\n",
    "        \n",
    "        # Return five values: observation, reward, done, truncated (False), and info\n",
    "        return obs, reward, done, False, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Render the environment (optional for debugging).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Generate the toy data with time-varying variables\n",
    "def generate_toy_data_with_time_varying(n_obs=1000, n_months=6):\n",
    "    \"\"\"\n",
    "    Generate toy data with time-varying variables for n_obs customers over n_months.\n",
    "    \"\"\"\n",
    "    time_varying_data = {\n",
    "        'customer_id': np.repeat(np.arange(1, n_obs + 1), n_months),\n",
    "        'month': np.tile(np.arange(1, n_months + 1), n_obs),\n",
    "        'monthly_income': np.random.randint(5000, 20000, size=n_obs * n_months),\n",
    "        'credit_limit': np.random.randint(10000, 500000, size=n_obs * n_months),\n",
    "        'interest_rate': np.random.uniform(22, 60, size=n_obs * n_months),\n",
    "        'payment_history': np.random.uniform(0.5, 1.5, size=n_obs * n_months),\n",
    "        'utilization_rate': np.random.uniform(0.2, 0.95, size=n_obs * n_months),\n",
    "        'default_risk': np.random.uniform(0, 0.5, size=n_obs * n_months),\n",
    "        'repayment_amount': np.random.randint(10000, 200000, size=n_obs * n_months),\n",
    "        'repayment_schedule': np.random.choice([0, 1], size=n_obs * n_months),  # Binary repayment schedule\n",
    "        'financial_behavior_score': np.random.uniform(300, 850, size=n_obs * n_months)\n",
    "    }\n",
    "    toy_data = pd.DataFrame(time_varying_data)\n",
    "    return toy_data\n",
    "\n",
    "# Split the dataset into training, testing, and holdout samples based on customer_id\n",
    "def split_full_data_by_customer_id(data, train_size=0.6, test_size=0.2):\n",
    "    # Split customer ids to ensure all observations of each customer are in one split\n",
    "    unique_customers = data['customer_id'].unique()\n",
    "    \n",
    "    # Split into training and remaining (test + holdout)\n",
    "    train_customers, remaining_customers = train_test_split(unique_customers, train_size=train_size, random_state=42)\n",
    "    \n",
    "    # Split remaining into test and holdout\n",
    "    test_customers, holdout_customers = train_test_split(remaining_customers, test_size=test_size/(1-train_size), random_state=42)\n",
    "    \n",
    "    # Create datasets based on the customer splits\n",
    "    train_data = data[data['customer_id'].isin(train_customers)]\n",
    "    test_data = data[data['customer_id'].isin(test_customers)]\n",
    "    holdout_data = data[data['customer_id'].isin(holdout_customers)]\n",
    "    \n",
    "    return train_data, test_data, holdout_data\n",
    "\n",
    "# Function to normalize data, excluding the 'customer_id' column\n",
    "def normalize_data(data, features_to_normalize):\n",
    "    data_normalized = data.copy()\n",
    "    data_normalized[features_to_normalize] = (data[features_to_normalize] - data[features_to_normalize].min()) / (data[features_to_normalize].max() - data[features_to_normalize].min())\n",
    "    return data_normalized\n",
    "\n",
    "# Function to predict for each customer over multiple months\n",
    "def predict_for_all_customers(model, env, future_months=6):\n",
    "    \"\"\"\n",
    "    Predict credit limit and interest rate for all customers for the next 'future_months'.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Loop over each customer\n",
    "    for customer_id in env.data['customer_id'].unique():\n",
    "        obs, _ = env.reset()  # Reset for each customer\n",
    "        for month in range(future_months):\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            predictions.append({\n",
    "                'Customer ID': customer_id,\n",
    "                'Month': month + 1,\n",
    "                'Predicted Credit Limit': action[0],\n",
    "                'Predicted Interest Rate': action[1],\n",
    "                'Profit (Reward)': reward\n",
    "            })\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "# Function for counterfactual analysis\n",
    "def counterfactual_analysis(model, env, holdout_data, future_months=6):\n",
    "    \"\"\"\n",
    "    Perform counterfactual analysis by comparing predicted values with actual values from the holdout data.\n",
    "    \"\"\"\n",
    "    predicted_df = predict_for_all_customers(model, env, future_months=future_months)\n",
    "    \n",
    "    # Get the true values from the holdout dataset\n",
    "    true_values = holdout_data[['customer_id', 'credit_limit', 'interest_rate']]\n",
    "    \n",
    "    # Merge predicted and actual values\n",
    "    comparison_df = pd.merge(predicted_df, true_values, left_on='Customer ID', right_on='customer_id', how='left')\n",
    "    \n",
    "    # Calculate profit difference and drop NaN values (handle missing values)\n",
    "    comparison_df['Profit Difference'] = comparison_df['Profit (Reward)'] - (\n",
    "        comparison_df['credit_limit'] * comparison_df['interest_rate'] * \n",
    "        holdout_data['utilization_rate'] * holdout_data['repayment_schedule'] - \n",
    "        comparison_df['credit_limit'] * holdout_data['default_risk']\n",
    "    )\n",
    "    \n",
    "    # Drop rows with NaN values in 'Profit Difference'\n",
    "    comparison_df = comparison_df.dropna(subset=['Profit Difference'])\n",
    "    \n",
    "    # Plot the profit differences\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(comparison_df['Profit Difference'], bins=50, alpha=0.7, color='blue', label='Profit Difference')\n",
    "    plt.title('Profit Difference Between Predicted and Actual')\n",
    "    plt.xlabel('Profit Difference')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Generate toy data\n",
    "n_obs = 1000\n",
    "n_months = 6\n",
    "data = generate_toy_data_with_time_varying(n_obs=n_obs, n_months=n_months)\n",
    "\n",
    "# Split into training, testing, and holdout sets based on customer_id\n",
    "train_data, test_data, holdout_data = split_full_data_by_customer_id(data)\n",
    "\n",
    "# Select relevant features for RL environment, including 'customer_id'\n",
    "features = ['customer_id', 'monthly_income', 'payment_history', 'utilization_rate', 'default_risk',\n",
    "            'repayment_amount', 'repayment_schedule', 'financial_behavior_score']\n",
    "train_for_rl = train_data[features]\n",
    "\n",
    "# Normalize the data for RL environment, except for 'customer_id'\n",
    "features_to_normalize = train_for_rl.columns.drop('customer_id')\n",
    "train_for_rl = normalize_data(train_for_rl, features_to_normalize)\n",
    "\n",
    "# Create the environment for training\n",
    "env = CreditEnv(train_for_rl)\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(train_for_rl['customer_id'].unique()):\n",
    "    train_customers = train_for_rl['customer_id'].unique()[train_index]\n",
    "    test_customers = train_for_rl['customer_id'].unique()[test_index]\n",
    "    \n",
    "    # Create training and test folds based on customer_id\n",
    "    train_fold = train_for_rl[train_for_rl['customer_id'].isin(train_customers)]\n",
    "    test_fold = train_for_rl[train_for_rl['customer_id'].isin(test_customers)]\n",
    "    \n",
    "    # Create environment for this training fold\n",
    "    env = CreditEnv(train_fold)\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "# After training on all folds, perform counterfactual analysis with the holdout data and predict for 6 months\n",
    "comparison_df = counterfactual_analysis(model, env, holdout_data, future_months=6)\n",
    "\n",
    "# Display the comparison DataFrame with predicted and actual values\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5de16c-188d-4018-898c-00aed21aff1f",
   "metadata": {},
   "source": [
    "1. Soft Actor-Critic (SAC)\n",
    "Why: SAC is a robust and efficient model for continuous action spaces, and your problem involves continuous variables like credit limits and interest rates. SAC can handle the continuous nature of your objectives while optimizing for a balance between exploration and exploitation using entropy maximization.\n",
    "How: You can define the credit limit and interest rate as continuous actions and use the reward function to optimize for profit, factoring in credit risk, default rates, and customer retention. You can also incorporate penalty mechanisms for defaults or higher risk.\n",
    "2. Proximal Policy Optimization (PPO)\n",
    "Why: PPO is a popular choice for stability and ease of use in multi-objective tasks. It can handle continuous prediction tasks like yours while ensuring policy updates remain stable.\n",
    "How: You could model credit limit and interest rate predictions as two separate outputs within the same policy. The reward function would encourage balancing risk, maximizing profit, and managing default probabilities.\n",
    "3. Multi-Objective Reinforcement Learning (MORL) with SAC or PPO\n",
    "Why: Since you have multiple objectives (profit maximization and minimizing risk of default), a multi-objective approach with SAC or PPO can be adapted to handle both objectives simultaneously. This approach allows you to balance the competing trade-offs, for example, between higher credit limits (higher potential profit but also higher risk) and lower interest rates (to retain customers but maintain profitability).\n",
    "How: The reward function could use a weighted sum of objectives or a Pareto front approach to optimize both the credit limit and interest rate simultaneously. This would allow you to manage trade-offs effectively.\n",
    "4. Hierarchical Reinforcement Learning (HRL)\n",
    "Why: In your case, predicting credit limits and interest rates involves multiple decisions over time (both short-term and long-term). HRL can model this by breaking down tasks into sub-tasks: determining an optimal credit limit first, then optimizing the interest rate based on that decision.\n",
    "How: HRL can handle your task by learning higher-level decisions (credit limit) and lower-level decisions (interest rate) hierarchically. This approach would allow you to optimize for each decision level while considering long-term profit and default risk.\n",
    "5. Multi-Agent Reinforcement Learning (MARL)\n",
    "Why: If you want to model each customer as an agent with individual characteristics and optimize for each agent’s credit limit and interest rate, MARL might be a good approach. It allows for modeling interactions between agents (e.g., customers with different risk profiles).\n",
    "How: Each customer (or agent) could have their own policy determining credit limits and interest rates, while the global system optimizes overall profit and risk at the portfolio level.\n",
    "\n",
    "\n",
    "Recommendation\n",
    "\n",
    "Soft Actor-Critic (SAC) or PPO with a multi-objective reward function is likely the best fit for your use case. These models are well-suited for continuous control problems and can handle both the credit limit and interest rate predictions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80e094-e51c-4e04-a7e4-de115f50d49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}