{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e581b6",
   "metadata": {},
   "source": [
    "# Credit Adjustment\n",
    "\n",
    "Now, we'll use reinforcement learning (RL) to dynamically adjust credit limits and interest rates over time based on repayment behavior.\n",
    "\n",
    "2.1. Data Structure for RL\n",
    "You will need:\n",
    " * State: Customer features and repayment history.\n",
    " * Actions: Adjusting credit limit and interest rate.\n",
    " * Reward: Profit (as described earlier).\n",
    "\n",
    "2.2. Deep Q-Learning (DQN) Agent\n",
    "\n",
    "You can now use Deep Q-Learning (DQN) to train the RL agent to adjust credit limits and interest rates. A neural network is used as the Q-function approximator to estimate the reward for each state-action pair.\n",
    "\n",
    "1. Setting up DQN with Stable Baselines3\n",
    "To train a DQN agent to dynamically adjust credit limits and interest rates based on customer repayment behavior, we need to wrap our custom environment (CreditCardEnv) and set up the DQN algorithm. Here's the continuation:\n",
    "\n",
    "```\n",
    "# Install necessary package for RL (Stable Baselines3)\n",
    "!pip install stable-baselines3\n",
    "\n",
    "# Import necessary packages\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.envs import DummyVecEnv\n",
    "\n",
    "# Define the environment (Credit Card Environment)\n",
    "class CreditCardEnv:\n",
    "    def __init__(self, customer_features, repayment_history, max_credit_limit=20000):\n",
    "        self.customer_features = customer_features\n",
    "        self.repayment_history = repayment_history\n",
    "        self.max_credit_limit = max_credit_limit\n",
    "        self.credit_limit = 0\n",
    "        self.interest_rate = 0.0\n",
    "        self.time_step = 0\n",
    "        self.max_time_step = 24  # Simulate 24 months\n",
    "        \n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        self.credit_limit = random.randint(1000, self.max_credit_limit)\n",
    "        self.interest_rate = random.uniform(0.05, 0.25)\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        return np.array([self.customer_features, self.repayment_history, self.credit_limit, self.interest_rate])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Action could be adjusting the credit limit or interest rate.\n",
    "        Simulate repayment behavior based on action.\n",
    "        \"\"\"\n",
    "        self.time_step += 1\n",
    "        \n",
    "        # Simulate repayment outcome (use random behavior for now)\n",
    "        repayment_made = np.random.choice([0, 1], p=[0.2, 0.8])  # 80% chance of repayment\n",
    "        \n",
    "        # Calculate reward (profit)\n",
    "        revenue = self.credit_limit * self.interest_rate * repayment_made\n",
    "        cost = self.credit_limit * (1 - repayment_made)\n",
    "        profit = revenue - cost\n",
    "        \n",
    "        # Adjust the credit limit or interest rate based on the action\n",
    "        # (Simplify action space for now, e.g., action=0 means no change, action=1 means increase interest rate)\n",
    "        if action == 1:\n",
    "            self.interest_rate = min(self.interest_rate + 0.01, 0.25)\n",
    "        elif action == 2:\n",
    "            self.credit_limit = min(self.credit_limit + 1000, self.max_credit_limit)\n",
    "        \n",
    "        # Done if we've reached the final month\n",
    "        done = self.time_step >= self.max_time_step\n",
    "        \n",
    "        return self._get_state(), profit, done\n",
    "    \n",
    "# Initialize environment with some sample data\n",
    "env = CreditCardEnv(customer_features=np.random.rand(5), repayment_history=[1, 1, 0])\n",
    "\n",
    "# Wrap the environment in a vectorized environment to support parallel simulation\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Initialize the DQN agent\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"credit_card_dqn_model\")\n",
    "\n",
    "# Load the model to use later\n",
    "model = DQN.load(\"credit_card_dqn_model\")\n",
    "\n",
    "# Test the trained agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n",
    "```\n",
    "\n",
    "Explanation of the Code:\n",
    "1. Environment: We use the `CreditCardEnv` to simulate a real-world scenario of dynamically adjusting credit limits and interest rates based on customer repayment behavior.\n",
    "2. Actions: Actions can include increasing interest rates or adjusting the credit limit, and the agent will learn to take these actions to maximize long-term profit.\n",
    "3. Reward: The reward is calculated as the **profit**, which is the difference between the revenue from repayments and the cost of defaults.\n",
    "4. DQN Agent: We use the `DQN` algorithm from `Stable Baselines3`, which is a Deep Q-Learning agent, to learn an optimal policy for maximizing the reward (profit) over multiple time steps (e.g., 24 months).\n",
    "\n",
    "Next Steps for RL\n",
    "\n",
    "1. Tune Hyperparameters: You can experiment with different learning rates, exploration/exploitation rates, and batch sizes to improve the performance of the RL agent.\n",
    "2. Action Space: Expand the action space to allow more granular adjustments to credit limits and interest rates. For example, you can allow both increments and decrements for interest rates and credit limits.\n",
    "3. Advanced RL Algorithms: If you want to explore more advanced RL techniques, you can try algorithms like DDPG (Deep Deterministic Policy Gradient) for continuous action spaces or PPO (Proximal Policy Optimization)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}