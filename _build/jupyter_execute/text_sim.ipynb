{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185c1dea-b5a8-4c73-99df-d937fc072021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers transformers torch scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c920f44-ae67-4b9a-8051-2cf366b2ea10",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sentence-BERT (SBERT)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize SBERT model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# Sentence-BERT (SBERT)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight and efficient model\n",
    "# Your industry description\n",
    "industry_text = \"industry data warehouse and data for businesses\"\n",
    "\n",
    "# Texts from different websites\n",
    "website_texts = [\n",
    "    \"We are the best business data seller in the country\",\n",
    "    \"We can give you the best warehouse for you in the country\",\n",
    "    \"Look no further for your warehouse needs\",\n",
    "    # Add more website texts\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "industry_embedding_sbert = sbert_model.encode(industry_text)\n",
    "website_embeddings_sbert = sbert_model.encode(website_texts)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarities\n",
    "similarities_sbert = cosine_similarity([industry_embedding_sbert], website_embeddings_sbert)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48ca60a-725c-43a5-839e-61b3696283ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miken\\anaconda3\\envs\\mlpy\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LaBSE (Language-agnostic BERT Sentence Embedding)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize LaBSE model\n",
    "labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "# Generate embeddings\n",
    "industry_embedding_labse = labse_model.encode(industry_text)\n",
    "website_embeddings_labse = labse_model.encode(website_texts)\n",
    "# Compute similarities\n",
    "similarities_labse = cosine_similarity([industry_embedding_labse], website_embeddings_labse)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc7c833-97df-44d2-834b-a3171af7f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with Cosine Similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine all texts for fitting the vectorizer\n",
    "all_texts = [industry_text] + website_texts\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "# Industry vector is the first vector\n",
    "industry_vector_tfidf = tfidf_matrix[0]\n",
    "\n",
    "# Website vectors\n",
    "website_vectors_tfidf = tfidf_matrix[1:]\n",
    "# Compute cosine similarities\n",
    "similarities_tfidf = cosine_similarity(industry_vector_tfidf, website_vectors_tfidf)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3432ccee-1d3a-4610-8a5b-302f3f5d468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Website  SBERT Similarity  LaBSE Similarity  TF-IDF Similarity\n",
      "0  Website 1          0.555404          0.442938           0.285859\n",
      "1  Website 2          0.418224          0.291702           0.143347\n",
      "2  Website 3          0.518358          0.312297           0.118696\n"
     ]
    }
   ],
   "source": [
    "# Comparing the Methods\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store similarity scores\n",
    "df_results = pd.DataFrame({\n",
    "    'Website': [f\"Website {i+1}\" for i in range(len(website_texts))],\n",
    "    'SBERT Similarity': similarities_sbert,\n",
    "    'LaBSE Similarity': similarities_labse,\n",
    "    'TF-IDF Similarity': similarities_tfidf\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170dc716-c0e8-4c9d-bf11-689fbebfb583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Matches per Method:\n",
      "SBERT: Website 1 with similarity 0.5554\n",
      "LaBSE: Website 1 with similarity 0.4429\n",
      "TF-IDF: Website 1 with similarity 0.2859\n",
      "\n",
      "Consensus on Best Match:\n",
      "Website 1    3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find the best match for each method\n",
    "best_sbert = df_results.loc[df_results['SBERT Similarity'].idxmax()]\n",
    "best_labse = df_results.loc[df_results['LaBSE Similarity'].idxmax()]\n",
    "best_tfidf = df_results.loc[df_results['TF-IDF Similarity'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Matches per Method:\")\n",
    "print(f\"SBERT: {best_sbert['Website']} with similarity {best_sbert['SBERT Similarity']:.4f}\")\n",
    "print(f\"LaBSE: {best_labse['Website']} with similarity {best_labse['LaBSE Similarity']:.4f}\")\n",
    "print(f\"TF-IDF: {best_tfidf['Website']} with similarity {best_tfidf['TF-IDF Similarity']:.4f}\")\n",
    "# Count the number of methods that selected each website as the best match\n",
    "best_matches = df_results[['SBERT Similarity', 'LaBSE Similarity', 'TF-IDF Similarity']].idxmax()\n",
    "\n",
    "# Map index to website names\n",
    "best_matches = best_matches.map(lambda x: df_results.loc[x, 'Website'])\n",
    "\n",
    "# Count occurrences\n",
    "consensus = best_matches.value_counts()\n",
    "\n",
    "print(\"\\nConsensus on Best Match:\")\n",
    "print(consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11937975-fe4e-4c96-ba7f-aaf479092e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}