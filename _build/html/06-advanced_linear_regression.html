
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Advanced Linear Regression Techniques &#8212; Machine Learning in Python</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=1a96265c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=c72506b3" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=1a96265c" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/styles/bootstrap.css?v=5340d9b1" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/styles/theme.css?v=a243ae73" />
    <link rel="stylesheet" type="text/css" href="_static/vendor/fontawesome/6.5.1/css/all.min.css?v=c786f70d" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-NK1GQ8CXSN"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-NK1GQ8CXSN');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-NK1GQ8CXSN');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '06-advanced_linear_regression';</script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/copybutton_funcs.js?v=776a791e"></script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/language_data.js?v=e49ba422"></script>
    <script src="_static/searchtools.js?v=d19c4805"></script>
    <script src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/scripts/bootstrap.js?v=3d67b3b1"></script>
    <script src="_static/scripts/pydata-sphinx-theme.js?v=b2908668"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?v=95180707"></script>
    <link rel="canonical" href="https://mikenguyen13.github.io/mlpy/06-advanced_linear_regression.html" />
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="07-regularized.html" />
    <link rel="prev" title="Supervised Machine Learning" href="05-supervised-ml.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Machine Learning in Python - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Machine Learning in Python - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction to Machine Learning and Artificial Intelligence
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Theory</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05-supervised-ml.html">Supervised Machine Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Advanced Linear Regression Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-optimization.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-matrix_factorization.html">Matrix Factorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="40-data-masking.html">Data Masking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Industry Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="50-credit-score.html">Credit Score Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="51-min_risk.html">Risk Minimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="51-min_risk_amount.html">Risk Minimization with Overdue Balance</a></li>
<li class="toctree-l1"><a class="reference internal" href="52-credit-approval.html">Credit Approval</a></li>
<li class="toctree-l1"><a class="reference internal" href="53-credit-adjustment.html">Credit Adjustment</a></li>
<li class="toctree-l1"><a class="reference internal" href="54-firm-valuation.html">Firm Valuation</a></li>
<li class="toctree-l1"><a class="reference internal" href="55-financial_fraud.html">Financial Fraud Detection</a></li>



<li class="toctree-l1"><a class="reference internal" href="70-approximate-nearest-neighbors.html">Approximate Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="71-sim_dat_bandits.html">Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="72-data-anoy-geo.html">Data Anonymization Techniques for Geospatial Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="73-sample-splitting-time-series.html">Split Samples in Time Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="74-leads-allocation.html">Maximizing Profits with a Simple Integer Linear Program in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="75-conversion-model.html">Conversion Model in Banks</a></li>
<li class="toctree-l1"><a class="reference internal" href="76-two-sided-matching.html">Two-Sided Matching &amp; Ranking for Credit Cards</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/mikenguyen13/mlpy" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/mikenguyen13/mlpy/edit/main/06-advanced_linear_regression.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/mikenguyen13/mlpy/issues/new?title=Issue%20on%20page%20%2F06-advanced_linear_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/06-advanced_linear_regression.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Advanced Linear Regression Techniques</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-linear-regression">Basic Linear Regression <a id="Basic-Linear-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-basic-linear-regression">Limitations of Basic Linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-regularized-linear-regression">Introduction to Regularized Linear Regression <a id="Introduction"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-methods-overview">Regularization Methods Overview</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">Ridge Regression <a id="Ridge-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-and-properties">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-tradeoff">Bias-Variance Tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-and-cons">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pros">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cons">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-implementation">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">Lasso Regression <a id="Lasso-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Bias-Variance Tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elastic-net">Elastic Net <a id="Elastic-Net"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-strengths-of-ridge-and-lasso">Combining Strengths of Ridge and Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-angle-regression-lars">Least Angle Regression (LARS) <a id="Least-Angle-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-characteristics">Key Characteristics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-lasso">Adaptive Lasso <a id="Adaptive-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-adaptive-lasso">Characteristics of Adaptive Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fused-lasso">Fused Lasso <a id="Fused-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-fused-lasso">Characteristics of Fused Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#group-lasso">Group Lasso <a id="Group-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id38">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-group-lasso">Characteristics of Group Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-group-lasso">Sparse Group Lasso <a id="Sparse-Group-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id44">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id45">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-sparse-group-lasso">Characteristics of Sparse Group Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id46">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id47">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id48">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id49">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#total-variation-regularization-tv-l1">Total Variation Regularization (TV-L1) <a id="TV-L1"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id51">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id52">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-tv-l1">Characteristics of TV-L1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id54">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id55">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id56">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id57">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothly-clipped-absolute-deviation-scad">Smoothly Clipped Absolute Deviation (SCAD) <a id="SCAD"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id58">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id59">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-scad">Characteristics of SCAD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id61">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id63">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id64">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimax-concave-penalty-mcp">Minimax Concave Penalty (MCP) <a id="MCP"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id65">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id66">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-mcp">Characteristics of MCP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id67">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id68">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id69">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id70">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id71">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-regularized-regression-techniques">Comparison of Regularized Regression Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-table">Overview Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id72">Bias-Variance Tradeoff <a id="Theoretical-Insights"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-considerations">Practical Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-regularization">Challenges in Regularization <a id="Challenges"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#over-regularization">Over-Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-of-hyperparameters">Selection of Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretability">Interpretability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-features">Correlated Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-regularization-parameters">Tuning Regularization Parameters <a id="Tuning-Parameters"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-search">Random Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation-techniques">Cross-Validation Techniques</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="advanced-linear-regression-techniques">
<h1>Advanced Linear Regression Techniques<a class="headerlink" href="#advanced-linear-regression-techniques" title="Link to this heading">#</a></h1>
<p>Linear regression serves as the foundation for many predictive modeling techniques in supervised machine learning. While the basic linear regression model is powerful, it often suffers from limitations such as overfitting, especially when dealing with high-dimensional data or multicollinearity among features. To address these challenges, various regularization methods have been developed, expanding the family of linear regression into more robust and flexible models. This chapter explores these advances.</p>
<section id="basic-linear-regression">
<h2>Basic Linear Regression <a id="Basic-Linear-Regression"></a><a class="headerlink" href="#basic-linear-regression" title="Link to this heading">#</a></h2>
<p>Linear regression is a fundamental technique in statistics and machine learning for modeling the relationship between a dependent variable (<span class="math notranslate nohighlight">\(y\)</span>) and one or more independent variables (<span class="math notranslate nohighlight">\(X\)</span>). The goal is to fit a linear equation of the form:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the dependent variable (what you’re trying to predict),</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1, X_2, \dots, X_p\)</span> are the independent variables (features),</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept,</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1, \dots, \beta_p\)</span> are the coefficients,</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is the error term.</p></li>
</ul>
<p>The coefficients (<span class="math notranslate nohighlight">\(\beta\)</span>) are estimated by minimizing the residual sum of squares (RSS) between the observed and predicted values:</p>
<div class="math notranslate nohighlight">
\[
RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<section id="limitations-of-basic-linear-regression">
<h3>Limitations of Basic Linear Regression<a class="headerlink" href="#limitations-of-basic-linear-regression" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Overfitting</strong>: When the model captures noise or variance in the training data, leading to poor generalization on new data.</p></li>
<li><p><strong>Multicollinearity</strong>: Occurs when independent variables are highly correlated, leading to unstable estimates of coefficients.</p></li>
<li><p><strong>High Dimensionality</strong>: When the number of features is large, basic linear regression models may become too complex, resulting in overfitting.</p></li>
</ol>
<p>To overcome these limitations, regularization techniques like Ridge Regression, Lasso Regression, and Elastic Net are applied.</p>
</section>
</section>
<hr class="docutils" />
<section id="introduction-to-regularized-linear-regression">
<h2>Introduction to Regularized Linear Regression <a id="Introduction"></a><a class="headerlink" href="#introduction-to-regularized-linear-regression" title="Link to this heading">#</a></h2>
<p>Regularization techniques enhance the basic linear regression model by adding a penalty term to the loss function, which discourages complex models and mitigates overfitting. These methods introduce additional constraints or modify the objective function to balance the trade-off between bias and variance.</p>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Overfitting</strong>: When a model captures noise in the training data, leading to poor generalization on unseen data.</p></li>
<li><p><strong>Regularization</strong>: Techniques that impose penalties on model coefficients to prevent overfitting.</p></li>
<li><p><strong>Multicollinearity</strong>: A situation where predictor variables are highly correlated, leading to unstable coefficient estimates.</p></li>
</ul>
</section>
<section id="regularization-methods-overview">
<h3>Regularization Methods Overview<a class="headerlink" href="#regularization-methods-overview" title="Link to this heading">#</a></h3>
<p>Regularization methods can be broadly categorized based on the type of penalty they introduce:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\( L_2 \)</span> Penalty (Ridge Regression)</strong></p></li>
<li><p><strong><span class="math notranslate nohighlight">\( L_1 \)</span> Penalty (Lasso Regression)</strong></p></li>
<li><p><strong>Combination of <span class="math notranslate nohighlight">\( L_1 \)</span> and <span class="math notranslate nohighlight">\( L_2 \)</span> Penalties (Elastic Net)</strong></p></li>
<li><p><strong>Other Variants (e.g., Fused Lasso, Group Lasso)</strong></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="ridge-regression">
<h2>Ridge Regression <a id="Ridge-Regression"></a><a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p>Ridge Regression, also known as Tikhonov regularization, addresses multicollinearity by adding an <span class="math notranslate nohighlight">\( L_2 \)</span> penalty to the loss function. This penalty term shrinks the coefficients towards zero but does not set any of them exactly to zero, thus retaining all features in the model.</p>
<p>The objective function in Ridge Regression is modified as follows:</p>
<div class="math notranslate nohighlight">
\[
RSS_{ridge} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter that controls the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( \sum_{j=1}^{p} \beta_j^2 \)</span> is the <span class="math notranslate nohighlight">\( L_2 \)</span> penalty (sum of the squared coefficients).</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the actual target value.</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> is the predicted value from the model.</p></li>
</ul>
<p>As <span class="math notranslate nohighlight">\( \lambda \)</span> increases, the penalty on the coefficients increases, forcing them to be smaller and reducing the variance of the model, thereby addressing overfitting. Ridge regression is especially useful in cases of multicollinearity, where predictor variables are highly correlated.</p>
<section id="mathematical-formulation">
<h3>Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h3>
<p>The Ridge Regression optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter controlling the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> is the intercept term.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\( L_2 \)</span> penalty term, <span class="math notranslate nohighlight">\( \lambda \sum_{j=1}^{p} \beta_j^2 \)</span>, ensures that large coefficients are penalized, leading to more stable and generalizable models. Unlike Lasso regression, Ridge Regression retains all variables in the model by shrinking the coefficients but not setting them to zero.</p>
</section>
<section id="characteristics-and-properties">
<h3>Characteristics and Properties<a class="headerlink" href="#characteristics-and-properties" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Ridge regression retains all the input features in the model but shrinks their coefficients.</p></li>
<li><p>It is particularly effective in addressing <strong>multicollinearity</strong> among input features.</p></li>
<li><p>Ridge regression can be viewed as a <strong>Bayesian linear regression</strong> model with a Gaussian prior on the coefficients.</p></li>
</ul>
</section>
<section id="bias-variance-tradeoff">
<h3>Bias-Variance Tradeoff<a class="headerlink" href="#bias-variance-tradeoff" title="Link to this heading">#</a></h3>
<p>Ridge regression reduces variance at the cost of increased bias, providing a better bias-variance tradeoff for improved generalization.</p>
</section>
<section id="pros-and-cons">
<h3>Pros and Cons<a class="headerlink" href="#pros-and-cons" title="Link to this heading">#</a></h3>
<section id="pros">
<h4>Pros<a class="headerlink" href="#pros" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Reduces model complexity and prevents overfitting.</p></li>
<li><p>Handles multicollinearity well.</p></li>
<li><p>Works well in scenarios with many correlated predictors.</p></li>
</ul>
</section>
<section id="cons">
<h4>Cons<a class="headerlink" href="#cons" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Does not perform feature selection; all features are retained.</p></li>
<li><p>Coefficients are only shrunk, not eliminated.</p></li>
</ul>
</section>
</section>
<section id="use-cases">
<h3>Use Cases<a class="headerlink" href="#use-cases" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Financial Forecasting</strong>: Ridge regression is used for predicting financial metrics, where multicollinearity between economic indicators is common.</p></li>
<li><p><strong>Healthcare</strong>: When analyzing correlated health indicators, ridge regression can help stabilize model predictions.</p></li>
</ul>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Load dataset</span>
<span class="n">diabetes</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Disease Progression&#39;</span><span class="p">)</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize Ridge Regression with a range of alpha values to visualize the impact</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">]</span>
<span class="n">mse_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">mse_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

<span class="c1"># Plot MSE for different alpha values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">mse_values</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Alpha (Regularization Strength)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Impact of Regularization Strength on Ridge Regression Performance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="lasso-regression">
<h2>Lasso Regression <a id="Lasso-Regression"></a><a class="headerlink" href="#lasso-regression" title="Link to this heading">#</a></h2>
<p>Lasso Regression, which stands for <strong>Least Absolute Shrinkage and Selection Operator</strong>, addresses some of the limitations of Ridge Regression by adding an <span class="math notranslate nohighlight">\(L_1\)</span> penalty to the loss function. This penalty has the effect of not only shrinking the coefficients but also setting some of them exactly to zero, effectively performing feature selection.</p>
<p>The objective function in Lasso Regression is modified as follows:</p>
<div class="math notranslate nohighlight">
\[
RSS_{lasso} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter controlling the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( \sum_{j=1}^{p} |\beta_j| \)</span> is the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty (sum of the absolute values of the coefficients).</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the actual target value.</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> is the predicted value from the model.</p></li>
</ul>
<p>As <span class="math notranslate nohighlight">\( \lambda \)</span> increases, the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty increases, leading to some coefficients being exactly zero. This makes Lasso useful in models with many features, as it can reduce complexity by selecting a subset of the most important features.</p>
<section id="id1">
<h3>Mathematical Formulation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>The Lasso Regression optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter controlling the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> is the intercept term.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\( L_1 \)</span> penalty term, <span class="math notranslate nohighlight">\( \lambda \sum_{j=1}^{p} |\beta_j| \)</span>, encourages sparsity in the model, leading to feature selection. As a result, some coefficients are exactly zero, allowing Lasso to automatically exclude irrelevant features.</p>
</section>
<section id="id2">
<h3>Characteristics and Properties<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Lasso performs <strong>automatic feature selection</strong> by driving some coefficients to zero.</p></li>
<li><p>It is especially useful in cases where we suspect that many features are irrelevant.</p></li>
<li><p>Lasso tends to outperform Ridge when the true underlying model is sparse (i.e., when only a few predictors have a real effect).</p></li>
</ul>
</section>
<section id="id3">
<h3>Bias-Variance Tradeoff<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Similar to Ridge, Lasso trades off between bias and variance. However, Lasso’s sparsity property adds another layer of complexity by reducing model variance while also potentially increasing bias due to excluded features.</p>
</section>
<section id="id4">
<h3>Pros and Cons<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<section id="id5">
<h4>Pros<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Performs feature selection, leading to simpler and more interpretable models.</p></li>
<li><p>Reduces the number of features, which is particularly useful in high-dimensional data settings.</p></li>
<li><p>Can handle multicollinearity like Ridge.</p></li>
</ul>
</section>
<section id="id6">
<h4>Cons<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>May exclude important features if <span class="math notranslate nohighlight">\(\lambda\)</span> is too large.</p></li>
<li><p>If the number of features is larger than the number of observations, Lasso may select only one feature from a group of highly correlated features.</p></li>
</ul>
</section>
</section>
<section id="id7">
<h3>Use Cases<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Genomic Data</strong>: Lasso regression is used in genetics to identify important genes that are associated with certain diseases while ignoring irrelevant ones.</p></li>
<li><p><strong>Marketing Analytics</strong>: When analyzing customer behavior data with many features, Lasso can help identify the most influential factors driving sales or customer churn.</p></li>
</ul>
</section>
<section id="id8">
<h3>Python Implementation<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Load dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Disease Progression&#39;</span><span class="p">)</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize Lasso Regression with a range of alpha values to visualize the impact</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">]</span>
<span class="n">mse_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">mse_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

<span class="c1"># Plot MSE for different alpha values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">mse_values</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Alpha (Regularization Strength)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Impact of Regularization Strength on Lasso Regression Performance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="elastic-net">
<h2>Elastic Net <a id="Elastic-Net"></a><a class="headerlink" href="#elastic-net" title="Link to this heading">#</a></h2>
<p>Elastic Net combines the penalties of both Ridge and Lasso regressions, balancing the <span class="math notranslate nohighlight">\( L_1 \)</span> and <span class="math notranslate nohighlight">\( L_2 \)</span> penalties. This hybrid approach is particularly effective in scenarios where there are multiple correlated features, as it can select groups of correlated variables together.</p>
<p>The objective function in Elastic Net is modified as follows:</p>
<div class="math notranslate nohighlight">
\[
RSS_{elastic\ net} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \lambda_1 \)</span> controls the strength of the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty (Lasso).</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda_2 \)</span> controls the strength of the <span class="math notranslate nohighlight">\( L_2 \)</span> penalty (Ridge).</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> is the predicted value from the model.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the actual target value.</p></li>
</ul>
<p>Elastic Net allows you to balance between Ridge and Lasso by tuning both <span class="math notranslate nohighlight">\( \lambda_1 \)</span> and <span class="math notranslate nohighlight">\( \lambda_2 \)</span>, making it more flexible than using either regularization technique alone.</p>
<section id="id9">
<h3>Mathematical Formulation<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>The Elastic Net optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda_1 \)</span> and <span class="math notranslate nohighlight">\( \lambda_2 \)</span> are the regularization parameters controlling the strengths of the <span class="math notranslate nohighlight">\( L_1 \)</span> and <span class="math notranslate nohighlight">\( L_2 \)</span> penalties, respectively.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> is the intercept term.</p></li>
</ul>
<p>Elastic Net combines both the sparsity property of Lasso (due to the <span class="math notranslate nohighlight">\( L_1 \)</span> term) and the coefficient stability of Ridge (due to the <span class="math notranslate nohighlight">\( L_2 \)</span> term), allowing for a more balanced and flexible model.</p>
</section>
<section id="id10">
<h3>Characteristics and Properties<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Feature selection</strong>: Like Lasso, Elastic Net performs automatic feature selection by driving some coefficients to zero.</p></li>
<li><p><strong>Coefficient shrinkage</strong>: Like Ridge, it shrinks coefficients but doesn’t eliminate them entirely unless necessary.</p></li>
<li><p><strong>Grouped variable selection</strong>: Elastic Net can select groups of highly correlated variables together, which is a limitation of Lasso alone.</p></li>
<li><p>Elastic Net requires tuning of both <span class="math notranslate nohighlight">\( \lambda_1 \)</span> and <span class="math notranslate nohighlight">\( \lambda_2 \)</span>, providing greater flexibility but also more complexity.</p></li>
</ul>
</section>
<section id="combining-strengths-of-ridge-and-lasso">
<h3>Combining Strengths of Ridge and Lasso<a class="headerlink" href="#combining-strengths-of-ridge-and-lasso" title="Link to this heading">#</a></h3>
<p>Elastic Net is particularly useful when there are <strong>highly correlated predictors</strong>. While Lasso may arbitrarily select one predictor from a group of correlated predictors, Elastic Net tends to select all of them together, taking advantage of the strengths of both regularization techniques.</p>
</section>
<section id="id11">
<h3>Pros and Cons<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<section id="id12">
<h4>Pros<a class="headerlink" href="#id12" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Feature selection</strong> and <strong>grouping</strong> of correlated features.</p></li>
<li><p>More stable than Lasso when predictors are correlated.</p></li>
<li><p>Offers a balance between Ridge and Lasso, making it versatile.</p></li>
<li><p>Reduces multicollinearity among features, like Ridge.</p></li>
</ul>
</section>
<section id="id13">
<h4>Cons<a class="headerlink" href="#id13" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Requires tuning of two hyperparameters (<span class="math notranslate nohighlight">\( \lambda_1 \)</span> and <span class="math notranslate nohighlight">\( \lambda_2 \)</span>).</p></li>
<li><p>More computationally expensive than Ridge or Lasso alone.</p></li>
<li><p>More complex model interpretation.</p></li>
</ul>
</section>
</section>
<section id="id14">
<h3>Use Cases<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Marketing Analysis</strong>: Elastic Net can be used to identify and group relevant marketing factors that influence customer purchasing behavior.</p></li>
<li><p><strong>Genomic Data</strong>: Used to model gene expression data, where many genes are correlated and some are irrelevant.</p></li>
<li><p><strong>Finance</strong>: Elastic Net is used to build predictive models in finance where factors such as macroeconomic indicators are often highly correlated.</p></li>
</ul>
</section>
<section id="id15">
<h3>Python Implementation<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Load dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Disease Progression&#39;</span><span class="p">)</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize ElasticNet with alpha and l1_ratio</span>
<span class="n">elastic_net</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">elastic_net</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_en</span> <span class="o">=</span> <span class="n">elastic_net</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_en</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_en</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (Elastic Net): </span><span class="si">{</span><span class="n">mse_en</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plot actual vs predicted values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_pred_en</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Actual Disease Progression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Disease Progression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Elastic Net Regression: Actual vs Predicted Disease Progression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span> <span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span> <span class="s1">&#39;r--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="least-angle-regression-lars">
<h2>Least Angle Regression (LARS) <a id="Least-Angle-Regression"></a><a class="headerlink" href="#least-angle-regression-lars" title="Link to this heading">#</a></h2>
<p>Least Angle Regression (LARS) is an efficient algorithm for fitting linear regression models to high-dimensional data. It is particularly useful in cases where the number of predictors (features) exceeds the number of observations. LARS is a stepwise method that progressively adds variables to the model, similar to forward stepwise regression, but it takes smaller and more adaptive steps. LARS can be used for both feature selection and coefficient estimation, making it an alternative to methods like Lasso.</p>
<section id="id16">
<h3>Characteristics and Properties<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Efficiency</strong>: LARS is computationally efficient, especially for high-dimensional data where the number of features can be much larger than the number of observations.</p></li>
<li><p><strong>Adaptive Stepwise Selection</strong>: LARS is similar to forward stepwise regression, but it takes smaller, adaptive steps when adding variables to the model. This makes it more efficient and precise in selecting important features.</p></li>
<li><p><strong>Exact Solutions</strong>: LARS produces the same solution as Lasso when there are less than <span class="math notranslate nohighlight">\(n\)</span> non-zero coefficients, making it a useful alternative in certain scenarios.</p></li>
<li><p><strong>Correlated Predictors</strong>: LARS can handle situations with correlated predictors better than Lasso, although Lasso still tends to perform better in high-dimensional settings when variable selection is critical.</p></li>
</ul>
</section>
<section id="id17">
<h3>Mathematical Formulation<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>LARS incrementally builds a model by identifying the most correlated variable with the residuals at each step. The coefficients of that variable are adjusted until another variable becomes equally correlated with the residuals. This process continues, with variables being added and coefficients updated, until the desired number of non-zero coefficients is reached.</p>
<p>Let <span class="math notranslate nohighlight">\(y\)</span> represent the target variable and <span class="math notranslate nohighlight">\(X\)</span> represent the feature matrix:</p>
<ol class="arabic simple">
<li><p><strong>Start with all coefficients (<span class="math notranslate nohighlight">\(\beta_j\)</span>) equal to zero.</strong></p></li>
<li><p><strong>At each step</strong>:</p>
<ul class="simple">
<li><p>Find the feature most correlated with the residuals.</p></li>
<li><p>Increase the coefficient for that feature in the direction of reducing the residual error.</p></li>
<li><p>Stop when another feature becomes equally correlated with the residuals, then update coefficients for both features simultaneously.</p></li>
</ul>
</li>
<li><p><strong>Repeat until the desired number of features is included</strong> or the residuals can no longer be reduced.</p></li>
</ol>
</section>
<section id="key-characteristics">
<h3>Key Characteristics<a class="headerlink" href="#key-characteristics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Stepwise selection</strong>: Like forward stepwise regression, LARS selects features one at a time but adjusts the coefficients more gradually.</p></li>
<li><p><strong>Exact solutions for Lasso</strong>: When the number of selected features is less than the number of observations, LARS provides the exact solution as Lasso.</p></li>
<li><p><strong>Feature selection</strong>: LARS is an excellent feature selection tool, especially in high-dimensional datasets.</p></li>
</ul>
</section>
<section id="id18">
<h3>Pros and Cons<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<section id="id19">
<h4>Pros<a class="headerlink" href="#id19" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Computationally efficient</strong>: LARS is faster than Lasso or Ridge in situations where the number of features exceeds the number of observations.</p></li>
<li><p><strong>Handles high-dimensional data well</strong>: Suitable for cases with a large number of predictors.</p></li>
<li><p><strong>Feature selection</strong>: LARS can perform feature selection by only including a subset of variables in the final model.</p></li>
</ul>
</section>
<section id="id20">
<h4>Cons<a class="headerlink" href="#id20" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Less stable</strong>: Compared to Lasso or Ridge, LARS can be less stable in scenarios where there is noise or weak relationships between predictors and the outcome.</p></li>
<li><p><strong>No regularization</strong>: Unlike Ridge or Lasso, LARS does not include a penalty term, which may lead to overfitting in some cases.</p></li>
<li><p><strong>Requires more tuning</strong>: LARS can be sensitive to the number of non-zero coefficients selected.</p></li>
</ul>
</section>
</section>
<section id="id21">
<h3>Use Cases<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>High-Dimensional Data</strong>: LARS is especially useful when the number of predictors exceeds the number of observations, such as in genomic data or text analysis.</p></li>
<li><p><strong>Feature Selection</strong>: It is a good alternative to Lasso for feature selection when predictors are highly correlated or when computational efficiency is a priority.</p></li>
<li><p><strong>Genomic Studies</strong>: LARS is often used in gene expression data analysis, where the number of genes (predictors) far exceeds the number of samples (observations).</p></li>
</ul>
</section>
<section id="id22">
<h3>Python Implementation<a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lars</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Load dataset (using the same dataset as for Ridge and Lasso examples)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Disease Progression&#39;</span><span class="p">)</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize LARS model</span>
<span class="n">lars</span> <span class="o">=</span> <span class="n">Lars</span><span class="p">(</span><span class="n">n_nonzero_coefs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">lars</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_lars</span> <span class="o">=</span> <span class="n">lars</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_lars</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lars</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (LARS): </span><span class="si">{</span><span class="n">mse_lars</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="n">coefficients_lars</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lars</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LARS Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients_lars</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="adaptive-lasso">
<h2>Adaptive Lasso <a id="Adaptive-Lasso"></a><a class="headerlink" href="#adaptive-lasso" title="Link to this heading">#</a></h2>
<p>Adaptive Lasso is an extension of Lasso Regression that aims to improve feature selection by assigning different weights to different coefficients. Unlike the standard Lasso, which applies the same <span class="math notranslate nohighlight">\( L_1 \)</span> penalty to all coefficients, Adaptive Lasso adjusts the penalties based on the importance of the features. This results in more accurate feature selection and improved prediction performance.</p>
<p>The key idea behind Adaptive Lasso is to use a weighted <span class="math notranslate nohighlight">\( L_1 \)</span> penalty, where the weights are determined by an initial estimator of the coefficients (often from OLS or Ridge regression). This ensures that important features are penalized less, while less important features are penalized more.</p>
<section id="id23">
<h3>Characteristics and Properties<a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Improved Feature Selection</strong>: By adjusting the weights on the penalty term, Adaptive Lasso tends to improve feature selection compared to the standard Lasso.</p></li>
<li><p><strong>Oracle Property</strong>: Under certain conditions, Adaptive Lasso possesses the so-called “oracle property,” meaning it can correctly identify the true underlying model with a high probability.</p></li>
<li><p><strong>Data-Driven Weights</strong>: The penalty applied to each coefficient is adjusted based on a preliminary estimate of the coefficients, often from OLS, Ridge, or initial Lasso estimations.</p></li>
</ul>
</section>
<section id="id24">
<h3>Mathematical Formulation<a class="headerlink" href="#id24" title="Link to this heading">#</a></h3>
<p>The Adaptive Lasso optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} w_j |\beta_j|
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter controlling the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( w_j \)</span> are the weights applied to the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty for each coefficient.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> is the intercept term.</p></li>
</ul>
<p>The weights <span class="math notranslate nohighlight">\( w_j \)</span> are typically set to be inversely proportional to the absolute values of the initial coefficient estimates:</p>
<div class="math notranslate nohighlight">
\[
w_j = \frac{1}{|\hat{\beta}_j|}
\]</div>
<p>Where <span class="math notranslate nohighlight">\( \hat{\beta}_j \)</span> is the initial estimate of the coefficient <span class="math notranslate nohighlight">\( \beta_j \)</span>, often obtained using Ordinary Least Squares (OLS), Ridge regression, or even an initial Lasso model. By assigning smaller weights to larger coefficients and larger weights to smaller coefficients, Adaptive Lasso ensures that important features are penalized less, leading to improved feature selection.</p>
</section>
<section id="characteristics-of-adaptive-lasso">
<h3>Characteristics of Adaptive Lasso<a class="headerlink" href="#characteristics-of-adaptive-lasso" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Improved feature selection</strong>: Adaptive Lasso can outperform standard Lasso in terms of both variable selection and prediction accuracy.</p></li>
<li><p><strong>Oracle property</strong>: Under certain conditions, Adaptive Lasso has the oracle property, which means it can correctly identify the true underlying model as if the true model were known.</p></li>
<li><p><strong>Data-driven weights</strong>: The weights are typically generated from an initial regression model such as OLS or Ridge, allowing the penalty to adapt to the relative importance of each feature.</p></li>
</ul>
</section>
<section id="id25">
<h3>Pros and Cons<a class="headerlink" href="#id25" title="Link to this heading">#</a></h3>
<section id="id26">
<h4>Pros<a class="headerlink" href="#id26" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>More accurate feature selection</strong>: Adaptive Lasso improves upon the standard Lasso by applying different levels of shrinkage to different coefficients, leading to more accurate selection of relevant variables.</p></li>
<li><p><strong>Oracle property</strong>: Under certain regularity conditions, Adaptive Lasso can achieve the oracle property, meaning it consistently selects the true model.</p></li>
<li><p><strong>Flexibility in weighting</strong>: The flexibility to adjust the penalty based on the preliminary estimates of the coefficients makes Adaptive Lasso more robust in feature selection, especially in the presence of correlated predictors.</p></li>
</ul>
</section>
<section id="id27">
<h4>Cons<a class="headerlink" href="#id27" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>More complex</strong>: Adaptive Lasso requires an initial model to estimate the weights, which adds complexity compared to the standard Lasso.</p></li>
<li><p><strong>Dependent on initial estimator</strong>: The performance of Adaptive Lasso can depend on the choice of the initial estimator, which may affect the final model if not chosen carefully.</p></li>
<li><p><strong>Tuning multiple parameters</strong>: In addition to tuning <span class="math notranslate nohighlight">\( \lambda \)</span>, Adaptive Lasso also requires determining the appropriate method to compute the initial weights.</p></li>
</ul>
</section>
</section>
<section id="id28">
<h3>Use Cases<a class="headerlink" href="#id28" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Genomic Data</strong>: Adaptive Lasso is often used in high-dimensional settings like genomic data, where accurate variable selection is critical and standard Lasso may fail due to correlated variables.</p></li>
<li><p><strong>Economics and Finance</strong>: In scenarios where the number of features exceeds the number of observations and some features are more important than others, Adaptive Lasso can outperform standard Lasso by assigning different penalties.</p></li>
<li><p><strong>Marketing Analytics</strong>: Adaptive Lasso can be used to refine customer segmentation models by selecting the most relevant features while adjusting for multicollinearity.</p></li>
</ul>
</section>
<section id="id29">
<h3>Python Implementation<a class="headerlink" href="#id29" title="Link to this heading">#</a></h3>
<p>While Adaptive Lasso is not directly available in popular Python libraries like <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, it can be implemented by using Lasso along with custom weighting schemes. Here’s an example of how you might implement it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Load dataset (using the same dataset as for Ridge and Lasso examples)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">diabetes</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Disease Progression&#39;</span><span class="p">)</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Standardize the features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Step 1: Fit initial OLS to get weights</span>
<span class="n">ols</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">ols</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">initial_coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ols</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="c1"># Step 2: Compute weights (inverse of absolute values of OLS coefficients)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">initial_coefs</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>  <span class="c1"># Add small value to avoid division by zero</span>

<span class="c1"># Step 3: Apply weighted Lasso (Adaptive Lasso)</span>
<span class="n">lasso_adaptive</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Regularization strength</span>
<span class="n">lasso_adaptive</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span> <span class="o">*</span> <span class="n">weights</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># Weighted data</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_adaptive</span> <span class="o">=</span> <span class="n">lasso_adaptive</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_adaptive</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_adaptive</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (Adaptive Lasso): </span><span class="si">{</span><span class="n">mse_adaptive</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="n">coefficients_adaptive</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lasso_adaptive</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Adaptive Lasso Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients_adaptive</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="fused-lasso">
<h2>Fused Lasso <a id="Fused-Lasso"></a><a class="headerlink" href="#fused-lasso" title="Link to this heading">#</a></h2>
<p>Fused Lasso is an extension of Lasso that introduces penalties not only on the size of the coefficients (as in Lasso) but also on the differences between consecutive coefficients. This method is particularly useful in settings where the features have a natural ordering, such as in time series data or spatial data, where smoothness or sparsity in the changes between adjacent coefficients is desired.</p>
<p>The Fused Lasso imposes both an <span class="math notranslate nohighlight">\( L_1 \)</span> penalty on the coefficients to encourage sparsity and an additional <span class="math notranslate nohighlight">\( L_1 \)</span> penalty on the differences between consecutive coefficients to enforce smoothness.</p>
<section id="id30">
<h3>Characteristics and Properties<a class="headerlink" href="#id30" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Sparsity and Smoothness</strong>: Fused Lasso encourages both sparsity (by shrinking some coefficients to zero) and smoothness (by shrinking the differences between adjacent coefficients). This makes it particularly suitable for structured data.</p></li>
<li><p><strong>Feature Selection</strong>: Like Lasso, Fused Lasso selects a subset of relevant features by driving some coefficients to zero.</p></li>
<li><p><strong>Piecewise Constant Solutions</strong>: Fused Lasso often results in piecewise constant coefficient profiles, which is useful in settings like signal processing or genomics, where we expect changes to occur in segments or regions rather than continuously.</p></li>
</ul>
</section>
<section id="id31">
<h3>Mathematical Formulation<a class="headerlink" href="#id31" title="Link to this heading">#</a></h3>
<p>The Fused Lasso optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=2}^{p} |\beta_j - \beta_{j-1}|
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda_1 \)</span> is the regularization parameter controlling the strength of the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty on the coefficients (like in Lasso).</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda_2 \)</span> is the regularization parameter controlling the strength of the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty on the differences between consecutive coefficients (the fusion penalty).</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \)</span> is the intercept term.</p></li>
</ul>
<p>The first term ensures a good fit to the data, while the two <span class="math notranslate nohighlight">\( L_1 \)</span> penalty terms promote sparsity and smoothness in the coefficients.</p>
</section>
<section id="characteristics-of-fused-lasso">
<h3>Characteristics of Fused Lasso<a class="headerlink" href="#characteristics-of-fused-lasso" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Sparsity</strong>: Like Lasso, Fused Lasso encourages sparsity by shrinking some coefficients to zero.</p></li>
<li><p><strong>Smoothness</strong>: The second penalty term promotes smoothness between consecutive coefficients, making it suitable for ordered data.</p></li>
<li><p><strong>Structured Data</strong>: Fused Lasso works well when the predictors have an inherent structure, such as time series data, where adjacent coefficients should change gradually.</p></li>
</ul>
</section>
<section id="id32">
<h3>Pros and Cons<a class="headerlink" href="#id32" title="Link to this heading">#</a></h3>
<section id="id33">
<h4>Pros<a class="headerlink" href="#id33" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Sparsity and smoothness</strong>: Fused Lasso not only selects important features but also enforces smoothness in the coefficients, which is ideal for ordered or structured data.</p></li>
<li><p><strong>Handles high-dimensional data</strong>: Fused Lasso can perform well in high-dimensional settings, especially when features have a natural ordering.</p></li>
<li><p><strong>Piecewise constant solutions</strong>: Often, Fused Lasso results in models where adjacent coefficients are constant or change gradually, making it a good choice for signal processing or genomics.</p></li>
</ul>
</section>
<section id="id34">
<h4>Cons<a class="headerlink" href="#id34" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>More complex</strong>: Fused Lasso introduces additional complexity due to the second penalty term, which requires tuning multiple hyperparameters (<span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>).</p></li>
<li><p><strong>Computationally intensive</strong>: The additional fusion penalty can make Fused Lasso more computationally expensive than standard Lasso or Ridge.</p></li>
<li><p><strong>Requires structured data</strong>: Fused Lasso is particularly suited for data with an inherent structure (e.g., time series or spatial data). It may not offer significant advantages in settings without such structure.</p></li>
</ul>
</section>
</section>
<section id="id35">
<h3>Use Cases<a class="headerlink" href="#id35" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Time Series Data</strong>: Fused Lasso is often used in time series analysis, where adjacent time points are expected to have similar effects, and smooth transitions between coefficients are desired.</p></li>
<li><p><strong>Genomics</strong>: In genomic data, where genes are ordered by their position on the chromosome, Fused Lasso can be used to identify regions where there are changes in gene expression levels.</p></li>
<li><p><strong>Signal Processing</strong>: Fused Lasso is useful in signal processing tasks where the goal is to detect changes or trends in the signal, often resulting in piecewise constant segments.</p></li>
<li><p><strong>Spatial Data</strong>: In scenarios where predictors are spatially organized (such as in image processing), Fused Lasso can enforce smoothness between neighboring spatial regions.</p></li>
</ul>
</section>
<section id="id36">
<h3>Python Implementation<a class="headerlink" href="#id36" title="Link to this heading">#</a></h3>
<p>While Fused Lasso is not directly available in popular libraries like <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, it can be implemented using specialized libraries like <code class="docutils literal notranslate"><span class="pre">glasso</span></code> in Python or by modifying the Lasso algorithm with additional constraints. Below is a simplified example of how you might implement Fused Lasso using Python.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Generate synthetic time series data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>  <span class="c1"># Piecewise constant coefficients</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize a Lasso model (used here as a proxy for Fused Lasso)</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_fused</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_fused</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_fused</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (Fused Lasso Approximation): </span><span class="si">{</span><span class="n">mse_fused</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="n">coefficients_fused</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fused Lasso Coefficients Approximation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients_fused</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="group-lasso">
<h2>Group Lasso <a id="Group-Lasso"></a><a class="headerlink" href="#group-lasso" title="Link to this heading">#</a></h2>
<p>Group Lasso is a regularization technique that extends the Lasso method by encouraging sparsity at the group level. Instead of applying the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty to individual coefficients, Group Lasso applies the penalty to predefined groups of coefficients, allowing entire groups of variables to be selected or discarded. This is particularly useful when features are naturally grouped, such as in multivariate regression or when working with categorical variables with multiple levels.</p>
<p>The key advantage of Group Lasso is that it performs structured variable selection, making it ideal for problems where variables can be grouped, such as in gene expression data or multi-task learning.</p>
<section id="id37">
<h3>Characteristics and Properties<a class="headerlink" href="#id37" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Group Sparsity</strong>: Group Lasso selects or discards entire groups of features, encouraging sparsity at the group level rather than at the individual feature level.</p></li>
<li><p><strong>Predefined Groups</strong>: Features must be divided into predefined groups, and the <span class="math notranslate nohighlight">\( L_2 \)</span> norm is applied within each group, followed by an <span class="math notranslate nohighlight">\( L_1 \)</span> penalty across groups.</p></li>
<li><p><strong>Multivariate Regression</strong>: Group Lasso is particularly effective in scenarios involving multivariate regression, multi-task learning, or when working with categorical features that have multiple dummy variables representing different levels.</p></li>
</ul>
</section>
<section id="id38">
<h3>Mathematical Formulation<a class="headerlink" href="#id38" title="Link to this heading">#</a></h3>
<p>The Group Lasso optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{g=1}^{G} \sqrt{|G_g|} ||\beta_g||_2
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_g \)</span> is the vector of coefficients corresponding to group <span class="math notranslate nohighlight">\( g \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( |G_g| \)</span> is the size of group <span class="math notranslate nohighlight">\( g \)</span> (i.e., the number of features in the group).</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter controlling the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( ||\beta_g||_2 \)</span> is the <span class="math notranslate nohighlight">\( L_2 \)</span> norm of the coefficients within group <span class="math notranslate nohighlight">\( g \)</span>.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\( L_1 \)</span> penalty is applied across groups (promoting sparsity at the group level), while the <span class="math notranslate nohighlight">\( L_2 \)</span> norm ensures that all variables within a selected group remain together.</p>
</section>
<section id="characteristics-of-group-lasso">
<h3>Characteristics of Group Lasso<a class="headerlink" href="#characteristics-of-group-lasso" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Structured Feature Selection</strong>: Unlike standard Lasso, which selects individual features, Group Lasso selects entire groups of features, making it ideal for problems where features are naturally grouped.</p></li>
<li><p><strong>Handles Grouped Variables</strong>: Group Lasso is effective when working with features that are grouped by nature, such as multi-level categorical variables or multivariate regression tasks.</p></li>
<li><p><strong>Multitask Learning</strong>: In multitask learning, where several related tasks are learned simultaneously, Group Lasso can select task-specific features while considering the structure of the tasks.</p></li>
</ul>
</section>
<section id="id39">
<h3>Pros and Cons<a class="headerlink" href="#id39" title="Link to this heading">#</a></h3>
<section id="id40">
<h4>Pros<a class="headerlink" href="#id40" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Group-level feature selection</strong>: Group Lasso allows for selecting or discarding entire groups of variables, making it particularly useful when features have a natural grouping, such as in multi-task learning or categorical variables.</p></li>
<li><p><strong>Flexibility in group structure</strong>: The method can be applied to any predefined grouping of features, providing flexibility in how groups are formed.</p></li>
<li><p><strong>Multi-task learning</strong>: Group Lasso can be applied in multi-task learning settings to select relevant groups of features for all tasks simultaneously, improving model performance.</p></li>
</ul>
</section>
<section id="id41">
<h4>Cons<a class="headerlink" href="#id41" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Requires predefined groups</strong>: Group Lasso requires the user to specify the groups of features in advance, which may not always be intuitive or straightforward.</p></li>
<li><p><strong>Computationally intensive</strong>: Group Lasso is more computationally expensive than Lasso due to the need to apply the <span class="math notranslate nohighlight">\( L_2 \)</span> norm within each group and the <span class="math notranslate nohighlight">\( L_1 \)</span> norm across groups.</p></li>
<li><p><strong>Less effective for individual feature selection</strong>: If individual feature sparsity is desired (rather than group-level sparsity), standard Lasso or Elastic Net might be more appropriate.</p></li>
</ul>
</section>
</section>
<section id="id42">
<h3>Use Cases<a class="headerlink" href="#id42" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Genomics</strong>: Group Lasso is useful in genomic data analysis, where genes can be grouped by biological pathways or chromosomal locations, allowing the model to select or discard entire pathways.</p></li>
<li><p><strong>Multivariate Regression</strong>: Group Lasso is effective when modeling multiple responses simultaneously (multi-task learning), as it can select relevant groups of features across tasks.</p></li>
<li><p><strong>Categorical Data</strong>: When working with categorical variables that are represented by multiple dummy variables (e.g., levels of a factor), Group Lasso can either include or exclude entire groups of dummy variables.</p></li>
<li><p><strong>Multi-task Learning</strong>: In settings where multiple related prediction tasks are performed simultaneously, Group Lasso selects the relevant features across multiple tasks in a structured way.</p></li>
</ul>
</section>
<section id="id43">
<h3>Python Implementation<a class="headerlink" href="#id43" title="Link to this heading">#</a></h3>
<p>While Group Lasso is not directly implemented in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, it can be implemented using specialized packages such as <code class="docutils literal notranslate"><span class="pre">group-lasso</span></code>. Here’s an example of how Group Lasso might be implemented in Python using the <code class="docutils literal notranslate"><span class="pre">group-lasso</span></code> package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install group-lasso package: !pip install group-lasso</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">group_lasso</span> <span class="kn">import</span> <span class="n">GroupLasso</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Create synthetic data</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_groups</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Define groups (each group has two features)</span>
<span class="n">groups</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize Group Lasso model</span>
<span class="n">group_lasso</span> <span class="o">=</span> <span class="n">GroupLasso</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">group_reg</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">l1_reg</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">group_lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_gl</span> <span class="o">=</span> <span class="n">group_lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_gl</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gl</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (Group Lasso): </span><span class="si">{</span><span class="n">mse_gl</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="n">coefficients_gl</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">group_lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Group Lasso Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients_gl</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="sparse-group-lasso">
<h2>Sparse Group Lasso <a id="Sparse-Group-Lasso"></a><a class="headerlink" href="#sparse-group-lasso" title="Link to this heading">#</a></h2>
<p>Sparse Group Lasso is an extension of Group Lasso that combines the benefits of both group-level and individual feature selection. It applies a penalty to groups of features, as in Group Lasso, while simultaneously encouraging sparsity within those groups, similar to Lasso. This makes Sparse Group Lasso especially useful when both group-level sparsity and individual-level sparsity are important for feature selection.</p>
<p>Sparse Group Lasso is a compromise between Group Lasso and Lasso. It allows for selecting relevant groups of variables while also discarding unimportant individual features within those groups, making it a more flexible regularization method for structured datasets.</p>
<section id="id44">
<h3>Characteristics and Properties<a class="headerlink" href="#id44" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Group-Level and Feature-Level Sparsity</strong>: Sparse Group Lasso encourages sparsity both at the group level and within individual groups, allowing the model to select or discard groups and individual features simultaneously.</p></li>
<li><p><strong>Predefined Groups</strong>: Similar to Group Lasso, Sparse Group Lasso requires the features to be divided into predefined groups.</p></li>
<li><p><strong>Combination of <span class="math notranslate nohighlight">\( L_1 \)</span> and <span class="math notranslate nohighlight">\( L_2 \)</span> Penalties</strong>: Sparse Group Lasso applies both <span class="math notranslate nohighlight">\( L_1 \)</span> and <span class="math notranslate nohighlight">\( L_2 \)</span> penalties, providing the flexibility of Lasso for individual feature selection and Group Lasso for group-level selection.</p></li>
</ul>
</section>
<section id="id45">
<h3>Mathematical Formulation<a class="headerlink" href="#id45" title="Link to this heading">#</a></h3>
<p>The Sparse Group Lasso optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda_1 \sum_{g=1}^{G} \sqrt{|G_g|} ||\beta_g||_2 + \lambda_2 \sum_{j=1}^{p} |\beta_j|
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_g \)</span> is the vector of coefficients corresponding to group <span class="math notranslate nohighlight">\( g \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( |G_g| \)</span> is the size of group <span class="math notranslate nohighlight">\( g \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda_1 \)</span> is the regularization parameter controlling the strength of the <span class="math notranslate nohighlight">\( L_2 \)</span> penalty on groups.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda_2 \)</span> is the regularization parameter controlling the strength of the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty on individual coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( ||\beta_g||_2 \)</span> is the <span class="math notranslate nohighlight">\( L_2 \)</span> norm of the coefficients within group <span class="math notranslate nohighlight">\( g \)</span>, and <span class="math notranslate nohighlight">\( |\beta_j| \)</span> is the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty on individual coefficients.</p></li>
</ul>
<p>This formulation combines the group-level selection enforced by Group Lasso and the individual-level selection enforced by Lasso. The <span class="math notranslate nohighlight">\( L_1 \)</span> penalty ensures sparsity at the individual feature level, while the <span class="math notranslate nohighlight">\( L_2 \)</span> penalty on groups encourages selection or exclusion of entire groups.</p>
</section>
<section id="characteristics-of-sparse-group-lasso">
<h3>Characteristics of Sparse Group Lasso<a class="headerlink" href="#characteristics-of-sparse-group-lasso" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Simultaneous Group and Feature Selection</strong>: Sparse Group Lasso provides the flexibility to select relevant groups while discarding irrelevant features within those groups, leading to sparse solutions at both levels.</p></li>
<li><p><strong>Structured Feature Selection</strong>: Like Group Lasso, Sparse Group Lasso is suitable for data where features are naturally grouped, such as in multivariate regression, multi-task learning, or when working with categorical variables.</p></li>
<li><p><strong>Combination of Penalties</strong>: The method uses both <span class="math notranslate nohighlight">\( L_1 \)</span> and <span class="math notranslate nohighlight">\( L_2 \)</span> penalties, offering the benefits of both Lasso (individual sparsity) and Group Lasso (group selection).</p></li>
</ul>
</section>
<section id="id46">
<h3>Pros and Cons<a class="headerlink" href="#id46" title="Link to this heading">#</a></h3>
<section id="id47">
<h4>Pros<a class="headerlink" href="#id47" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Group and individual sparsity</strong>: Sparse Group Lasso selects groups of variables and simultaneously promotes sparsity within groups, offering more flexibility than Group Lasso alone.</p></li>
<li><p><strong>Flexible feature selection</strong>: Allows the model to retain or discard individual features within selected groups, making it ideal for high-dimensional datasets with both group and individual structure.</p></li>
<li><p><strong>Improves interpretability</strong>: By selecting relevant groups and features within those groups, Sparse Group Lasso provides a clearer understanding of the relationships between predictors and outcomes.</p></li>
</ul>
</section>
<section id="id48">
<h4>Cons<a class="headerlink" href="#id48" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Requires predefined groups</strong>: Similar to Group Lasso, Sparse Group Lasso requires that the features be divided into groups beforehand, which may not always be intuitive or straightforward.</p></li>
<li><p><strong>More computationally expensive</strong>: Sparse Group Lasso introduces additional complexity compared to standard Lasso or Group Lasso, requiring the tuning of multiple hyperparameters.</p></li>
<li><p><strong>Tuning two parameters</strong>: Both <span class="math notranslate nohighlight">\( \lambda_1 \)</span> and <span class="math notranslate nohighlight">\( \lambda_2 \)</span> must be tuned, adding complexity to the model selection process.</p></li>
</ul>
</section>
</section>
<section id="id49">
<h3>Use Cases<a class="headerlink" href="#id49" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Multivariate Regression</strong>: Sparse Group Lasso is useful in multivariate regression settings where groups of variables are related, and sparsity is desired both at the group level and the individual level.</p></li>
<li><p><strong>Genomics</strong>: In genomic data, Sparse Group Lasso can be used to select relevant groups of genes (e.g., based on biological pathways) while discarding irrelevant genes within those groups.</p></li>
<li><p><strong>Multi-task Learning</strong>: When performing multiple related tasks simultaneously, Sparse Group Lasso can select groups of features relevant across tasks while pruning unnecessary individual features within those groups.</p></li>
<li><p><strong>Marketing Analytics</strong>: In scenarios with customer segmentation, Sparse Group Lasso can be used to select relevant customer groups while identifying the most important features within those groups.</p></li>
</ul>
</section>
<section id="id50">
<h3>Python Implementation<a class="headerlink" href="#id50" title="Link to this heading">#</a></h3>
<p>Sparse Group Lasso can be implemented using specialized libraries like <code class="docutils literal notranslate"><span class="pre">group-lasso</span></code> or by modifying existing implementations of Group Lasso and Lasso to account for both group and individual penalties. Below is a sample implementation using Python:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install group-lasso package: !pip install group-lasso</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">group_lasso</span> <span class="kn">import</span> <span class="n">GroupLasso</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_groups</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Define groups (each group has two features)</span>
<span class="n">groups</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize Sparse Group Lasso model</span>
<span class="n">sparse_group_lasso</span> <span class="o">=</span> <span class="n">GroupLasso</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">group_reg</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">l1_reg</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">scale_reg</span><span class="o">=</span><span class="s2">&quot;group_size&quot;</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">sparse_group_lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_sgl</span> <span class="o">=</span> <span class="n">sparse_group_lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_sgl</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_sgl</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (Sparse Group Lasso): </span><span class="si">{</span><span class="n">mse_sgl</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="n">coefficients_sgl</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">sparse_group_lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sparse Group Lasso Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefficients_sgl</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="total-variation-regularization-tv-l1">
<h2>Total Variation Regularization (TV-L1) <a id="TV-L1"></a><a class="headerlink" href="#total-variation-regularization-tv-l1" title="Link to this heading">#</a></h2>
<p>Total Variation Regularization, commonly referred to as TV-L1, is a technique primarily used in image processing and signal denoising to preserve edges while removing noise. Unlike Lasso or Ridge, which penalize the magnitude of the coefficients, TV-L1 minimizes the total variation of the coefficients. It aims to enforce piecewise constant solutions by reducing variations between neighboring coefficients, making it particularly suitable for tasks involving spatial or temporal data where changes should be abrupt rather than gradual.</p>
<p>In the case of TV-L1, the total variation (TV) of a signal is the sum of the absolute differences between neighboring coefficients, and the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty is applied to promote sparsity in these differences.</p>
<section id="id51">
<h3>Characteristics and Properties<a class="headerlink" href="#id51" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Preservation of Edges</strong>: TV-L1 is particularly well-suited for tasks where edge preservation is important, such as image processing or signal processing. It encourages piecewise constant solutions where rapid changes (edges) are maintained, while smoothing out gradual variations.</p></li>
<li><p><strong>Sparsity in Differences</strong>: The <span class="math notranslate nohighlight">\( L_1 \)</span> penalty is applied to the differences between neighboring coefficients, promoting sparsity in these differences. This results in models where neighboring coefficients are often equal, but sharp changes between adjacent coefficients are retained.</p></li>
<li><p><strong>Structured Data</strong>: TV-L1 works well for spatial or temporal data, where it is expected that changes occur between adjacent observations or features.</p></li>
</ul>
</section>
<section id="id52">
<h3>Mathematical Formulation<a class="headerlink" href="#id52" title="Link to this heading">#</a></h3>
<p>The Total Variation Regularization (TV-L1) optimization problem is defined as:</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=2}^{p} |\beta_j - \beta_{j-1}|
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> is the target variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{ij} \)</span> are the input features.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> are the coefficients.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter controlling the strength of the total variation penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( |\beta_j - \beta_{j-1}| \)</span> is the absolute difference between adjacent coefficients, which promotes sparsity in the differences.</p></li>
</ul>
<p>The total variation penalty promotes smooth regions in the data by minimizing the differences between adjacent coefficients, while allowing for sharp changes (discontinuities) where necessary.</p>
</section>
<section id="characteristics-of-tv-l1">
<h3>Characteristics of TV-L1<a class="headerlink" href="#characteristics-of-tv-l1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Edge Preservation</strong>: TV-L1 is designed to preserve sharp transitions between adjacent values, which is crucial in applications like image denoising or signal segmentation.</p></li>
<li><p><strong>Sparsity in Gradients</strong>: By applying the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty to differences between adjacent coefficients, TV-L1 encourages sparsity in the changes between neighboring values, resulting in piecewise constant solutions.</p></li>
<li><p><strong>Ideal for Spatial and Temporal Data</strong>: TV-L1 is particularly useful for structured data where the features or observations have an inherent order, such as time series or spatial data.</p></li>
</ul>
</section>
<section id="id53">
<h3>Pros and Cons<a class="headerlink" href="#id53" title="Link to this heading">#</a></h3>
<section id="id54">
<h4>Pros<a class="headerlink" href="#id54" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Preserves edges</strong>: TV-L1 is ideal for preserving sharp changes in spatial or temporal data, such as in image denoising or signal processing, where abrupt transitions are important.</p></li>
<li><p><strong>Sparsity in differences</strong>: By penalizing differences between adjacent coefficients, TV-L1 promotes smooth regions while maintaining important edges, leading to more interpretable models in structured data.</p></li>
<li><p><strong>Efficient for structured data</strong>: TV-L1 is particularly suited for tasks where the features are ordered, such as in time series, spatial data, or image processing.</p></li>
</ul>
</section>
<section id="id55">
<h4>Cons<a class="headerlink" href="#id55" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Not suitable for unstructured data</strong>: TV-L1 is designed for structured data with inherent spatial or temporal order. It may not perform well on datasets where the features do not have a meaningful sequence or adjacency.</p></li>
<li><p><strong>Requires tuning</strong>: Like other regularization methods, the <span class="math notranslate nohighlight">\( \lambda \)</span> parameter must be carefully tuned to balance the tradeoff between edge preservation and noise reduction.</p></li>
<li><p><strong>Computational complexity</strong>: The TV-L1 regularization problem can be computationally expensive due to the non-differentiable <span class="math notranslate nohighlight">\( L_1 \)</span> term, especially when applied to large datasets.</p></li>
</ul>
</section>
</section>
<section id="id56">
<h3>Use Cases<a class="headerlink" href="#id56" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Image Denoising</strong>: TV-L1 is widely used in image processing tasks to remove noise while preserving important edges, such as in medical imaging or satellite imagery.</p></li>
<li><p><strong>Signal Processing</strong>: In signal processing, TV-L1 is used to segment signals into piecewise constant sections, retaining important transitions and discarding noise.</p></li>
<li><p><strong>Time Series Analysis</strong>: For time series data, TV-L1 can help in identifying abrupt changes or regime shifts in the data while smoothing over periods of stability.</p></li>
<li><p><strong>Genomics</strong>: TV-L1 is also useful in genomics, where it can be applied to detect regions of constant or changing gene expression across adjacent chromosomal positions.</p></li>
</ul>
</section>
<section id="id57">
<h3>Python Implementation<a class="headerlink" href="#id57" title="Link to this heading">#</a></h3>
<p>Total Variation Regularization (TV-L1) is not directly available in common libraries like <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, but it can be implemented using libraries such as <code class="docutils literal notranslate"><span class="pre">cvxpy</span></code> for convex optimization or specialized image processing libraries. Below is a simplified Python example using <code class="docutils literal notranslate"><span class="pre">cvxpy</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install cvxpy package: !pip install cvxpy</span>
<span class="kn">import</span> <span class="nn">cvxpy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Generate synthetic data (structured)</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># Piecewise constant coefficients</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Define the TV-L1 problem in cvxpy</span>
<span class="n">beta_tv</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
<span class="n">lambda_tv</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Regularization strength</span>
<span class="n">objective</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Minimize</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">sum_squares</span><span class="p">(</span><span class="n">X_train</span> <span class="o">@</span> <span class="n">beta_tv</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">+</span> 
                        <span class="n">lambda_tv</span> <span class="o">*</span> <span class="n">cp</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">beta_tv</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">beta_tv</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">problem</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">objective</span><span class="p">)</span>
<span class="n">problem</span><span class="o">.</span><span class="n">solve</span><span class="p">()</span>

<span class="c1"># Get the solution</span>
<span class="n">beta_tv_value</span> <span class="o">=</span> <span class="n">beta_tv</span><span class="o">.</span><span class="n">value</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_tv</span> <span class="o">=</span> <span class="n">X_test</span> <span class="o">@</span> <span class="n">beta_tv_value</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_tv</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_tv</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (TV-L1): </span><span class="si">{</span><span class="n">mse_tv</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TV-L1 Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">beta_tv_value</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="smoothly-clipped-absolute-deviation-scad">
<h2>Smoothly Clipped Absolute Deviation (SCAD) <a id="SCAD"></a><a class="headerlink" href="#smoothly-clipped-absolute-deviation-scad" title="Link to this heading">#</a></h2>
<p>Smoothly Clipped Absolute Deviation (SCAD) is a regularization method designed to address some of the limitations of Lasso, particularly the bias introduced in the estimates of large coefficients. SCAD applies a non-convex penalty that behaves like Lasso for small coefficients, encouraging sparsity, but reduces the penalization for large coefficients, making it less biased than Lasso for important features. SCAD is particularly useful in high-dimensional settings where accurate variable selection and unbiased estimation are critical.</p>
<section id="id58">
<h3>Characteristics and Properties<a class="headerlink" href="#id58" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Non-Convex Penalty</strong>: Unlike Lasso, which uses a convex <span class="math notranslate nohighlight">\( L_1 \)</span> penalty, SCAD uses a non-convex penalty function that smoothly transitions between <span class="math notranslate nohighlight">\( L_1 \)</span> for small coefficients (to encourage sparsity) and no penalty for large coefficients (to reduce bias).</p></li>
<li><p><strong>Reduced Bias for Large Coefficients</strong>: SCAD reduces the bias on large coefficients, a limitation of Lasso, making it more effective in selecting important features without overly shrinking their magnitudes.</p></li>
<li><p><strong>Sparsity and Consistency</strong>: SCAD encourages sparsity like Lasso but is more consistent in its variable selection, particularly in high-dimensional data.</p></li>
</ul>
</section>
<section id="id59">
<h3>Mathematical Formulation<a class="headerlink" href="#id59" title="Link to this heading">#</a></h3>
<p>The SCAD penalty is defined as a piecewise function, where <span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter, and <span class="math notranslate nohighlight">\( a \)</span> is a parameter that controls the shape of the penalty:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P_{\lambda}(\beta_j) =
\begin{cases}
\lambda |\beta_j| &amp; \text{if } |\beta_j| \leq \lambda \\
\frac{-\beta_j^2 + 2a\lambda |\beta_j| - \lambda^2}{2(a-1)} &amp; \text{if } \lambda &lt; |\beta_j| \leq a\lambda \\
\frac{(a+1)\lambda^2}{2} &amp; \text{if } |\beta_j| &gt; a\lambda
\end{cases}
\end{split}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> is the coefficient for the <span class="math notranslate nohighlight">\( j \)</span>th variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter that controls the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( a \)</span> is a tuning parameter, typically set to 3.7, which controls the smoothness of the transition between the <span class="math notranslate nohighlight">\( L_1 \)</span> penalty and the reduced penalty for large coefficients.</p></li>
</ul>
<p>The SCAD penalty behaves like Lasso for small values of <span class="math notranslate nohighlight">\( \beta_j \)</span>, encouraging sparsity, but gradually reduces the penalty for larger coefficients, reducing bias and promoting more accurate estimates.</p>
</section>
<section id="characteristics-of-scad">
<h3>Characteristics of SCAD<a class="headerlink" href="#characteristics-of-scad" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias Reduction</strong>: SCAD reduces the bias on large coefficients that is typically introduced by Lasso, making it particularly suitable for applications where large coefficients are important.</p></li>
<li><p><strong>Sparsity for Small Coefficients</strong>: For small coefficients, SCAD behaves like Lasso and encourages sparsity, making it useful for variable selection in high-dimensional data.</p></li>
<li><p><strong>Non-Convex Optimization</strong>: SCAD involves a non-convex penalty function, which can make optimization more challenging, but leads to better variable selection and estimation performance in some cases.</p></li>
</ul>
</section>
<section id="id60">
<h3>Pros and Cons<a class="headerlink" href="#id60" title="Link to this heading">#</a></h3>
<section id="id61">
<h4>Pros<a class="headerlink" href="#id61" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Reduces bias for large coefficients</strong>: SCAD penalizes small coefficients heavily (like Lasso) but reduces the penalty for larger coefficients, resulting in less biased estimates for important features.</p></li>
<li><p><strong>Consistent variable selection</strong>: SCAD is known to exhibit the “oracle property,” meaning it can consistently select the correct model in large samples, even in high-dimensional settings.</p></li>
<li><p><strong>Sparsity</strong>: Like Lasso, SCAD encourages sparsity, leading to simpler and more interpretable models.</p></li>
</ul>
</section>
<section id="id62">
<h4>Cons<a class="headerlink" href="#id62" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Non-convexity</strong>: The non-convex nature of SCAD makes the optimization problem more challenging to solve compared to Lasso or Ridge. Specialized optimization algorithms are required.</p></li>
<li><p><strong>More complex tuning</strong>: SCAD involves tuning both the regularization parameter <span class="math notranslate nohighlight">\( \lambda \)</span> and the shape parameter <span class="math notranslate nohighlight">\( a \)</span>, adding complexity to the model selection process.</p></li>
<li><p><strong>Limited availability in standard libraries</strong>: SCAD is not as commonly implemented in popular machine learning libraries, requiring custom implementations or specialized packages.</p></li>
</ul>
</section>
</section>
<section id="id63">
<h3>Use Cases<a class="headerlink" href="#id63" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>High-Dimensional Data</strong>: SCAD is well-suited for high-dimensional datasets where accurate variable selection and minimal bias for important coefficients are crucial, such as in genomics or finance.</p></li>
<li><p><strong>Signal Processing</strong>: In signal processing tasks where large coefficients correspond to important signals, SCAD helps retain those signals while eliminating noise.</p></li>
<li><p><strong>Economics and Finance</strong>: SCAD is used in economic and financial modeling where variable selection is important, and large coefficients need to be estimated with minimal bias.</p></li>
</ul>
</section>
<section id="id64">
<h3>Python Implementation<a class="headerlink" href="#id64" title="Link to this heading">#</a></h3>
<p>SCAD is not natively available in popular machine learning libraries like <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, but it can be implemented using specialized optimization libraries. Below is a basic Python example using the <code class="docutils literal notranslate"><span class="pre">pySCAD</span></code> library (a hypothetical library for SCAD implementation) or using custom optimization methods via <code class="docutils literal notranslate"><span class="pre">cvxpy</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assuming pySCAD library exists (hypothetical example)</span>
<span class="c1"># Install pySCAD package: !pip install pySCAD</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pySCAD</span> <span class="kn">import</span> <span class="n">SCADRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>  <span class="c1"># Sparse coefficients</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize SCAD model</span>
<span class="n">scad</span> <span class="o">=</span> <span class="n">SCADRegressor</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">3.7</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">scad</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_scad</span> <span class="o">=</span> <span class="n">scad</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_scad</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_scad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (SCAD): </span><span class="si">{</span><span class="n">mse_scad</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SCAD Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scad</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="minimax-concave-penalty-mcp">
<h2>Minimax Concave Penalty (MCP) <a id="MCP"></a><a class="headerlink" href="#minimax-concave-penalty-mcp" title="Link to this heading">#</a></h2>
<p>Minimax Concave Penalty (MCP) is a non-convex regularization technique that, like SCAD, addresses the limitations of Lasso by reducing the bias on large coefficients while still promoting sparsity for small coefficients. MCP applies a concave penalty that grows more slowly than the Lasso penalty as the coefficient values increase, thus leading to less bias for large coefficients. This makes MCP particularly useful in high-dimensional settings where accurate feature selection and unbiased estimation are critical.</p>
<p>MCP, similar to SCAD, balances the tradeoff between sparsity and bias but with a slightly different penalty function. It’s an effective alternative to Lasso when it’s essential to preserve large coefficients while ensuring a sparse solution.</p>
<section id="id65">
<h3>Characteristics and Properties<a class="headerlink" href="#id65" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Non-Convex Penalty</strong>: MCP uses a non-convex penalty function that penalizes small coefficients similarly to Lasso but reduces the penalization for large coefficients, mitigating the bias often introduced by Lasso.</p></li>
<li><p><strong>Reduced Bias for Large Coefficients</strong>: MCP is specifically designed to reduce bias in large coefficients while retaining the sparsity benefits of Lasso for small coefficients.</p></li>
<li><p><strong>Sparsity and Efficiency</strong>: MCP, like Lasso, promotes sparsity by shrinking small coefficients toward zero, which makes it useful for variable selection in high-dimensional data.</p></li>
</ul>
</section>
<section id="id66">
<h3>Mathematical Formulation<a class="headerlink" href="#id66" title="Link to this heading">#</a></h3>
<p>The MCP penalty is defined as:</p>
<div class="math notranslate nohighlight">
\[
P_{\lambda}(\beta_j) = \lambda \int_0^{|\beta_j|} \left(1 - \frac{t}{\gamma \lambda}\right)_+ dt
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span> is the regularization parameter controlling the strength of the penalty.</p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_j \)</span> is the coefficient for the <span class="math notranslate nohighlight">\( j \)</span>th variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( \gamma \)</span> is a tuning parameter that controls the concavity of the penalty function.</p></li>
</ul>
<p>The MCP penalty behaves like Lasso for small values of <span class="math notranslate nohighlight">\( \beta_j \)</span>, encouraging sparsity, but the penalty increases more slowly as <span class="math notranslate nohighlight">\( \beta_j \)</span> grows larger. This reduces the shrinkage of large coefficients, thus minimizing the bias.</p>
<p>For practical implementation, the MCP penalty can also be expressed in piecewise form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P_{\lambda}(\beta_j) =
\begin{cases}
\lambda |\beta_j| - \frac{\beta_j^2}{2\gamma} &amp; \text{if } |\beta_j| \leq \gamma \lambda \\
\frac{\gamma \lambda^2}{2} &amp; \text{if } |\beta_j| &gt; \gamma \lambda
\end{cases}
\end{split}\]</div>
</section>
<section id="characteristics-of-mcp">
<h3>Characteristics of MCP<a class="headerlink" href="#characteristics-of-mcp" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias Reduction</strong>: Like SCAD, MCP reduces the bias for large coefficients, making it more suitable for estimating important features compared to Lasso.</p></li>
<li><p><strong>Sparsity</strong>: MCP retains the sparsity-promoting properties of Lasso, making it effective for variable selection in high-dimensional datasets.</p></li>
<li><p><strong>Non-Convexity</strong>: The non-convex nature of MCP makes it harder to optimize but allows for better variable selection and less bias compared to Lasso.</p></li>
</ul>
</section>
<section id="id67">
<h3>Pros and Cons<a class="headerlink" href="#id67" title="Link to this heading">#</a></h3>
<section id="id68">
<h4>Pros<a class="headerlink" href="#id68" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Reduces bias for large coefficients</strong>: MCP reduces the penalty on large coefficients, leading to more accurate estimates for important features while retaining sparsity for small coefficients.</p></li>
<li><p><strong>Sparsity</strong>: Like Lasso, MCP encourages sparsity, making it a good choice for variable selection in high-dimensional datasets.</p></li>
<li><p><strong>Oracle Property</strong>: Under certain conditions, MCP exhibits the oracle property, meaning it can identify the true model in large samples with high probability.</p></li>
<li><p><strong>Efficient for variable selection</strong>: MCP combines the benefits of sparsity and bias reduction, making it well-suited for high-dimensional feature selection.</p></li>
</ul>
</section>
<section id="id69">
<h4>Cons<a class="headerlink" href="#id69" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Non-convex optimization</strong>: MCP involves solving a non-convex optimization problem, which can be more computationally complex compared to Lasso or Ridge.</p></li>
<li><p><strong>More complex tuning</strong>: MCP introduces an additional tuning parameter <span class="math notranslate nohighlight">\( \gamma \)</span>, which must be selected carefully, increasing the complexity of model selection.</p></li>
<li><p><strong>Limited library support</strong>: MCP is not as widely implemented in standard machine learning libraries, requiring custom optimization or specialized packages.</p></li>
</ul>
</section>
</section>
<section id="id70">
<h3>Use Cases<a class="headerlink" href="#id70" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>High-Dimensional Data</strong>: MCP is effective in high-dimensional settings such as genomics or finance, where variable selection and reduced bias for large coefficients are crucial.</p></li>
<li><p><strong>Signal Processing</strong>: MCP can be used in signal processing tasks to retain large signals while eliminating noise, similar to SCAD.</p></li>
<li><p><strong>Economics and Finance</strong>: MCP is applied in economic and financial modeling where large coefficients need to be estimated with minimal bias, while small, irrelevant coefficients are shrunk to zero.</p></li>
</ul>
</section>
<section id="id71">
<h3>Python Implementation<a class="headerlink" href="#id71" title="Link to this heading">#</a></h3>
<p>MCP is not natively available in common machine learning libraries like <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, but it can be implemented using optimization libraries such as <code class="docutils literal notranslate"><span class="pre">cvxpy</span></code> or specialized libraries for non-convex optimization. Below is a hypothetical Python implementation using a custom MCP optimization routine:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hypothetical MCP implementation (assuming pyMCP library exists)</span>
<span class="c1"># Install pyMCP package: !pip install pyMCP</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pyMCP</span> <span class="kn">import</span> <span class="n">MCPRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>  <span class="c1"># Sparse coefficients</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize MCP model</span>
<span class="n">mcp</span> <span class="o">=</span> <span class="n">MCPRegressor</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">mcp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred_mcp</span> <span class="o">=</span> <span class="n">mcp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="n">mse_mcp</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mcp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean Squared Error (MCP): </span><span class="si">{</span><span class="n">mse_mcp</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Display coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MCP Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mcp</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

</pre></div>
</div>
</section>
</section>
<section id="comparison-of-regularized-regression-techniques">
<h2>Comparison of Regularized Regression Techniques<a class="headerlink" href="#comparison-of-regularized-regression-techniques" title="Link to this heading">#</a></h2>
<p>To better understand the differences and similarities between the various regularized regression techniques, let’s compare Ridge, Lasso, Elastic Net, and other advanced methods.</p>
<section id="overview-table">
<h3>Overview Table<a class="headerlink" href="#overview-table" title="Link to this heading">#</a></h3>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Penalty Type</p></th>
<th class="head"><p>Feature Selection</p></th>
<th class="head"><p>Handles Multicollinearity</p></th>
<th class="head"><p>Bias for Large Coefficients</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Ridge Regression</strong></p></td>
<td><p><span class="math notranslate nohighlight">\( L_2 \)</span></p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Lasso Regression</strong></p></td>
<td><p><span class="math notranslate nohighlight">\( L_1 \)</span></p></td>
<td><p>Yes</p></td>
<td><p>Partially</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><strong>Elastic Net</strong></p></td>
<td><p><span class="math notranslate nohighlight">\( L_1 + L_2 \)</span></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Adaptive Lasso</strong></p></td>
<td><p>Weighted <span class="math notranslate nohighlight">\( L_1 \)</span></p></td>
<td><p>Yes</p></td>
<td><p>Partially</p></td>
<td><p>Reduced</p></td>
</tr>
<tr class="row-even"><td><p><strong>Fused Lasso</strong></p></td>
<td><p><span class="math notranslate nohighlight">\( L_1 \)</span> + Differences</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Group Lasso</strong></p></td>
<td><p>Group <span class="math notranslate nohighlight">\( L_1 \)</span></p></td>
<td><p>Group Level</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><strong>Sparse Group Lasso</strong></p></td>
<td><p>Group <span class="math notranslate nohighlight">\( L_1 \)</span> + <span class="math notranslate nohighlight">\( L_1 \)</span></p></td>
<td><p>Group &amp; Individual</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><strong>TV-L1 (Total Variation)</strong></p></td>
<td><p><span class="math notranslate nohighlight">\( L_1 \)</span> on Differences</p></td>
<td><p>Yes (Sparsity in Diff)</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><strong>SCAD</strong></p></td>
<td><p>Non-Convex (Smooth) <span class="math notranslate nohighlight">\( L_1 \)</span></p></td>
<td><p>Yes</p></td>
<td><p>Partially</p></td>
<td><p>Reduced (Minimal Bias)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>MCP</strong></p></td>
<td><p>Non-Convex (Concave) <span class="math notranslate nohighlight">\( L_1 \)</span></p></td>
<td><p>Yes</p></td>
<td><p>Partially</p></td>
<td><p>Reduced (Minimal Bias)</p></td>
</tr>
<tr class="row-even"><td><p><strong>LARS (Least Angle Regression)</strong></p></td>
<td><p>No explicit penalty</p></td>
<td><p>Yes</p></td>
<td><p>Partially</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id72">
<h3>Bias-Variance Tradeoff <a id="Theoretical-Insights"></a><a class="headerlink" href="#id72" title="Link to this heading">#</a></h3>
<p>The bias-variance tradeoff is a fundamental concept in machine learning. Regularization methods help in balancing the tradeoff:</p>
<ul class="simple">
<li><p><strong>Ridge Regression</strong>: Reduces variance by shrinking coefficients, but may increase bias.</p></li>
<li><p><strong>Lasso Regression</strong>: Reduces variance and increases bias but provides interpretable models by selecting features.</p></li>
<li><p><strong>Elastic Net</strong>: Provides a balance between bias and variance, and is useful in selecting groups of correlated variables.</p></li>
</ul>
</section>
<section id="practical-considerations">
<h3>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Selection of Regularization Parameter</strong>: The regularization parameter (<span class="math notranslate nohighlight">\( \lambda \)</span>) controls the penalty strength and must be tuned carefully. Too high a value may lead to underfitting, while too low a value may cause overfitting.</p></li>
<li><p><strong>Scaling of Features</strong>: Regularization is sensitive to the scale of features, so standardization is typically recommended.</p></li>
</ul>
</section>
</section>
<section id="challenges-in-regularization">
<h2>Challenges in Regularization <a id="Challenges"></a><a class="headerlink" href="#challenges-in-regularization" title="Link to this heading">#</a></h2>
<p>Despite their advantages, regularization techniques come with certain challenges that need to be addressed for optimal model performance:</p>
<section id="over-regularization">
<h3>Over-Regularization<a class="headerlink" href="#over-regularization" title="Link to this heading">#</a></h3>
<p>Over-regularization can occur when the value of the regularization parameter is too high, resulting in coefficients being shrunk too much. This can lead to <strong>underfitting</strong> where the model is too simple to capture the underlying relationships in the data.</p>
</section>
<section id="selection-of-hyperparameters">
<h3>Selection of Hyperparameters<a class="headerlink" href="#selection-of-hyperparameters" title="Link to this heading">#</a></h3>
<p>Choosing the right value for the regularization parameter (<span class="math notranslate nohighlight">\( \lambda \)</span>) is crucial for achieving the desired trade-off between bias and variance. Incorrect selection can lead to poor model performance. Techniques such as <strong>cross-validation</strong> are often used to tune the hyperparameters.</p>
</section>
<section id="interpretability">
<h3>Interpretability<a class="headerlink" href="#interpretability" title="Link to this heading">#</a></h3>
<p>While Lasso can perform feature selection and improve model interpretability, other methods like Ridge retain all features, which can lead to models that are less interpretable. Feature importance may be harder to interpret in models that do not perform selection.</p>
</section>
<section id="correlated-features">
<h3>Correlated Features<a class="headerlink" href="#correlated-features" title="Link to this heading">#</a></h3>
<p>In the case of correlated features, Lasso may arbitrarily select one feature from a group, potentially disregarding other equally important features. Elastic Net helps mitigate this issue, but careful tuning of the <strong><span class="math notranslate nohighlight">\( l_1 \)</span> and <span class="math notranslate nohighlight">\( l_2 \)</span></strong> ratios is required.</p>
</section>
</section>
<section id="tuning-regularization-parameters">
<h2>Tuning Regularization Parameters <a id="Tuning-Parameters"></a><a class="headerlink" href="#tuning-regularization-parameters" title="Link to this heading">#</a></h2>
<p>Tuning the regularization parameters is an essential step to get the best performance out of regularized regression models. Here are some common techniques used for parameter tuning:</p>
<section id="grid-search">
<h3>Grid Search<a class="headerlink" href="#grid-search" title="Link to this heading">#</a></h3>
<p><strong>Grid Search</strong> involves exhaustively searching through a manually specified subset of hyperparameters. It is effective but computationally expensive, especially when dealing with multiple hyperparameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Define parameter grid</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]}</span>

<span class="c1"># Initialize Ridge Regression model</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>

<span class="c1"># Perform Grid Search</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">ridge</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Get the best parameter</span>
<span class="n">best_alpha</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best alpha found using Grid Search: </span><span class="si">{</span><span class="n">best_alpha</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="random-search">
<h3>Random Search<a class="headerlink" href="#random-search" title="Link to this heading">#</a></h3>
<p><strong>Random Search</strong> is an alternative to Grid Search where a random subset of the hyperparameter space is sampled. This approach can be more efficient in finding good hyperparameter values without the exhaustive computational cost of Grid Search.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="c1"># Perform Random Search</span>
<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">ridge</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Get the best parameter</span>
<span class="n">best_alpha_random</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best alpha found using Random Search: </span><span class="si">{</span><span class="n">best_alpha_random</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="cross-validation-techniques">
<h3>Cross-Validation Techniques<a class="headerlink" href="#cross-validation-techniques" title="Link to this heading">#</a></h3>
<p><strong>Cross-validation</strong> is often used to estimate the performance of a model and select the best hyperparameters. For regularized regression models, cross-validation helps determine the regularization strength that provides the best balance between bias and variance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Cross-validate Ridge Regression model</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">ridge</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
<span class="n">mean_cv_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean cross-validation score (MSE) for Ridge Regression: </span><span class="si">{</span><span class="n">mean_cv_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="05-supervised-ml.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Supervised Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="07-regularized.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">&lt;no title&gt;</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-linear-regression">Basic Linear Regression <a id="Basic-Linear-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-basic-linear-regression">Limitations of Basic Linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-regularized-linear-regression">Introduction to Regularized Linear Regression <a id="Introduction"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-methods-overview">Regularization Methods Overview</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">Ridge Regression <a id="Ridge-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-and-properties">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-tradeoff">Bias-Variance Tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-and-cons">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pros">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cons">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-implementation">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">Lasso Regression <a id="Lasso-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Bias-Variance Tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elastic-net">Elastic Net <a id="Elastic-Net"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-strengths-of-ridge-and-lasso">Combining Strengths of Ridge and Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-angle-regression-lars">Least Angle Regression (LARS) <a id="Least-Angle-Regression"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-characteristics">Key Characteristics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-lasso">Adaptive Lasso <a id="Adaptive-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-adaptive-lasso">Characteristics of Adaptive Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fused-lasso">Fused Lasso <a id="Fused-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-fused-lasso">Characteristics of Fused Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id36">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#group-lasso">Group Lasso <a id="Group-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id38">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-group-lasso">Characteristics of Group Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-group-lasso">Sparse Group Lasso <a id="Sparse-Group-Lasso"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id44">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id45">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-sparse-group-lasso">Characteristics of Sparse Group Lasso</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id46">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id47">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id48">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id49">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#total-variation-regularization-tv-l1">Total Variation Regularization (TV-L1) <a id="TV-L1"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id51">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id52">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-tv-l1">Characteristics of TV-L1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id54">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id55">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id56">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id57">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#smoothly-clipped-absolute-deviation-scad">Smoothly Clipped Absolute Deviation (SCAD) <a id="SCAD"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id58">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id59">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-scad">Characteristics of SCAD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id61">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id63">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id64">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimax-concave-penalty-mcp">Minimax Concave Penalty (MCP) <a id="MCP"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id65">Characteristics and Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id66">Mathematical Formulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-mcp">Characteristics of MCP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id67">Pros and Cons</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id68">Pros</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id69">Cons</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id70">Use Cases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id71">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-regularized-regression-techniques">Comparison of Regularized Regression Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-table">Overview Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id72">Bias-Variance Tradeoff <a id="Theoretical-Insights"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-considerations">Practical Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-regularization">Challenges in Regularization <a id="Challenges"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#over-regularization">Over-Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-of-hyperparameters">Selection of Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretability">Interpretability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-features">Correlated Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-regularization-parameters">Tuning Regularization Parameters <a id="Tuning-Parameters"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-search">Random Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation-techniques">Cross-Validation Techniques</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mike Nguyen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>