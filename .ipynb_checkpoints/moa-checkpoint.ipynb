{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b42c18-4a2c-49d4-ae76-050ed113d0c6",
   "metadata": {},
   "source": [
    "Mixture-of-Agents\n",
    "\n",
    "Paper that proposes this idea (https://arxiv.org/pdf/2406.04692)\n",
    "Model is avialabe on HUgging face (https://huggingface.co/papers/2406.04692)\n",
    "\n",
    "Implementation by [Together AI](https://www.together.ai/blog/together-moa)\n",
    "Implemntation by [Groq](https://groq.com/)\n",
    "Implementation by [Swarm](https://docs.swarms.world/en/latest/swarms/structs/moa/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e9d96-df8f-4154-b3bc-e0d4eaed3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7af18c1-02d0-49a1-a899-69d6afa3d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"dd26ca3ddf031d1c399077914b9824f9e7f9dc5c256d872c14d8a23383731ed2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55791672-5028-401e-9977-915342edf6e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m finalStream:\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 84\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\mlpy\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# Advanced Mixture-of-Agents example â€“ 3 layers\n",
    "import asyncio\n",
    "import os\n",
    "import together\n",
    "from together import AsyncTogether, Together\n",
    "\n",
    "client = Together()\n",
    "async_client = AsyncTogether()\n",
    "\n",
    "user_prompt = \"What are 3 fun things to do in SF?\"\n",
    "reference_models = [\n",
    "    \"Qwen/Qwen2-72B-Instruct\",\n",
    "    \"Qwen/Qwen1.5-72B-Chat\",\n",
    "    \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "    \"databricks/dbrx-instruct\",\n",
    "]\n",
    "aggregator_model = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "aggreagator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n",
    "\n",
    "Responses from models:\"\"\"\n",
    "layers = 3\n",
    "\n",
    "def getFinalSystemPrompt(system_prompt, results):\n",
    "    \"\"\"Construct a system prompt for layers 2+ that includes the previous responses to synthesize.\"\"\"\n",
    "    return (\n",
    "        system_prompt\n",
    "        + \"\\n\"\n",
    "        + \"\\n\".join([f\"{i+1}. {str(element)}\" for i, element in enumerate(results)])\n",
    "    )\n",
    "\n",
    "async def run_llm(model, prev_response=None):\n",
    "    \"\"\"Run a single LLM call with a model while accounting for previous responses + rate limits.\"\"\"\n",
    "    for sleep_time in [1, 2, 4]:\n",
    "        try:\n",
    "            messages = (\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": getFinalSystemPrompt(\n",
    "                            aggreagator_system_prompt, prev_response\n",
    "                        ),\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ]\n",
    "                if prev_response\n",
    "                else [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "            )\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "            )\n",
    "            print(\"Model: \", model)\n",
    "            break\n",
    "        except together.error.RateLimitError as e:\n",
    "            print(e)\n",
    "            await asyncio.sleep(sleep_time)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Run the main loop of the MOA process.\"\"\"\n",
    "    results = await asyncio.gather(*[run_llm(model) for model in reference_models])\n",
    "\n",
    "    for _ in range(1, layers - 1):\n",
    "        results = await asyncio.gather(\n",
    "            *[run_llm(model, prev_response=results) for model in reference_models]\n",
    "        )\n",
    "\n",
    "    finalStream = client.chat.completions.create(\n",
    "        model=aggregator_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": getFinalSystemPrompt(aggreagator_system_prompt, results),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    for chunk in finalStream:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403a9160-b27b-49de-b14a-7dba68364ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached: Error code: 429 - {\"message\": \"Request was rejected due to request rate limiting. Your rate limits are 60 RPM (1 QPS) and 60000 TPM (1000 TPS). See details: https://docs.together.ai/docs/rate-limits\", \"type_\": \"credit_limit\"}. Retrying in 1 seconds.\n",
      "Rate limit reached: Error code: 429 - {\"message\": \"Request was rejected due to request rate limiting. Your rate limits are 60 RPM (1 QPS) and 60000 TPM (1000 TPS). See details: https://docs.together.ai/docs/rate-limits\", \"type_\": \"credit_limit\"}. Retrying in 2 seconds.\n",
      "Model: mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "Model: Qwen/Qwen2-72B-Instruct\n",
      "Model: databricks/dbrx-instruct\n",
      "Rate limit reached: Error code: 429 - {\"message\": \"Request was rejected due to request rate limiting. Your rate limits are 60 RPM (1 QPS) and 60000 TPM (1000 TPS). See details: https://docs.together.ai/docs/rate-limits\", \"type_\": \"credit_limit\"}. Retrying in 1 seconds.\n",
      "Rate limit reached: Error code: 429 - {\"message\": \"Request was rejected due to request rate limiting. Your rate limits are 60 RPM (1 QPS) and 60000 TPM (1000 TPS). See details: https://docs.together.ai/docs/rate-limits\", \"type_\": \"credit_limit\"}. Retrying in 2 seconds.\n",
      "Model: mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "Model: databricks/dbrx-instruct\n",
      "Model: Qwen/Qwen2-72B-Instruct\n",
      " 1. Cross the Golden Gate Bridge: This iconic landmark offers breathtaking views of the San Francisco Bay and the city skyline. You can walk or bike across the bridge and visit the Golden Gate Bridge Welcome Center to learn about its history and engineering.\n",
      "2. Discover Alcatraz Island: Take a ferry to this historic island and explore the infamous federal prison that once housed notorious criminals. The self-guided audio tour provides a fascinating look into the island's history, and you can enjoy panoramic views of the city and bay.\n",
      "3. Experience the Exploratorium: This interactive science museum offers a fun and educational experience for all ages with over 600 hands-on exhibits covering topics such as physics, biology, and human perception."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import together\n",
    "from together import AsyncTogether, Together\n",
    "\n",
    "nest_asyncio.apply()  # Allows nested event loops\n",
    "\n",
    "client = Together()\n",
    "async_client = AsyncTogether()\n",
    "\n",
    "user_prompt = \"What are 3 fun things to do in SF?\"\n",
    "\n",
    "# Update models based on Together's available models\n",
    "reference_models = [\n",
    "    \"Qwen/Qwen2-72B-Instruct\",  # Ensure this model is available\n",
    "    \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "    \"databricks/dbrx-instruct\",\n",
    "]\n",
    "aggregator_model = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "aggregator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models...\"\"\"\n",
    "\n",
    "layers = 3\n",
    "\n",
    "def get_final_system_prompt(system_prompt, results):\n",
    "    \"\"\"Construct a system prompt for layers 2+ that includes the previous responses to synthesize.\"\"\"\n",
    "    return (\n",
    "        system_prompt\n",
    "        + \"\\n\"\n",
    "        + \"\\n\".join([f\"{i+1}. {str(element)}\" for i, element in enumerate(results)])\n",
    "    )\n",
    "\n",
    "async def run_llm(model, prev_response=None):\n",
    "    \"\"\"Run a single LLM call with a model while accounting for previous responses + rate limits.\"\"\"\n",
    "    for sleep_time in [1, 2, 4, 8]:  # Increasing wait times for retry\n",
    "        try:\n",
    "            messages = (\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": get_final_system_prompt(\n",
    "                            aggregator_system_prompt, prev_response\n",
    "                        ),\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ]\n",
    "                if prev_response\n",
    "                else [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "            )\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=512,\n",
    "            )\n",
    "            print(\"Model:\", model)\n",
    "            return response.choices[0].message.content\n",
    "        except together.error.RateLimitError as e:\n",
    "            print(f\"Rate limit reached: {e}. Retrying in {sleep_time} seconds.\")\n",
    "            await asyncio.sleep(sleep_time)  # Wait before retrying\n",
    "        except together.error.InvalidRequestError as e:\n",
    "            print(f\"Model {model} is not available. Skipping.\")\n",
    "            return None  # Skip unavailable model\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Run the main loop of the MOA process.\"\"\"\n",
    "    results = await asyncio.gather(*[run_llm(model) for model in reference_models])\n",
    "\n",
    "    for _ in range(1, layers - 1):\n",
    "        await asyncio.sleep(1)  # Pause to avoid rate limits\n",
    "        results = await asyncio.gather(\n",
    "            *[run_llm(model, prev_response=results) for model in reference_models]\n",
    "        )\n",
    "\n",
    "    finalStream = client.chat.completions.create(\n",
    "        model=aggregator_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": get_final_system_prompt(aggregator_system_prompt, results),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for chunk in finalStream:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0ae8a-9d89-477e-a8e7-2f0661183bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
